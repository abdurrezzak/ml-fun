{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame with the feature data\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "df.to_csv('iris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "# Add the target variable to the DataFrame\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Optionally, display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (30, 4), (120,), (30,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do not need to normalize the data because we will implement a decision tree classifier\n",
    "# which is not sensitive to the scale of the features\n",
    "# However, we do need to split the data into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "Decision tree classifier divides the data into segments to reduce hidden information as much as possible so that when a new datapoint is presented, its class is determined accordingly. \n",
    "For it to work, we have two choices:\n",
    "\n",
    "- Information Gain\n",
    "- Gini Score\n",
    "\n",
    "We will go with Information Gain, but implementing the other is no harder.\n",
    "\n",
    "\n",
    "Let's define entropy first!\n",
    "Entropy is the amount of hidden information in a state of a system. \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    " \n",
    "$S = \\sum_{i=1}^{n} - p_i \\log{p_i}$\n",
    "\n",
    "</div>\n",
    "\n",
    "A decision tree has decision nodes that will divide the data into separate parts to \"gain\" information. That is, when the data fits \"a rule\" following that rule will give us purer data, with more insights.\n",
    "\n",
    "So, starting from a root node (with all the data), we start making decisions (dividing the data in subsets), and making sure we do not overfit with either max depth, or min number of datapoints while dividing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, entropy, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.entropy = entropy\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class  # e.g., {class_label: count}\n",
    "        self.feature_index = None      # Feature index used for splitting\n",
    "        self.threshold = None          # Threshold value for the feature split\n",
    "        self.left = None               # Left child node\n",
    "        self.right = None              # Right child node\n",
    "        self.predicted_class = predicted_class  # The majority class (for leaf prediction)\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        \"\"\"\n",
    "        max_depth: Maximum depth of the tree.\n",
    "        min_samples_split: Minimum samples required to split a node.\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.num_classes = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        self.num_features = X.shape[1]\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples = len(y)\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        num_samples_per_class = dict(zip(classes, counts))\n",
    "        predicted_class = classes[np.argmax(counts)]\n",
    "        parent_entropy = self._calculate_entropy(y)\n",
    "        node = Node(\n",
    "            entropy=parent_entropy,\n",
    "            num_samples=n_samples,\n",
    "            num_samples_per_class=num_samples_per_class,\n",
    "            predicted_class=predicted_class\n",
    "        )\n",
    "\n",
    "        # Stopping criteria:\n",
    "        # 1. Maximum depth reached.\n",
    "        # 2. Not enough samples to split.\n",
    "        # 3. Node is pure.\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            parent_entropy == 0):\n",
    "            return node\n",
    "\n",
    "        # Determine the best split by maximizing information gain.\n",
    "        best_feature_idx, best_threshold, best_info_gain = self._determine_best_split(X, y, parent_entropy)\n",
    "        if best_feature_idx is None or best_info_gain <= 0:\n",
    "            return node  # No valid split\n",
    "\n",
    "        # Split data on the best feature/threshold.\n",
    "        left_indices = X[:, best_feature_idx] <= best_threshold\n",
    "        right_indices = X[:, best_feature_idx] > best_threshold\n",
    "\n",
    "        # If a split ends up with an empty node, return current node.\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            return node\n",
    "\n",
    "        node.feature_index = best_feature_idx\n",
    "        node.threshold = best_threshold\n",
    "        node.left = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        node.right = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _calculate_entropy(self, y):\n",
    "        \"\"\"Calculate entropy for labels y.\"\"\"\n",
    "        n_samples = len(y)\n",
    "        if n_samples == 0:\n",
    "            return 0\n",
    "        entropy = 0\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        for count in counts:\n",
    "            p = count / n_samples\n",
    "            if p > 0:\n",
    "                entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def _determine_best_split(self, X, y, parent_entropy):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_info_gain = -float('inf')\n",
    "        best_feature_idx, best_threshold = None, None\n",
    "\n",
    "        # Loop over every feature.\n",
    "        for feature_idx in range(n_features):\n",
    "            # Sort feature values and corresponding labels.\n",
    "            sorted_idx = np.argsort(X[:, feature_idx])\n",
    "            X_sorted = X[sorted_idx, feature_idx]\n",
    "            y_sorted = y[sorted_idx]\n",
    "\n",
    "            # Iterate over potential split points (only when the feature value changes).\n",
    "            for i in range(1, n_samples):\n",
    "                if X_sorted[i] == X_sorted[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                # Use the midpoint between consecutive values as the threshold.\n",
    "                threshold = (X_sorted[i] + X_sorted[i - 1]) / 2.0\n",
    "\n",
    "                # Split labels into left and right groups.\n",
    "                left_y = y_sorted[:i]\n",
    "                right_y = y_sorted[i:]\n",
    "\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Compute the entropy for the splits.\n",
    "                left_entropy = self._calculate_entropy(left_y)\n",
    "                right_entropy = self._calculate_entropy(right_y)\n",
    "\n",
    "                # Compute weighted average of children entropies.\n",
    "                weighted_entropy = (len(left_y) / n_samples) * left_entropy + (len(right_y) / n_samples) * right_entropy\n",
    "\n",
    "                # Information Gain = Parent Entropy - Weighted Entropy of children.\n",
    "                info_gain = parent_entropy - weighted_entropy\n",
    "\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_idx, best_threshold, best_info_gain\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_instance(x, self.root) for x in X])\n",
    "\n",
    "    def _predict_instance(self, x, node):\n",
    "        # If the node is a leaf, return its predicted class.\n",
    "        if node.is_leaf_node():\n",
    "            return node.predicted_class\n",
    "\n",
    "        # Traverse the tree recursively.\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict_instance(x, node.left)\n",
    "        else:\n",
    "            return self._predict_instance(x, node.right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can make some predictions using our own DecisionTree class\n",
    "import numpy as np\n",
    "decision_tree = DecisionTree(max_depth=3)\n",
    "decision_tree.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Recall: 1.00\n",
      "Precision: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# now we can get accuracy, recall, precision, and F1 score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score \n",
    "\n",
    "y_pred = decision_tree.predict(X_test.values)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn DecisionTreeClassifier\n",
      "Accuracy: 1.00\n",
      "Recall: 1.00\n",
      "Precision: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# let's compare our results with the sklearn DecisionTreeClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=4)\n",
    "sklearn_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
    "\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "recall_sklearn = recall_score(y_test, y_pred_sklearn, average='weighted')\n",
    "precision_sklearn = precision_score(y_test, y_pred_sklearn, average='weighted')\n",
    "f1_sklearn = f1_score(y_test, y_pred_sklearn, average='weighted')\n",
    "\n",
    "print(f'Sklearn DecisionTreeClassifier')\n",
    "print(f'Accuracy: {accuracy_sklearn:.2f}')\n",
    "print(f'Recall: {recall_sklearn:.2f}')\n",
    "print(f'Precision: {precision_sklearn:.2f}')\n",
    "print(f'F1 Score: {f1_sklearn:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
