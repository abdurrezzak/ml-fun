{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we will start with easy stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# let's create a tensor with integers\n",
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "print(tensor_1.dtype)\n",
    "\n",
    "tensor_2 = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(tensor_2.dtype)\n",
    "\n",
    "tensor_3 = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "print(tensor_3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# let's create a 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(tensor_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5371, 0.3849, 0.8320, 0.7840, 0.1607, 0.6498, 0.9343, 0.5910],\n",
      "        [0.2814, 0.4740, 0.5027, 0.3182, 0.4595, 0.0767, 0.2318, 0.1066],\n",
      "        [0.8719, 0.2292, 0.3428, 0.0359, 0.0762, 0.4600, 0.8769, 0.3245],\n",
      "        [0.1792, 0.3918, 0.1266, 0.3143, 0.6167, 0.4740, 0.5066, 0.1777],\n",
      "        [0.9177, 0.9157, 0.1392, 0.2947, 0.5844, 0.0366, 0.6809, 0.1838]])\n"
     ]
    }
   ],
   "source": [
    "# now let's see how to create a tensor with random values\n",
    "tensor_random = torch.rand(5,8)\n",
    "print(tensor_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 10, 18])\n"
     ]
    }
   ],
   "source": [
    "# we can multiply tensors\n",
    "tensor_a = torch.tensor([1, 2, 3])\n",
    "tensor_b = torch.tensor([4, 5, 6])\n",
    "\n",
    "print(tensor_a * tensor_b) # element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n",
      "tensor([[ 4.,  5.,  6.],\n",
      "        [ 8., 10., 12.],\n",
      "        [12., 15., 18.]])\n"
     ]
    }
   ],
   "source": [
    "# we can also do matrix multiplication with torch.matmul\n",
    "tensor_a = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "tensor_b = torch.tensor([4,5,6], dtype=torch.float32)\n",
    "\n",
    "# this does a dot product of the two tensors\n",
    "print(tensor_a.matmul(tensor_b)) # torch assumes this is a row vector * column vector multiplication\n",
    "\n",
    "# but if we reshaped out vectors\n",
    "tensor_a = tensor_a.reshape(3,1)\n",
    "tensor_b = tensor_b.reshape(1,3)\n",
    "print(tensor_a.matmul(tensor_b)) # now this is a column vector * row vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[0.7907, 1.1798, 0.9568, 0.8844, 1.1013],\n",
      "        [0.6953, 0.6785, 0.6407, 0.8998, 0.7474],\n",
      "        [0.8896, 0.8440, 0.8338, 1.1703, 0.9573]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.rand(3,4)\n",
    "tensor_b = torch.rand(4,5)\n",
    "\n",
    "# the expected shape of the output is (3,5) if we do a matrix multiplication\n",
    "tensor_c = tensor_a.matmul(tensor_b)\n",
    "print(tensor_c.shape)\n",
    "print(tensor_c)\n",
    "\n",
    "# note that the second dim of tensor_a must match the first dim of tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# we can do diagonal matrices\n",
    "tensor_diag = torch.eye(3)\n",
    "print(tensor_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3575, 0.0000, 0.0000],\n",
      "        [0.1097, 0.1298, 0.0000],\n",
      "        [0.6207, 0.5240, 0.1318]])\n",
      "tensor([[0.2183, 0.4070, 0.0014],\n",
      "        [0.0000, 0.3033, 0.5835],\n",
      "        [0.0000, 0.0000, 0.1753]])\n",
      "tensor([[0.5535, 0.1925, 0.9134],\n",
      "        [0.3229, 0.6823, 0.5823],\n",
      "        [0.5536, 0.1410, 0.1985]])\n",
      "tensor([[0.1979, 0.0000, 0.0000],\n",
      "        [0.0354, 0.0886, 0.0000],\n",
      "        [0.3436, 0.0739, 0.0262]])\n",
      "tensor([[0.1208, 0.0783, 0.0013],\n",
      "        [0.0000, 0.2070, 0.3398],\n",
      "        [0.0000, 0.0000, 0.0348]])\n"
     ]
    }
   ],
   "source": [
    "# we can do triangular matrices\n",
    "tensor_tril = torch.tril(torch.rand(3,3))\n",
    "print(tensor_tril)\n",
    "\n",
    "# we can do up triangular matrices\n",
    "tensor_triu = torch.triu(torch.rand(3,3))\n",
    "print(tensor_triu)\n",
    "\n",
    "# we can use this triangular matrices to mask other tensors\n",
    "tensor = torch.rand(3,3)\n",
    "print(tensor)\n",
    "print(tensor * tensor_tril)\n",
    "print(tensor * tensor_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0155, 0.5738, 0.6506],\n",
      "        [0.0606, 0.1988, 0.0831]])\n",
      "tensor([[0.0155, 0.0606],\n",
      "        [0.5738, 0.1988],\n",
      "        [0.6506, 0.0831]])\n",
      "tensor([[0.1892, 0.8319, 0.0568],\n",
      "        [0.4324, 0.8717, 0.3599]])\n",
      "tensor([[0.1892, 0.4324],\n",
      "        [0.8319, 0.8717],\n",
      "        [0.0568, 0.3599]])\n",
      "----------\n",
      "tensor([[[0.9143, 0.1955, 0.1037, 0.7143],\n",
      "         [0.0418, 0.3188, 0.0610, 0.6084],\n",
      "         [0.7997, 0.2773, 0.6745, 0.3430]],\n",
      "\n",
      "        [[0.2062, 0.6677, 0.6808, 0.1901],\n",
      "         [0.8118, 0.9877, 0.8359, 0.2496],\n",
      "         [0.3655, 0.8972, 0.5180, 0.5843]]])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([3, 2, 4])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# we can do transpose\n",
    "tensor = torch.rand(2,3)\n",
    "print(tensor)\n",
    "print(tensor.T)\n",
    "\n",
    "# we can also do .transpose\n",
    "tensor = torch.rand(2,3)\n",
    "print(tensor)\n",
    "print(tensor.transpose(0,1))\n",
    "\n",
    "print(\"----------\")\n",
    "# but there is a difference between .T and .transpose\n",
    "tensor = torch.rand(2,3,4)\n",
    "print(tensor)\n",
    "print(tensor.T.shape)\n",
    "\n",
    "print(tensor.transpose(0,1).shape)\n",
    "print(tensor.transpose(0,2).shape)\n",
    "print(tensor.transpose(1,2).shape)\n",
    "\n",
    "# so, .transpose is more general than .T and swaps the dimensions according to the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.9789e-05, 6.8113e-01, 7.7449e-01, 7.7670e-01],\n",
      "         [4.0751e-01, 8.6675e-01, 5.4533e-01, 5.6070e-01],\n",
      "         [9.8145e-01, 4.4511e-01, 5.2002e-01, 8.5224e-01]],\n",
      "\n",
      "        [[6.2254e-01, 9.2072e-01, 7.4007e-01, 3.4289e-01],\n",
      "         [9.0323e-01, 1.7981e-01, 2.8845e-01, 7.0987e-01],\n",
      "         [2.8712e-01, 3.1458e-01, 3.8200e-01, 5.3390e-01]]])\n",
      "tensor([[1.9789e-05, 6.8113e-01, 7.7449e-01, 7.7670e-01, 4.0751e-01, 8.6675e-01,\n",
      "         5.4533e-01, 5.6070e-01, 9.8145e-01, 4.4511e-01, 5.2002e-01, 8.5224e-01],\n",
      "        [6.2254e-01, 9.2072e-01, 7.4007e-01, 3.4289e-01, 9.0323e-01, 1.7981e-01,\n",
      "         2.8845e-01, 7.0987e-01, 2.8712e-01, 3.1458e-01, 3.8200e-01, 5.3390e-01]])\n",
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "# we can also do reshaping\n",
    "tensor = torch.rand(2,3,4)\n",
    "print(tensor)\n",
    "print(tensor.reshape(2,12))\n",
    "print(tensor.reshape(2,12).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4960, 0.8896, 0.5225],\n",
      "        [0.9050, 0.0382, 0.5969]])\n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[0.4960, 0.8896, 0.5225],\n",
      "         [0.9050, 0.0382, 0.5969]]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([[[0.4960, 0.8896, 0.5225]],\n",
      "\n",
      "        [[0.9050, 0.0382, 0.5969]]])\n",
      "torch.Size([2, 3, 1])\n",
      "tensor([[[0.4960],\n",
      "         [0.8896],\n",
      "         [0.5225]],\n",
      "\n",
      "        [[0.9050],\n",
      "         [0.0382],\n",
      "         [0.5969]]])\n"
     ]
    }
   ],
   "source": [
    "# we can also do unsqueeze which means adding a new dimension at the specified index\n",
    "tensor = torch.rand(2,3)\n",
    "print(tensor)\n",
    "print(tensor.unsqueeze(0).shape)\n",
    "print(tensor.unsqueeze(0))\n",
    "print(tensor.unsqueeze(1).shape)\n",
    "\n",
    "print(tensor.unsqueeze(1))\n",
    "print(tensor.unsqueeze(2).shape)\n",
    "print(tensor.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6588, 0.1480, 0.1260]],\n",
      "\n",
      "         [[0.2302, 0.9396, 0.9848]]]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0.6588, 0.1480, 0.1260],\n",
      "        [0.2302, 0.9396, 0.9848]])\n"
     ]
    }
   ],
   "source": [
    "# we can also do squeeze which removes dimensions of size 1\n",
    "tensor = torch.rand(1,2,1,3)\n",
    "print(tensor)\n",
    "print(tensor.squeeze().shape)\n",
    "print(tensor.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3985, 0.0787, 0.4075],\n",
      "        [0.2281, 0.8134, 0.8530]])\n",
      "tensor([[0.0575, 0.9972, 0.7663],\n",
      "        [0.0813, 0.9820, 0.7008]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[0.3985, 0.0787, 0.4075],\n",
      "        [0.2281, 0.8134, 0.8530],\n",
      "        [0.0575, 0.9972, 0.7663],\n",
      "        [0.0813, 0.9820, 0.7008]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[0.3985, 0.0787, 0.4075, 0.0575, 0.9972, 0.7663],\n",
      "        [0.2281, 0.8134, 0.8530, 0.0813, 0.9820, 0.7008]])\n"
     ]
    }
   ],
   "source": [
    "# lets see how to concatenate tensors\n",
    "tensor_a = torch.rand(2,3)\n",
    "print(tensor_a)\n",
    "tensor_b = torch.rand(2,3)\n",
    "print(tensor_b)\n",
    "print(torch.cat((tensor_a, tensor_b), dim=0).shape)\n",
    "print(torch.cat((tensor_a, tensor_b), dim=0))\n",
    "print(torch.cat((tensor_a, tensor_b), dim=1).shape)\n",
    "print(torch.cat((tensor_a, tensor_b), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6750, 0.7801, 0.1586],\n",
      "        [0.7337, 0.6567, 0.4342]])\n",
      "tensor([[0.7450, 0.4820, 0.2920],\n",
      "        [0.2151, 0.0710, 0.4618]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6750, 0.7801, 0.1586],\n",
      "         [0.7337, 0.6567, 0.4342]],\n",
      "\n",
      "        [[0.7450, 0.4820, 0.2920],\n",
      "         [0.2151, 0.0710, 0.4618]]])\n"
     ]
    }
   ],
   "source": [
    "# now let's see how to stack tensors\n",
    "tensor_a = torch.rand(2,3)\n",
    "print(tensor_a)\n",
    "tensor_b = torch.rand(2,3)\n",
    "print(tensor_b)\n",
    "print(torch.stack((tensor_a, tensor_b), dim=0).shape)\n",
    "print(torch.stack((tensor_a, tensor_b), dim=0))\n",
    "\n",
    "# realize that this is not concatenation, but stacking the tensors along a new dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2394, 0.8496, 0.8830, 0.9283, 0.7817, 0.5668],\n",
      "        [0.0480, 0.3386, 0.1572, 0.4007, 0.2569, 0.0700],\n",
      "        [0.4446, 0.9578, 0.9941, 0.7145, 0.3539, 0.4140],\n",
      "        [0.4593, 0.1551, 0.8149, 0.9446, 0.8731, 0.3459]])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "tensor([[0.2394, 0.8496, 0.8830],\n",
      "        [0.0480, 0.3386, 0.1572],\n",
      "        [0.4446, 0.9578, 0.9941],\n",
      "        [0.4593, 0.1551, 0.8149]])\n",
      "tensor([[0.9283, 0.7817, 0.5668],\n",
      "        [0.4007, 0.2569, 0.0700],\n",
      "        [0.7145, 0.3539, 0.4140],\n",
      "        [0.9446, 0.8731, 0.3459]])\n",
      "torch.Size([2, 6])\n",
      "torch.Size([2, 6])\n",
      "tensor([[0.2394, 0.8496, 0.8830, 0.9283, 0.7817, 0.5668],\n",
      "        [0.0480, 0.3386, 0.1572, 0.4007, 0.2569, 0.0700]])\n",
      "tensor([[0.4446, 0.9578, 0.9941, 0.7145, 0.3539, 0.4140],\n",
      "        [0.4593, 0.1551, 0.8149, 0.9446, 0.8731, 0.3459]])\n"
     ]
    }
   ],
   "source": [
    "# we can also split tensors\n",
    "tensor = torch.rand(4,6)\n",
    "print(tensor)\n",
    "\n",
    "tensors = torch.split(tensor, 3, dim=1) # here the parameters are the tensor to split, the size of the split and the dimension to split\n",
    "print(tensors[0].shape)\n",
    "print(tensors[1].shape)\n",
    "print(tensors[0])\n",
    "print(tensors[1])\n",
    "\n",
    "tensors_ = torch.split(tensor, 2, dim=0) \n",
    "print(tensors_[0].shape)\n",
    "print(tensors_[1].shape)\n",
    "print(tensors_[0])\n",
    "print(tensors_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "tensor([[0.2575, 0.2275, 0.0819],\n",
      "        [0.8984, 0.7934, 0.4547],\n",
      "        [0.1704, 0.6474, 0.0275],\n",
      "        [0.3353, 0.0175, 0.0109]])\n",
      "tensor([[0.7875, 0.2249, 0.8932],\n",
      "        [0.8918, 0.3278, 0.6561],\n",
      "        [0.7794, 0.8901, 0.6110],\n",
      "        [0.3115, 0.0858, 0.4633]])\n",
      "tensor([[0.5766, 0.2049, 0.4238],\n",
      "        [0.6132, 0.2260, 0.8210],\n",
      "        [0.5317, 0.3881, 0.4715],\n",
      "        [0.9549, 0.3136, 0.7583]])\n"
     ]
    }
   ],
   "source": [
    "# we can also do chunk\n",
    "tensor = torch.rand(4,9)\n",
    "tensors = torch.chunk(tensor, 3, dim=1) # here the parameters are the tensor to split, the number of chunks and the dimension to split\n",
    "print(tensors[0].shape)\n",
    "print(tensors[1].shape)\n",
    "print(tensors[2].shape)\n",
    "print(tensors[0])\n",
    "print(tensors[1])\n",
    "print(tensors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7577, 0.3734, 0.9325, 0.6722],\n",
      "        [0.9541, 0.3060, 0.0182, 0.9159],\n",
      "        [0.6839, 0.1839, 0.3785, 0.8505]])\n",
      "tensor([0.7577, 0.3734, 0.9325, 0.6722])\n",
      "tensor([0.7577, 0.3734, 0.9325, 0.6722])\n",
      "tensor([0.7577, 0.9541, 0.6839])\n",
      "tensor(0.7577)\n"
     ]
    }
   ],
   "source": [
    "# we can also do indexing\n",
    "tensor = torch.rand(3,4)\n",
    "print(tensor)\n",
    "\n",
    "print(tensor[0])\n",
    "print(tensor[0, :])\n",
    "print(tensor[:, 0])\n",
    "print(tensor[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0389, 0.8066, 0.8206, 0.8835],\n",
      "        [0.7215, 0.7207, 0.6425, 0.6857],\n",
      "        [0.8858, 0.1784, 0.6704, 0.3493]])\n",
      "tensor([0.3801, 0.8884, 0.6549, 0.7298])\n",
      "Adding the two tensors\n",
      "tensor([[0.4190, 1.6950, 1.4755, 1.6133],\n",
      "        [1.1016, 1.6091, 1.2974, 1.4156],\n",
      "        [1.2659, 1.0668, 1.3253, 1.0791]])\n",
      "Multiplying the two tensors\n",
      "tensor([[0.0148, 0.7166, 0.5374, 0.6448],\n",
      "        [0.2742, 0.6403, 0.4208, 0.5005],\n",
      "        [0.3367, 0.1585, 0.4390, 0.2550]])\n"
     ]
    }
   ],
   "source": [
    "# Now we can move on to broadcasting\n",
    "tensor_a = torch.rand(3,4)\n",
    "tensor_b = torch.rand(4)\n",
    "print(tensor_a)\n",
    "print(tensor_b)\n",
    "\n",
    "print(\"Adding the two tensors\")\n",
    "print(tensor_a + tensor_b)\n",
    "# see that all the elements of tensor_b are added to each row of tensor_a because of broadcasting\n",
    "\n",
    "print(\"Multiplying the two tensors\")\n",
    "print(tensor_a * tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2478, 0.2698, 0.4801, 0.3738],\n",
      "        [0.6383, 0.1079, 0.8313, 0.0323],\n",
      "        [0.7407, 0.2307, 0.0487, 0.0233]])\n",
      "tensor([[2.2478, 2.2698, 2.4801, 2.3738],\n",
      "        [2.6383, 2.1079, 2.8313, 2.0323],\n",
      "        [2.7407, 2.2307, 2.0487, 2.0233]])\n"
     ]
    }
   ],
   "source": [
    "# we can add a scalar to a tensor\n",
    "tensor = torch.rand(3,4)\n",
    "print(tensor)\n",
    "print(tensor + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2666, 0.7235, 0.3030, 0.1200],\n",
      "        [0.8834, 0.2651, 0.0047, 0.3896],\n",
      "        [0.8517, 0.6083, 0.5672, 0.5638]])\n",
      "tensor([[1.3055, 2.0616, 1.3539, 1.1275],\n",
      "        [2.4192, 1.3035, 1.0047, 1.4763],\n",
      "        [2.3435, 1.8373, 1.7632, 1.7573]])\n",
      "tensor([[-1.3222, -0.3237, -1.1940, -2.1199],\n",
      "        [-0.1239, -1.3277, -5.3647, -0.9427],\n",
      "        [-0.1606, -0.4971, -0.5671, -0.5730]])\n",
      "tensor([[0.5163, 0.8506, 0.5505, 0.3465],\n",
      "        [0.9399, 0.5149, 0.0684, 0.6241],\n",
      "        [0.9229, 0.7799, 0.7531, 0.7509]])\n"
     ]
    }
   ],
   "source": [
    "# we can also do element-wise operations\n",
    "tensor = torch.rand(3,4)\n",
    "print(tensor)\n",
    "print(tensor.exp())\n",
    "print(tensor.log())\n",
    "print(tensor.sqrt())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now move on to gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "None\n",
      "tensor(5.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# let's see how to use gradients\n",
    "tensor_a = torch.tensor(2.0, requires_grad=True)\n",
    "tensor_b = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "tensor_c = tensor_a * tensor_b\n",
    "print(tensor_c.item())\n",
    "\n",
    "print(tensor_c.backward())\n",
    "\n",
    "print(tensor_a.grad) # this is partial derivative of tensor_c w.r.t tensor_a\n",
    "print(tensor_b.grad) # this is partial derivative of tensor_c w.r.t tensor_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 * e^3.0:  40.17107391357422\n",
      "None\n",
      "∂c / ∂a = e^3.0 :  tensor(20.0855)\n",
      "∂c / ∂b = 2.0 * e^3.0 :  tensor(40.1711)\n"
     ]
    }
   ],
   "source": [
    "# we can do some more advanced gradient stuff\n",
    "tensor_a = torch.tensor(2.0, requires_grad=True)\n",
    "tensor_b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "tensor_c = tensor_a * tensor_b.exp()\n",
    "\n",
    "print(f'{tensor_a.item()} * e^{tensor_b.item()}: ', tensor_c.item())\n",
    "\n",
    "print(tensor_c.backward()) # realize that we do not need to pass a tensor to the backward function because the tensor_c is a scalar\n",
    "\n",
    "print(f'∂c / ∂a = e^{tensor_b.item()} : ',   tensor_a.grad) # this is partial derivative of tensor_c w.r.t tensor_a\n",
    "print(f'∂c / ∂b = {tensor_a.item()} * e^{tensor_b.item()} : ', tensor_b.grad) # this is partial derivative of tensor_c w.r.t tensor_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about nn module?\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3376, -0.2829,  0.3824],\n",
      "        [ 0.1842,  0.4232, -0.2334],\n",
      "        [ 0.4765, -0.2510,  0.3828],\n",
      "        [-0.0841,  0.0112, -0.3659]], requires_grad=True)\n",
      "torch.Size([4, 3])\n",
      "Parameter containing:\n",
      "tensor([ 0.4603,  0.1470, -0.0852,  0.1140], requires_grad=True)\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# we can create a linear layer\n",
    "linear = nn.Linear(3, 4)\n",
    "print(linear.weight)\n",
    "print(linear.weight.shape)\n",
    "print(linear.bias)\n",
    "print(linear.bias.shape)\n",
    "\n",
    "# realize that the dimensions of the weight matrix are (output_dim, input_dim)\n",
    "# and the bias is of size output_dim\n",
    "# the reason for this is that the output of the linear layer is given by y = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4901,  0.2869, -0.0297, -0.0403]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 4])\n",
      "tensor([[ 0.4901,  0.2869, -0.0297, -0.0403]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# let's do an input tensor for our linear layer\n",
    "tensor = torch.rand(1,3)\n",
    "\n",
    "output = linear(tensor)\n",
    "print(output)\n",
    "print(output.shape)\n",
    "\n",
    "# can we double check the output? YES!\n",
    "print(tensor.matmul(linear.weight.T) + linear.bias) # realize how we do a transpose to match the dimensions of the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create a sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 4),\n",
    "    nn.Linear(4, 5)\n",
    ") \n",
    "# realized that we need to match the output of the first layer to the input of the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2272,  0.2534, -0.4547, -0.6422,  0.2278]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.rand(1,3)\n",
    "output = model(input_tensor)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "tensor([[ 0.9340, -0.5364, -0.9925]])\n"
     ]
    }
   ],
   "source": [
    "# we can use gradient on the model\n",
    "input_tensor = torch.rand(1,3)\n",
    "input_tensor.requires_grad = True\n",
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "\n",
    "output.backward(torch.ones_like(output)) # the reason we pass torch.ones_like(output) is because we need to pass the gradient of the loss w.r.t the output\n",
    "\n",
    "print(input_tensor.grad) # the meaning of this is the gradient of the loss w.r.t the input tensor (which is the input to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  0.,  1.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([0.2689, 0.5000, 0.7311])\n",
      "tensor([-0.7616,  0.0000,  0.7616])\n"
     ]
    }
   ],
   "source": [
    "# we can also use activation functions in torch\n",
    "tensor_a = torch.tensor([-1.0, 0.0, 1.0])\n",
    "print(tensor_a)\n",
    "\n",
    "print(torch.relu(tensor_a))\n",
    "print(torch.sigmoid(tensor_a))\n",
    "print(torch.tanh(tensor_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a custom model and play with it we want this network to learn XOR\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(2, 5)\n",
    "        self.linear2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4685],\n",
      "        [0.4719],\n",
      "        [0.4585],\n",
      "        [0.4282]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we can now use a sample input, calculate a loss and maybe do some gradients\n",
    "# we can actually teach our model something simple\n",
    "# for this one we can try to learn XOR\n",
    "\n",
    "# let's create a dataset for XOR\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "model = CustomModel()\n",
    "criterion = nn.MSELoss() # we can use this criterion to calculate the loss\n",
    "# we need to pass the output of the model and the target where model outputs will be logits\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1) # we can use this optimizer to optimize the model\n",
    "\n",
    "output = model(X)\n",
    "print(output) \n",
    "# the model is almost random at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.24373997747898102\n",
      "Epoch 1 Loss 0.23806247115135193\n",
      "Epoch 2 Loss 0.2298159897327423\n",
      "Epoch 3 Loss 0.22642108798027039\n",
      "Epoch 4 Loss 0.22096151113510132\n",
      "Epoch 5 Loss 0.2113042026758194\n",
      "Epoch 6 Loss 0.19932299852371216\n",
      "Epoch 7 Loss 0.18724441528320312\n",
      "Epoch 8 Loss 0.17479342222213745\n",
      "Epoch 9 Loss 0.16263025999069214\n",
      "Epoch 10 Loss 0.14839157462120056\n",
      "Epoch 11 Loss 0.1390179842710495\n",
      "Epoch 12 Loss 0.1262802928686142\n",
      "Epoch 13 Loss 0.10997021943330765\n",
      "Epoch 14 Loss 0.09854793548583984\n",
      "Epoch 15 Loss 0.08865281939506531\n",
      "Epoch 16 Loss 0.0751839205622673\n",
      "Epoch 17 Loss 0.06583049148321152\n",
      "Epoch 18 Loss 0.057983964681625366\n",
      "Epoch 19 Loss 0.04977748170495033\n",
      "Epoch 20 Loss 0.04124902933835983\n",
      "Epoch 21 Loss 0.03405613452196121\n",
      "Epoch 22 Loss 0.02895057201385498\n",
      "Epoch 23 Loss 0.024521619081497192\n",
      "Epoch 24 Loss 0.019668281078338623\n",
      "Epoch 25 Loss 0.01581171154975891\n",
      "Epoch 26 Loss 0.013328375294804573\n",
      "Epoch 27 Loss 0.011249680072069168\n",
      "Epoch 28 Loss 0.009409968741238117\n",
      "Epoch 29 Loss 0.007882269099354744\n",
      "Epoch 30 Loss 0.006575504783540964\n",
      "Epoch 31 Loss 0.005510582588613033\n",
      "Epoch 32 Loss 0.00459368247538805\n",
      "Epoch 33 Loss 0.003817688673734665\n",
      "Epoch 34 Loss 0.003233626252040267\n",
      "Epoch 35 Loss 0.00282331183552742\n",
      "Epoch 36 Loss 0.0024742805399000645\n",
      "Epoch 37 Loss 0.002180526265874505\n",
      "Epoch 38 Loss 0.0019407357322052121\n",
      "Epoch 39 Loss 0.0017379901837557554\n",
      "Epoch 40 Loss 0.0015652176225557923\n",
      "Epoch 41 Loss 0.0014176217373460531\n",
      "Epoch 42 Loss 0.0012919869041070342\n",
      "Epoch 43 Loss 0.0011822889791801572\n",
      "Epoch 44 Loss 0.0010862565832212567\n",
      "Epoch 45 Loss 0.001002588076516986\n",
      "Epoch 46 Loss 0.0009292229078710079\n",
      "Epoch 47 Loss 0.0008643307955935597\n",
      "Epoch 48 Loss 0.0008066582959145308\n",
      "Epoch 49 Loss 0.0007562313694506884\n",
      "Epoch 50 Loss 0.0007112754974514246\n",
      "Epoch 51 Loss 0.0006709347944706678\n",
      "Epoch 52 Loss 0.0006347396411001682\n",
      "Epoch 53 Loss 0.0006028004572726786\n",
      "Epoch 54 Loss 0.0005739399348385632\n",
      "Epoch 55 Loss 0.0005479644751176238\n",
      "Epoch 56 Loss 0.0005247939843684435\n",
      "Epoch 57 Loss 0.0005034963833168149\n",
      "Epoch 58 Loss 0.00048407670692540705\n",
      "Epoch 59 Loss 0.0004665711021516472\n",
      "Epoch 60 Loss 0.0004504651587922126\n",
      "Epoch 61 Loss 0.0004357554134912789\n",
      "Epoch 62 Loss 0.0004222457646392286\n",
      "Epoch 63 Loss 0.00041008141124621034\n",
      "Epoch 64 Loss 0.00039880682015791535\n",
      "Epoch 65 Loss 0.0003883232129737735\n",
      "Epoch 66 Loss 0.0003785610315389931\n",
      "Epoch 67 Loss 0.00036945167812518775\n",
      "Epoch 68 Loss 0.0003614976885728538\n",
      "Epoch 69 Loss 0.00035382132045924664\n",
      "Epoch 70 Loss 0.0003474699333310127\n",
      "Epoch 71 Loss 0.0003413176746107638\n",
      "Epoch 72 Loss 0.0003355457738507539\n",
      "Epoch 73 Loss 0.0003297970979474485\n",
      "Epoch 74 Loss 0.00032410037238150835\n",
      "Epoch 75 Loss 0.00031848635990172625\n",
      "Epoch 76 Loss 0.0003129765682388097\n",
      "Epoch 77 Loss 0.00030833962955512106\n",
      "Epoch 78 Loss 0.0003053832333534956\n",
      "Epoch 79 Loss 0.000302055268548429\n",
      "Epoch 80 Loss 0.0002985487808473408\n",
      "Epoch 81 Loss 0.000295159756205976\n",
      "Epoch 82 Loss 0.00029147573513910174\n",
      "Epoch 83 Loss 0.0002884439309127629\n",
      "Epoch 84 Loss 0.00028539213235490024\n",
      "Epoch 85 Loss 0.0002822487149387598\n",
      "Epoch 86 Loss 0.0002794972388073802\n",
      "Epoch 87 Loss 0.00027661205967888236\n",
      "Epoch 88 Loss 0.0002735833404585719\n",
      "Epoch 89 Loss 0.00027043011505156755\n",
      "Epoch 90 Loss 0.0002672388218343258\n",
      "Epoch 91 Loss 0.000264542642980814\n",
      "Epoch 92 Loss 0.00026206564507447183\n",
      "Epoch 93 Loss 0.00025945171364583075\n",
      "Epoch 94 Loss 0.00025671927141956985\n",
      "Epoch 95 Loss 0.00025400970480404794\n",
      "Epoch 96 Loss 0.0002520435082260519\n",
      "Epoch 97 Loss 0.0002499483525753021\n",
      "Epoch 98 Loss 0.0002472788910381496\n",
      "Epoch 99 Loss 0.0002449376042932272\n",
      "Epoch 100 Loss 0.00024292690795846283\n",
      "Epoch 101 Loss 0.00024087794008664787\n",
      "Epoch 102 Loss 0.00023867745767347515\n",
      "Epoch 103 Loss 0.00023676041746512055\n",
      "Epoch 104 Loss 0.00023471108579542488\n",
      "Epoch 105 Loss 0.00023274807608686388\n",
      "Epoch 106 Loss 0.00023082300322130322\n",
      "Epoch 107 Loss 0.00022893684217706323\n",
      "Epoch 108 Loss 0.00022690177138429135\n",
      "Epoch 109 Loss 0.00022495369194075465\n",
      "Epoch 110 Loss 0.00022302601428236812\n",
      "Epoch 111 Loss 0.0002211267565144226\n",
      "Epoch 112 Loss 0.00021925664623267949\n",
      "Epoch 113 Loss 0.0002174628316424787\n",
      "Epoch 114 Loss 0.00021581011242233217\n",
      "Epoch 115 Loss 0.00021419311815407127\n",
      "Epoch 116 Loss 0.0002123592421412468\n",
      "Epoch 117 Loss 0.000210647500352934\n",
      "Epoch 118 Loss 0.00020905709243379533\n",
      "Epoch 119 Loss 0.00020746955124195665\n",
      "Epoch 120 Loss 0.00020583234436344355\n",
      "Epoch 121 Loss 0.0002041419065790251\n",
      "Epoch 122 Loss 0.00020245308405719697\n",
      "Epoch 123 Loss 0.00020099320681765676\n",
      "Epoch 124 Loss 0.0001993719197344035\n",
      "Epoch 125 Loss 0.00019774014072027057\n",
      "Epoch 126 Loss 0.00019627137226052582\n",
      "Epoch 127 Loss 0.00019477715250104666\n",
      "Epoch 128 Loss 0.00019328639609739184\n",
      "Epoch 129 Loss 0.00019182762480340898\n",
      "Epoch 130 Loss 0.00019023293862119317\n",
      "Epoch 131 Loss 0.00018891367653850466\n",
      "Epoch 132 Loss 0.00018744768749456853\n",
      "Epoch 133 Loss 0.00018594443099573255\n",
      "Epoch 134 Loss 0.00018470038776285946\n",
      "Epoch 135 Loss 0.00018335592176299542\n",
      "Epoch 136 Loss 0.0001820418401621282\n",
      "Epoch 137 Loss 0.00018066787743009627\n",
      "Epoch 138 Loss 0.0001793471456039697\n",
      "Epoch 139 Loss 0.00017798092449083924\n",
      "Epoch 140 Loss 0.00017651449888944626\n",
      "Epoch 141 Loss 0.00017534934158902615\n",
      "Epoch 142 Loss 0.00017423789540771395\n",
      "Epoch 143 Loss 0.00017271400429308414\n",
      "Epoch 144 Loss 0.00017151211795862764\n",
      "Epoch 145 Loss 0.0001702992303762585\n",
      "Epoch 146 Loss 0.00016904996300581843\n",
      "Epoch 147 Loss 0.00016779176075942814\n",
      "Epoch 148 Loss 0.00016652655904181302\n",
      "Epoch 149 Loss 0.00016524239617865533\n",
      "Epoch 150 Loss 0.00016426837828475982\n",
      "Epoch 151 Loss 0.00016313149535562843\n",
      "Epoch 152 Loss 0.00016185108688659966\n",
      "Epoch 153 Loss 0.00016078949556685984\n",
      "Epoch 154 Loss 0.00015972706023603678\n",
      "Epoch 155 Loss 0.00015860970597714186\n",
      "Epoch 156 Loss 0.00015750296006444842\n",
      "Epoch 157 Loss 0.00015631376300007105\n",
      "Epoch 158 Loss 0.00015519345470238477\n",
      "Epoch 159 Loss 0.00015414535300806165\n",
      "Epoch 160 Loss 0.00015305160195566714\n",
      "Epoch 161 Loss 0.00015194811567198485\n",
      "Epoch 162 Loss 0.00015088723739609122\n",
      "Epoch 163 Loss 0.00014995764649938792\n",
      "Epoch 164 Loss 0.00014896159700583667\n",
      "Epoch 165 Loss 0.0001477975893067196\n",
      "Epoch 166 Loss 0.00014674989506602287\n",
      "Epoch 167 Loss 0.00014593658852390945\n",
      "Epoch 168 Loss 0.00014494780043605715\n",
      "Epoch 169 Loss 0.00014387996634468436\n",
      "Epoch 170 Loss 0.00014293359708972275\n",
      "Epoch 171 Loss 0.00014194569666869938\n",
      "Epoch 172 Loss 0.00014102419663686305\n",
      "Epoch 173 Loss 0.00014005761477164924\n",
      "Epoch 174 Loss 0.00013904621300753206\n",
      "Epoch 175 Loss 0.000138130591949448\n",
      "Epoch 176 Loss 0.00013716424291487783\n",
      "Epoch 177 Loss 0.00013629187014885247\n",
      "Epoch 178 Loss 0.00013543230306822807\n",
      "Epoch 179 Loss 0.00013459011097438633\n",
      "Epoch 180 Loss 0.00013372267130762339\n",
      "Epoch 181 Loss 0.00013284661690704525\n",
      "Epoch 182 Loss 0.0001319530128967017\n",
      "Epoch 183 Loss 0.0001310712832491845\n",
      "Epoch 184 Loss 0.0001301650336245075\n",
      "Epoch 185 Loss 0.00012927119678352028\n",
      "Epoch 186 Loss 0.00012843440345022827\n",
      "Epoch 187 Loss 0.00012762237747665495\n",
      "Epoch 188 Loss 0.00012681943189818412\n",
      "Epoch 189 Loss 0.00012602913193404675\n",
      "Epoch 190 Loss 0.00012524066551122814\n",
      "Epoch 191 Loss 0.0001244230370502919\n",
      "Epoch 192 Loss 0.0001236045645782724\n",
      "Epoch 193 Loss 0.00012276938650757074\n",
      "Epoch 194 Loss 0.00012213096488267183\n",
      "Epoch 195 Loss 0.00012128398520871997\n",
      "Epoch 196 Loss 0.0001204987638629973\n",
      "Epoch 197 Loss 0.00011980050476267934\n",
      "Epoch 198 Loss 0.00011902472760993987\n",
      "Epoch 199 Loss 0.00011826871195808053\n",
      "Epoch 200 Loss 0.00011762042413465679\n",
      "Epoch 201 Loss 0.00011687868391163647\n",
      "Epoch 202 Loss 0.00011612493835855275\n",
      "Epoch 203 Loss 0.00011542530410224572\n",
      "Epoch 204 Loss 0.00011476026702439412\n",
      "Epoch 205 Loss 0.00011402468953747302\n",
      "Epoch 206 Loss 0.00011332832946209237\n",
      "Epoch 207 Loss 0.00011264628847129643\n",
      "Epoch 208 Loss 0.00011192596139153466\n",
      "Epoch 209 Loss 0.0001112424215534702\n",
      "Epoch 210 Loss 0.00011055770301027223\n",
      "Epoch 211 Loss 0.00010990396549459547\n",
      "Epoch 212 Loss 0.00010925928654614836\n",
      "Epoch 213 Loss 0.00010861754708457738\n",
      "Epoch 214 Loss 0.00010805240162881091\n",
      "Epoch 215 Loss 0.00010735323303379118\n",
      "Epoch 216 Loss 0.00010672490316210315\n",
      "Epoch 217 Loss 0.0001061146758729592\n",
      "Epoch 218 Loss 0.00010549819853622466\n",
      "Epoch 219 Loss 0.00010488404222996905\n",
      "Epoch 220 Loss 0.00010424942593090236\n",
      "Epoch 221 Loss 0.0001036753601511009\n",
      "Epoch 222 Loss 0.00010307487536920235\n",
      "Epoch 223 Loss 0.00010242951975669712\n",
      "Epoch 224 Loss 0.00010181146353716031\n",
      "Epoch 225 Loss 0.00010119461512658745\n",
      "Epoch 226 Loss 0.00010059542546514422\n",
      "Epoch 227 Loss 0.00010010978439822793\n",
      "Epoch 228 Loss 9.950666571967304e-05\n",
      "Epoch 229 Loss 9.896115807350725e-05\n",
      "Epoch 230 Loss 9.840562415774912e-05\n",
      "Epoch 231 Loss 9.782487904885784e-05\n",
      "Epoch 232 Loss 9.725124982651323e-05\n",
      "Epoch 233 Loss 9.672055603004992e-05\n",
      "Epoch 234 Loss 9.61326586548239e-05\n",
      "Epoch 235 Loss 9.558131569065154e-05\n",
      "Epoch 236 Loss 9.50683606788516e-05\n",
      "Epoch 237 Loss 9.4548289780505e-05\n",
      "Epoch 238 Loss 9.401906572747976e-05\n",
      "Epoch 239 Loss 9.34908093768172e-05\n",
      "Epoch 240 Loss 9.298414806835353e-05\n",
      "Epoch 241 Loss 9.243291424354538e-05\n",
      "Epoch 242 Loss 9.194572339765728e-05\n",
      "Epoch 243 Loss 9.144641808234155e-05\n",
      "Epoch 244 Loss 9.093590051634237e-05\n",
      "Epoch 245 Loss 9.045170736499131e-05\n",
      "Epoch 246 Loss 8.994920062832534e-05\n",
      "Epoch 247 Loss 8.944922592490911e-05\n",
      "Epoch 248 Loss 8.896569488570094e-05\n",
      "Epoch 249 Loss 8.845315460348502e-05\n",
      "Epoch 250 Loss 8.796941256150603e-05\n",
      "Epoch 251 Loss 8.757473551668227e-05\n",
      "Epoch 252 Loss 8.707486267667264e-05\n",
      "Epoch 253 Loss 8.65765759954229e-05\n",
      "Epoch 254 Loss 8.612564124632627e-05\n",
      "Epoch 255 Loss 8.567467011744156e-05\n",
      "Epoch 256 Loss 8.520243864040822e-05\n",
      "Epoch 257 Loss 8.475466165691614e-05\n",
      "Epoch 258 Loss 8.43060843180865e-05\n",
      "Epoch 259 Loss 8.382995292777196e-05\n",
      "Epoch 260 Loss 8.339488704223186e-05\n",
      "Epoch 261 Loss 8.294808503706008e-05\n",
      "Epoch 262 Loss 8.252324187196791e-05\n",
      "Epoch 263 Loss 8.210314263124019e-05\n",
      "Epoch 264 Loss 8.16717729321681e-05\n",
      "Epoch 265 Loss 8.123959560180083e-05\n",
      "Epoch 266 Loss 8.08284676168114e-05\n",
      "Epoch 267 Loss 8.039585372898728e-05\n",
      "Epoch 268 Loss 8.003500261111185e-05\n",
      "Epoch 269 Loss 7.963149255374447e-05\n",
      "Epoch 270 Loss 7.925173122202978e-05\n",
      "Epoch 271 Loss 7.885189552325755e-05\n",
      "Epoch 272 Loss 7.843396451789886e-05\n",
      "Epoch 273 Loss 7.801926403772086e-05\n",
      "Epoch 274 Loss 7.76494707679376e-05\n",
      "Epoch 275 Loss 7.723120506852865e-05\n",
      "Epoch 276 Loss 7.680342241656035e-05\n",
      "Epoch 277 Loss 7.642861601198092e-05\n",
      "Epoch 278 Loss 7.60489419917576e-05\n",
      "Epoch 279 Loss 7.566802378278226e-05\n",
      "Epoch 280 Loss 7.530247967224568e-05\n",
      "Epoch 281 Loss 7.491580618079752e-05\n",
      "Epoch 282 Loss 7.454621663782746e-05\n",
      "Epoch 283 Loss 7.415938307531178e-05\n",
      "Epoch 284 Loss 7.378803275059909e-05\n",
      "Epoch 285 Loss 7.34226850909181e-05\n",
      "Epoch 286 Loss 7.306162297027186e-05\n",
      "Epoch 287 Loss 7.271674985531718e-05\n",
      "Epoch 288 Loss 7.236685632960871e-05\n",
      "Epoch 289 Loss 7.200537947937846e-05\n",
      "Epoch 290 Loss 7.165120041463524e-05\n",
      "Epoch 291 Loss 7.130084850359708e-05\n",
      "Epoch 292 Loss 7.095396722434089e-05\n",
      "Epoch 293 Loss 7.061116048134863e-05\n",
      "Epoch 294 Loss 7.027517858659849e-05\n",
      "Epoch 295 Loss 6.99467200320214e-05\n",
      "Epoch 296 Loss 6.956940342206508e-05\n",
      "Epoch 297 Loss 6.927025970071554e-05\n",
      "Epoch 298 Loss 6.89560329192318e-05\n",
      "Epoch 299 Loss 6.864778697490692e-05\n",
      "Epoch 300 Loss 6.832612416474149e-05\n",
      "Epoch 301 Loss 6.799336551921442e-05\n",
      "Epoch 302 Loss 6.768928142264485e-05\n",
      "Epoch 303 Loss 6.735313218086958e-05\n",
      "Epoch 304 Loss 6.701215170323849e-05\n",
      "Epoch 305 Loss 6.6688509832602e-05\n",
      "Epoch 306 Loss 6.636060425080359e-05\n",
      "Epoch 307 Loss 6.605523230973631e-05\n",
      "Epoch 308 Loss 6.575793668162078e-05\n",
      "Epoch 309 Loss 6.543572817463428e-05\n",
      "Epoch 310 Loss 6.51346636004746e-05\n",
      "Epoch 311 Loss 6.482079334091395e-05\n",
      "Epoch 312 Loss 6.451096851378679e-05\n",
      "Epoch 313 Loss 6.425007450161502e-05\n",
      "Epoch 314 Loss 6.396080425474793e-05\n",
      "Epoch 315 Loss 6.36410914012231e-05\n",
      "Epoch 316 Loss 6.337590457405895e-05\n",
      "Epoch 317 Loss 6.309480522759259e-05\n",
      "Epoch 318 Loss 6.279884837567806e-05\n",
      "Epoch 319 Loss 6.252470484469086e-05\n",
      "Epoch 320 Loss 6.222206866368651e-05\n",
      "Epoch 321 Loss 6.195188325364143e-05\n",
      "Epoch 322 Loss 6.169098196551204e-05\n",
      "Epoch 323 Loss 6.140924961073324e-05\n",
      "Epoch 324 Loss 6.110303365858272e-05\n",
      "Epoch 325 Loss 6.083039625082165e-05\n",
      "Epoch 326 Loss 6.0554652009159327e-05\n",
      "Epoch 327 Loss 6.0289276007097214e-05\n",
      "Epoch 328 Loss 6.0012927860952914e-05\n",
      "Epoch 329 Loss 5.974478699499741e-05\n",
      "Epoch 330 Loss 5.9488047554623336e-05\n",
      "Epoch 331 Loss 5.9218204114586115e-05\n",
      "Epoch 332 Loss 5.895739741390571e-05\n",
      "Epoch 333 Loss 5.870466338819824e-05\n",
      "Epoch 334 Loss 5.842951941303909e-05\n",
      "Epoch 335 Loss 5.816272459924221e-05\n",
      "Epoch 336 Loss 5.7928151363739744e-05\n",
      "Epoch 337 Loss 5.766643153037876e-05\n",
      "Epoch 338 Loss 5.742869325331412e-05\n",
      "Epoch 339 Loss 5.719329783460125e-05\n",
      "Epoch 340 Loss 5.6947454140754417e-05\n",
      "Epoch 341 Loss 5.668875382980332e-05\n",
      "Epoch 342 Loss 5.642064934363589e-05\n",
      "Epoch 343 Loss 5.620674346573651e-05\n",
      "Epoch 344 Loss 5.5954064009711146e-05\n",
      "Epoch 345 Loss 5.56846171093639e-05\n",
      "Epoch 346 Loss 5.5480588343925774e-05\n",
      "Epoch 347 Loss 5.524420703295618e-05\n",
      "Epoch 348 Loss 5.50060867681168e-05\n",
      "Epoch 349 Loss 5.4773365263827145e-05\n",
      "Epoch 350 Loss 5.4536383686354384e-05\n",
      "Epoch 351 Loss 5.430335659184493e-05\n",
      "Epoch 352 Loss 5.405996125773527e-05\n",
      "Epoch 353 Loss 5.384979158407077e-05\n",
      "Epoch 354 Loss 5.3623210988007486e-05\n",
      "Epoch 355 Loss 5.3385778301162645e-05\n",
      "Epoch 356 Loss 5.3163970733294263e-05\n",
      "Epoch 357 Loss 5.293931462801993e-05\n",
      "Epoch 358 Loss 5.272385533316992e-05\n",
      "Epoch 359 Loss 5.249744208413176e-05\n",
      "Epoch 360 Loss 5.231285103945993e-05\n",
      "Epoch 361 Loss 5.20829125889577e-05\n",
      "Epoch 362 Loss 5.186188718653284e-05\n",
      "Epoch 363 Loss 5.1641734899021685e-05\n",
      "Epoch 364 Loss 5.1434948545647785e-05\n",
      "Epoch 365 Loss 5.122108632349409e-05\n",
      "Epoch 366 Loss 5.1012142648687586e-05\n",
      "Epoch 367 Loss 5.0789683882612735e-05\n",
      "Epoch 368 Loss 5.060371768195182e-05\n",
      "Epoch 369 Loss 5.040494943386875e-05\n",
      "Epoch 370 Loss 5.0198432290926576e-05\n",
      "Epoch 371 Loss 4.996576899429783e-05\n",
      "Epoch 372 Loss 4.976073250873014e-05\n",
      "Epoch 373 Loss 4.9582413339521736e-05\n",
      "Epoch 374 Loss 4.938519487041049e-05\n",
      "Epoch 375 Loss 4.917835758533329e-05\n",
      "Epoch 376 Loss 4.8996516852639616e-05\n",
      "Epoch 377 Loss 4.880001870333217e-05\n",
      "Epoch 378 Loss 4.857786188949831e-05\n",
      "Epoch 379 Loss 4.841019836021587e-05\n",
      "Epoch 380 Loss 4.820845788344741e-05\n",
      "Epoch 381 Loss 4.800860915565863e-05\n",
      "Epoch 382 Loss 4.782223186339252e-05\n",
      "Epoch 383 Loss 4.7632998757762834e-05\n",
      "Epoch 384 Loss 4.743523822980933e-05\n",
      "Epoch 385 Loss 4.725225880974904e-05\n",
      "Epoch 386 Loss 4.707190964836627e-05\n",
      "Epoch 387 Loss 4.6874174586264417e-05\n",
      "Epoch 388 Loss 4.668160909204744e-05\n",
      "Epoch 389 Loss 4.650066694011912e-05\n",
      "Epoch 390 Loss 4.631237607100047e-05\n",
      "Epoch 391 Loss 4.614672070601955e-05\n",
      "Epoch 392 Loss 4.5973451051395386e-05\n",
      "Epoch 393 Loss 4.578449807013385e-05\n",
      "Epoch 394 Loss 4.559735316433944e-05\n",
      "Epoch 395 Loss 4.5418964873533696e-05\n",
      "Epoch 396 Loss 4.525519034359604e-05\n",
      "Epoch 397 Loss 4.5081862481310964e-05\n",
      "Epoch 398 Loss 4.490410356083885e-05\n",
      "Epoch 399 Loss 4.4742577301803976e-05\n",
      "Epoch 400 Loss 4.456637179828249e-05\n",
      "Epoch 401 Loss 4.4378903112374246e-05\n",
      "Epoch 402 Loss 4.421025369083509e-05\n",
      "Epoch 403 Loss 4.4034692109562457e-05\n",
      "Epoch 404 Loss 4.3890839151572436e-05\n",
      "Epoch 405 Loss 4.3725645809900016e-05\n",
      "Epoch 406 Loss 4.3539148464333266e-05\n",
      "Epoch 407 Loss 4.3380900024203584e-05\n",
      "Epoch 408 Loss 4.3219530198257416e-05\n",
      "Epoch 409 Loss 4.3074338464066386e-05\n",
      "Epoch 410 Loss 4.2906598537229e-05\n",
      "Epoch 411 Loss 4.276109393686056e-05\n",
      "Epoch 412 Loss 4.259590423316695e-05\n",
      "Epoch 413 Loss 4.2446186853339896e-05\n",
      "Epoch 414 Loss 4.2289437260478735e-05\n",
      "Epoch 415 Loss 4.2124844185309485e-05\n",
      "Epoch 416 Loss 4.195298970444128e-05\n",
      "Epoch 417 Loss 4.180578252999112e-05\n",
      "Epoch 418 Loss 4.164694109931588e-05\n",
      "Epoch 419 Loss 4.1468345443718135e-05\n",
      "Epoch 420 Loss 4.133225593250245e-05\n",
      "Epoch 421 Loss 4.118258584639989e-05\n",
      "Epoch 422 Loss 4.1027215047506616e-05\n",
      "Epoch 423 Loss 4.088302011950873e-05\n",
      "Epoch 424 Loss 4.073410673299804e-05\n",
      "Epoch 425 Loss 4.0586535760667175e-05\n",
      "Epoch 426 Loss 4.042821456096135e-05\n",
      "Epoch 427 Loss 4.028253533761017e-05\n",
      "Epoch 428 Loss 4.014789737993851e-05\n",
      "Epoch 429 Loss 3.999898035544902e-05\n",
      "Epoch 430 Loss 3.985781586379744e-05\n",
      "Epoch 431 Loss 3.972104605054483e-05\n",
      "Epoch 432 Loss 3.9581624150741845e-05\n",
      "Epoch 433 Loss 3.943342744605616e-05\n",
      "Epoch 434 Loss 3.928423757315613e-05\n",
      "Epoch 435 Loss 3.9140908484114334e-05\n",
      "Epoch 436 Loss 3.900927185895853e-05\n",
      "Epoch 437 Loss 3.885807382175699e-05\n",
      "Epoch 438 Loss 3.8725407648598775e-05\n",
      "Epoch 439 Loss 3.859697244479321e-05\n",
      "Epoch 440 Loss 3.846480467473157e-05\n",
      "Epoch 441 Loss 3.8326044887071475e-05\n",
      "Epoch 442 Loss 3.8194673834368587e-05\n",
      "Epoch 443 Loss 3.8057063648011535e-05\n",
      "Epoch 444 Loss 3.790803748415783e-05\n",
      "Epoch 445 Loss 3.7766829336760566e-05\n",
      "Epoch 446 Loss 3.7656536733265966e-05\n",
      "Epoch 447 Loss 3.752019256353378e-05\n",
      "Epoch 448 Loss 3.7374949897639453e-05\n",
      "Epoch 449 Loss 3.7251709727570415e-05\n",
      "Epoch 450 Loss 3.712825127877295e-05\n",
      "Epoch 451 Loss 3.6997887946199626e-05\n",
      "Epoch 452 Loss 3.686153650050983e-05\n",
      "Epoch 453 Loss 3.6734651075676084e-05\n",
      "Epoch 454 Loss 3.6611636460293084e-05\n",
      "Epoch 455 Loss 3.647631092462689e-05\n",
      "Epoch 456 Loss 3.634741005953401e-05\n",
      "Epoch 457 Loss 3.6235076549928635e-05\n",
      "Epoch 458 Loss 3.6110373912379146e-05\n",
      "Epoch 459 Loss 3.598439070628956e-05\n",
      "Epoch 460 Loss 3.585792728699744e-05\n",
      "Epoch 461 Loss 3.5744575143326074e-05\n",
      "Epoch 462 Loss 3.5619857953861356e-05\n",
      "Epoch 463 Loss 3.548721724655479e-05\n",
      "Epoch 464 Loss 3.5378136090002954e-05\n",
      "Epoch 465 Loss 3.5251854569651186e-05\n",
      "Epoch 466 Loss 3.512419425533153e-05\n",
      "Epoch 467 Loss 3.5015698813367635e-05\n",
      "Epoch 468 Loss 3.4897013392765075e-05\n",
      "Epoch 469 Loss 3.4772485378198326e-05\n",
      "Epoch 470 Loss 3.466025009402074e-05\n",
      "Epoch 471 Loss 3.4538068575784564e-05\n",
      "Epoch 472 Loss 3.441475928411819e-05\n",
      "Epoch 473 Loss 3.43021129083354e-05\n",
      "Epoch 474 Loss 3.419944187044166e-05\n",
      "Epoch 475 Loss 3.408853444852866e-05\n",
      "Epoch 476 Loss 3.396286047063768e-05\n",
      "Epoch 477 Loss 3.38638637913391e-05\n",
      "Epoch 478 Loss 3.3754604373825714e-05\n",
      "Epoch 479 Loss 3.3638658351264894e-05\n",
      "Epoch 480 Loss 3.35262157022953e-05\n",
      "Epoch 481 Loss 3.342033960507251e-05\n",
      "Epoch 482 Loss 3.33028583554551e-05\n",
      "Epoch 483 Loss 3.319948154967278e-05\n",
      "Epoch 484 Loss 3.309533349238336e-05\n",
      "Epoch 485 Loss 3.2980620744638145e-05\n",
      "Epoch 486 Loss 3.286176797701046e-05\n",
      "Epoch 487 Loss 3.275476046837866e-05\n",
      "Epoch 488 Loss 3.265061241108924e-05\n",
      "Epoch 489 Loss 3.254846888012253e-05\n",
      "Epoch 490 Loss 3.2449665013700724e-05\n",
      "Epoch 491 Loss 3.2343676139134914e-05\n",
      "Epoch 492 Loss 3.223802559659816e-05\n",
      "Epoch 493 Loss 3.212942829122767e-05\n",
      "Epoch 494 Loss 3.2018378988141194e-05\n",
      "Epoch 495 Loss 3.191160794813186e-05\n",
      "Epoch 496 Loss 3.180778367095627e-05\n",
      "Epoch 497 Loss 3.1706815207144246e-05\n",
      "Epoch 498 Loss 3.161572749377228e-05\n",
      "Epoch 499 Loss 3.151746932417154e-05\n",
      "Epoch 500 Loss 3.141006891382858e-05\n",
      "Epoch 501 Loss 3.130074037471786e-05\n",
      "Epoch 502 Loss 3.120381006738171e-05\n",
      "Epoch 503 Loss 3.110723628196865e-05\n",
      "Epoch 504 Loss 3.100588946836069e-05\n",
      "Epoch 505 Loss 3.091053804382682e-05\n",
      "Epoch 506 Loss 3.080819078604691e-05\n",
      "Epoch 507 Loss 3.070645470870659e-05\n",
      "Epoch 508 Loss 3.0609087843913585e-05\n",
      "Epoch 509 Loss 3.0512892408296466e-05\n",
      "Epoch 510 Loss 3.0428567697526887e-05\n",
      "Epoch 511 Loss 3.0332172173075378e-05\n",
      "Epoch 512 Loss 3.0230879929149523e-05\n",
      "Epoch 513 Loss 3.0138649890432134e-05\n",
      "Epoch 514 Loss 3.004918289661873e-05\n",
      "Epoch 515 Loss 2.9960145184304565e-05\n",
      "Epoch 516 Loss 2.985911851283163e-05\n",
      "Epoch 517 Loss 2.9763281418127008e-05\n",
      "Epoch 518 Loss 2.9676139092771336e-05\n",
      "Epoch 519 Loss 2.9579881811514497e-05\n",
      "Epoch 520 Loss 2.9486855055438355e-05\n",
      "Epoch 521 Loss 2.9394675948424265e-05\n",
      "Epoch 522 Loss 2.9310311219887808e-05\n",
      "Epoch 523 Loss 2.9223250749055296e-05\n",
      "Epoch 524 Loss 2.9128445021342486e-05\n",
      "Epoch 525 Loss 2.9035521947662346e-05\n",
      "Epoch 526 Loss 2.8943788493052125e-05\n",
      "Epoch 527 Loss 2.8856327844550833e-05\n",
      "Epoch 528 Loss 2.8775590180885047e-05\n",
      "Epoch 529 Loss 2.868442243197933e-05\n",
      "Epoch 530 Loss 2.859930827980861e-05\n",
      "Epoch 531 Loss 2.8509799449238926e-05\n",
      "Epoch 532 Loss 2.842610774678178e-05\n",
      "Epoch 533 Loss 2.8345899409032427e-05\n",
      "Epoch 534 Loss 2.8249716706341133e-05\n",
      "Epoch 535 Loss 2.8161175578134134e-05\n",
      "Epoch 536 Loss 2.8081380150979385e-05\n",
      "Epoch 537 Loss 2.7993575713480823e-05\n",
      "Epoch 538 Loss 2.7906153263757005e-05\n",
      "Epoch 539 Loss 2.7832820705953054e-05\n",
      "Epoch 540 Loss 2.7753820177167654e-05\n",
      "Epoch 541 Loss 2.765305180219002e-05\n",
      "Epoch 542 Loss 2.7582806069403887e-05\n",
      "Epoch 543 Loss 2.7505369871505536e-05\n",
      "Epoch 544 Loss 2.7419910111348145e-05\n",
      "Epoch 545 Loss 2.733983455982525e-05\n",
      "Epoch 546 Loss 2.7255588065600023e-05\n",
      "Epoch 547 Loss 2.7171558031113818e-05\n",
      "Epoch 548 Loss 2.709370164666325e-05\n",
      "Epoch 549 Loss 2.70143173111137e-05\n",
      "Epoch 550 Loss 2.6930014428216964e-05\n",
      "Epoch 551 Loss 2.685306935745757e-05\n",
      "Epoch 552 Loss 2.6773352146847174e-05\n",
      "Epoch 553 Loss 2.6689709557103924e-05\n",
      "Epoch 554 Loss 2.6618738047545776e-05\n",
      "Epoch 555 Loss 2.6538189558777958e-05\n",
      "Epoch 556 Loss 2.645930726430379e-05\n",
      "Epoch 557 Loss 2.6387369871372357e-05\n",
      "Epoch 558 Loss 2.6310444809496403e-05\n",
      "Epoch 559 Loss 2.6230074581690133e-05\n",
      "Epoch 560 Loss 2.615444827824831e-05\n",
      "Epoch 561 Loss 2.6071598767885007e-05\n",
      "Epoch 562 Loss 2.599034814920742e-05\n",
      "Epoch 563 Loss 2.5916999220498838e-05\n",
      "Epoch 564 Loss 2.584537651273422e-05\n",
      "Epoch 565 Loss 2.5770774300326593e-05\n",
      "Epoch 566 Loss 2.569866228441242e-05\n",
      "Epoch 567 Loss 2.5621829990996048e-05\n",
      "Epoch 568 Loss 2.554729871917516e-05\n",
      "Epoch 569 Loss 2.5472116249147803e-05\n",
      "Epoch 570 Loss 2.539931119827088e-05\n",
      "Epoch 571 Loss 2.5324150556116365e-05\n",
      "Epoch 572 Loss 2.525468880776316e-05\n",
      "Epoch 573 Loss 2.518014662200585e-05\n",
      "Epoch 574 Loss 2.5114204618148506e-05\n",
      "Epoch 575 Loss 2.5045046641025692e-05\n",
      "Epoch 576 Loss 2.497526293154806e-05\n",
      "Epoch 577 Loss 2.4902765289880335e-05\n",
      "Epoch 578 Loss 2.4827400920912623e-05\n",
      "Epoch 579 Loss 2.4756183847784996e-05\n",
      "Epoch 580 Loss 2.4690945792826824e-05\n",
      "Epoch 581 Loss 2.4615561414975673e-05\n",
      "Epoch 582 Loss 2.4550059606553987e-05\n",
      "Epoch 583 Loss 2.4477471015416086e-05\n",
      "Epoch 584 Loss 2.440919342916459e-05\n",
      "Epoch 585 Loss 2.4345092242583632e-05\n",
      "Epoch 586 Loss 2.427230720059015e-05\n",
      "Epoch 587 Loss 2.420450618956238e-05\n",
      "Epoch 588 Loss 2.4141374524333514e-05\n",
      "Epoch 589 Loss 2.4069575374596752e-05\n",
      "Epoch 590 Loss 2.40076315094484e-05\n",
      "Epoch 591 Loss 2.394314287812449e-05\n",
      "Epoch 592 Loss 2.38788707065396e-05\n",
      "Epoch 593 Loss 2.3811102437321097e-05\n",
      "Epoch 594 Loss 2.3739861717331223e-05\n",
      "Epoch 595 Loss 2.3667904315516353e-05\n",
      "Epoch 596 Loss 2.360986763960682e-05\n",
      "Epoch 597 Loss 2.355460310354829e-05\n",
      "Epoch 598 Loss 2.348677662666887e-05\n",
      "Epoch 599 Loss 2.3415315808961168e-05\n",
      "Epoch 600 Loss 2.335462522751186e-05\n",
      "Epoch 601 Loss 2.329086601093877e-05\n",
      "Epoch 602 Loss 2.3228976715472527e-05\n",
      "Epoch 603 Loss 2.3163604055298492e-05\n",
      "Epoch 604 Loss 2.3093023628462106e-05\n",
      "Epoch 605 Loss 2.3028253053780645e-05\n",
      "Epoch 606 Loss 2.2971085854806006e-05\n",
      "Epoch 607 Loss 2.2912388885742985e-05\n",
      "Epoch 608 Loss 2.284440415678546e-05\n",
      "Epoch 609 Loss 2.2785003238823265e-05\n",
      "Epoch 610 Loss 2.2722091671312228e-05\n",
      "Epoch 611 Loss 2.265659350086935e-05\n",
      "Epoch 612 Loss 2.2598145733354613e-05\n",
      "Epoch 613 Loss 2.2543568775290623e-05\n",
      "Epoch 614 Loss 2.2468047973234206e-05\n",
      "Epoch 615 Loss 2.2414860723074526e-05\n",
      "Epoch 616 Loss 2.235874126199633e-05\n",
      "Epoch 617 Loss 2.2299273041426204e-05\n",
      "Epoch 618 Loss 2.2239777536015026e-05\n",
      "Epoch 619 Loss 2.217653309344314e-05\n",
      "Epoch 620 Loss 2.211857463407796e-05\n",
      "Epoch 621 Loss 2.2060185074224137e-05\n",
      "Epoch 622 Loss 2.2003136109560728e-05\n",
      "Epoch 623 Loss 2.1943771571386606e-05\n",
      "Epoch 624 Loss 2.1883919544052333e-05\n",
      "Epoch 625 Loss 2.1824936993652955e-05\n",
      "Epoch 626 Loss 2.1773284970549867e-05\n",
      "Epoch 627 Loss 2.171731466660276e-05\n",
      "Epoch 628 Loss 2.165397017961368e-05\n",
      "Epoch 629 Loss 2.1592435587081127e-05\n",
      "Epoch 630 Loss 2.1541516616707668e-05\n",
      "Epoch 631 Loss 2.148202111129649e-05\n",
      "Epoch 632 Loss 2.1426470993901603e-05\n",
      "Epoch 633 Loss 2.13733746932121e-05\n",
      "Epoch 634 Loss 2.1319630832294933e-05\n",
      "Epoch 635 Loss 2.1262299924273975e-05\n",
      "Epoch 636 Loss 2.1204665245022625e-05\n",
      "Epoch 637 Loss 2.114438211719971e-05\n",
      "Epoch 638 Loss 2.1093128452776e-05\n",
      "Epoch 639 Loss 2.103579754475504e-05\n",
      "Epoch 640 Loss 2.0974415747332387e-05\n",
      "Epoch 641 Loss 2.092285649268888e-05\n",
      "Epoch 642 Loss 2.08730562007986e-05\n",
      "Epoch 643 Loss 2.0821451471420005e-05\n",
      "Epoch 644 Loss 2.0767660316778347e-05\n",
      "Epoch 645 Loss 2.0715466234833002e-05\n",
      "Epoch 646 Loss 2.065895932901185e-05\n",
      "Epoch 647 Loss 2.0613868400687352e-05\n",
      "Epoch 648 Loss 2.055895674857311e-05\n",
      "Epoch 649 Loss 2.0501243852777407e-05\n",
      "Epoch 650 Loss 2.0449635485420004e-05\n",
      "Epoch 651 Loss 2.0395315004861914e-05\n",
      "Epoch 652 Loss 2.0345447410363704e-05\n",
      "Epoch 653 Loss 2.0289853637223132e-05\n",
      "Epoch 654 Loss 2.0233452232787386e-05\n",
      "Epoch 655 Loss 2.0185498215141706e-05\n",
      "Epoch 656 Loss 2.0134086298639886e-05\n",
      "Epoch 657 Loss 2.0084604329895228e-05\n",
      "Epoch 658 Loss 2.0035155102959834e-05\n",
      "Epoch 659 Loss 1.9989962311228737e-05\n",
      "Epoch 660 Loss 1.9940533093176782e-05\n",
      "Epoch 661 Loss 1.9888066162820905e-05\n",
      "Epoch 662 Loss 1.98379439098062e-05\n",
      "Epoch 663 Loss 1.978521868295502e-05\n",
      "Epoch 664 Loss 1.9731065549422055e-05\n",
      "Epoch 665 Loss 1.9676643205457367e-05\n",
      "Epoch 666 Loss 1.963032627827488e-05\n",
      "Epoch 667 Loss 1.9578210412873887e-05\n",
      "Epoch 668 Loss 1.9526822143234313e-05\n",
      "Epoch 669 Loss 1.947969212778844e-05\n",
      "Epoch 670 Loss 1.9432027329457924e-05\n",
      "Epoch 671 Loss 1.9380679077585228e-05\n",
      "Epoch 672 Loss 1.9331837393110618e-05\n",
      "Epoch 673 Loss 1.9283599613117985e-05\n",
      "Epoch 674 Loss 1.9235485524404794e-05\n",
      "Epoch 675 Loss 1.9188839360140264e-05\n",
      "Epoch 676 Loss 1.9142942619509995e-05\n",
      "Epoch 677 Loss 1.909061575133819e-05\n",
      "Epoch 678 Loss 1.9047340174438432e-05\n",
      "Epoch 679 Loss 1.900150891742669e-05\n",
      "Epoch 680 Loss 1.8953534890897572e-05\n",
      "Epoch 681 Loss 1.891402280307375e-05\n",
      "Epoch 682 Loss 1.8867498511099257e-05\n",
      "Epoch 683 Loss 1.881571370176971e-05\n",
      "Epoch 684 Loss 1.8768587324302644e-05\n",
      "Epoch 685 Loss 1.8722132153925486e-05\n",
      "Epoch 686 Loss 1.8673772501642816e-05\n",
      "Epoch 687 Loss 1.8625843949848786e-05\n",
      "Epoch 688 Loss 1.8581906260806136e-05\n",
      "Epoch 689 Loss 1.853765206760727e-05\n",
      "Epoch 690 Loss 1.8495744370738976e-05\n",
      "Epoch 691 Loss 1.844714643084444e-05\n",
      "Epoch 692 Loss 1.8403519788989797e-05\n",
      "Epoch 693 Loss 1.8359975001658313e-05\n",
      "Epoch 694 Loss 1.8315791749046184e-05\n",
      "Epoch 695 Loss 1.8268898202222772e-05\n",
      "Epoch 696 Loss 1.822790909500327e-05\n",
      "Epoch 697 Loss 1.818412056309171e-05\n",
      "Epoch 698 Loss 1.813478047552053e-05\n",
      "Epoch 699 Loss 1.8090806406689808e-05\n",
      "Epoch 700 Loss 1.8045673641609028e-05\n",
      "Epoch 701 Loss 1.8006670870818198e-05\n",
      "Epoch 702 Loss 1.7962353012990206e-05\n",
      "Epoch 703 Loss 1.791975591913797e-05\n",
      "Epoch 704 Loss 1.7880080122267827e-05\n",
      "Epoch 705 Loss 1.783337393135298e-05\n",
      "Epoch 706 Loss 1.779185186023824e-05\n",
      "Epoch 707 Loss 1.7750040569808334e-05\n",
      "Epoch 708 Loss 1.7705471691442654e-05\n",
      "Epoch 709 Loss 1.766143395798281e-05\n",
      "Epoch 710 Loss 1.7624523025006056e-05\n",
      "Epoch 711 Loss 1.7582970031071454e-05\n",
      "Epoch 712 Loss 1.7535283404868096e-05\n",
      "Epoch 713 Loss 1.7495494830654934e-05\n",
      "Epoch 714 Loss 1.7454272892791778e-05\n",
      "Epoch 715 Loss 1.7415490219718777e-05\n",
      "Epoch 716 Loss 1.7373946320731193e-05\n",
      "Epoch 717 Loss 1.7332338757114485e-05\n",
      "Epoch 718 Loss 1.729529321892187e-05\n",
      "Epoch 719 Loss 1.7253560145036317e-05\n",
      "Epoch 720 Loss 1.7213478713529184e-05\n",
      "Epoch 721 Loss 1.717184750305023e-05\n",
      "Epoch 722 Loss 1.7131837012129836e-05\n",
      "Epoch 723 Loss 1.7090085748350248e-05\n",
      "Epoch 724 Loss 1.704628812149167e-05\n",
      "Epoch 725 Loss 1.70093171618646e-05\n",
      "Epoch 726 Loss 1.6975061953417026e-05\n",
      "Epoch 727 Loss 1.693057856755331e-05\n",
      "Epoch 728 Loss 1.688770680630114e-05\n",
      "Epoch 729 Loss 1.6850794054334983e-05\n",
      "Epoch 730 Loss 1.681296998867765e-05\n",
      "Epoch 731 Loss 1.6773599782027304e-05\n",
      "Epoch 732 Loss 1.673385122558102e-05\n",
      "Epoch 733 Loss 1.669307857810054e-05\n",
      "Epoch 734 Loss 1.6657340893289074e-05\n",
      "Epoch 735 Loss 1.661940223129932e-05\n",
      "Epoch 736 Loss 1.6576654161326587e-05\n",
      "Epoch 737 Loss 1.6535665054107085e-05\n",
      "Epoch 738 Loss 1.6501673599123023e-05\n",
      "Epoch 739 Loss 1.6467343812109903e-05\n",
      "Epoch 740 Loss 1.6430405594292097e-05\n",
      "Epoch 741 Loss 1.6393516489188187e-05\n",
      "Epoch 742 Loss 1.6353636965504847e-05\n",
      "Epoch 743 Loss 1.6319956557708792e-05\n",
      "Epoch 744 Loss 1.6283314835163765e-05\n",
      "Epoch 745 Loss 1.6242553101619706e-05\n",
      "Epoch 746 Loss 1.6203648556256667e-05\n",
      "Epoch 747 Loss 1.6164865883183666e-05\n",
      "Epoch 748 Loss 1.6125944966915995e-05\n",
      "Epoch 749 Loss 1.609419814485591e-05\n",
      "Epoch 750 Loss 1.6056736058089882e-05\n",
      "Epoch 751 Loss 1.6018093447200954e-05\n",
      "Epoch 752 Loss 1.598322887730319e-05\n",
      "Epoch 753 Loss 1.594790410308633e-05\n",
      "Epoch 754 Loss 1.5911216905806214e-05\n",
      "Epoch 755 Loss 1.5873978554736823e-05\n",
      "Epoch 756 Loss 1.583532321092207e-05\n",
      "Epoch 757 Loss 1.580097341502551e-05\n",
      "Epoch 758 Loss 1.5770128811709583e-05\n",
      "Epoch 759 Loss 1.5736222849227488e-05\n",
      "Epoch 760 Loss 1.5692115994170308e-05\n",
      "Epoch 761 Loss 1.5661908037145622e-05\n",
      "Epoch 762 Loss 1.5631156202289276e-05\n",
      "Epoch 763 Loss 1.5598303434671834e-05\n",
      "Epoch 764 Loss 1.556342249386944e-05\n",
      "Epoch 765 Loss 1.5530471500824206e-05\n",
      "Epoch 766 Loss 1.5495550542254932e-05\n",
      "Epoch 767 Loss 1.545773193356581e-05\n",
      "Epoch 768 Loss 1.541783058200963e-05\n",
      "Epoch 769 Loss 1.5383580830530263e-05\n",
      "Epoch 770 Loss 1.535155752208084e-05\n",
      "Epoch 771 Loss 1.5317835277528502e-05\n",
      "Epoch 772 Loss 1.528185748611577e-05\n",
      "Epoch 773 Loss 1.5241975233948324e-05\n",
      "Epoch 774 Loss 1.5209870980470441e-05\n",
      "Epoch 775 Loss 1.5177865861915052e-05\n",
      "Epoch 776 Loss 1.5144016288104467e-05\n",
      "Epoch 777 Loss 1.5112997971300501e-05\n",
      "Epoch 778 Loss 1.508341847511474e-05\n",
      "Epoch 779 Loss 1.5048624845803715e-05\n",
      "Epoch 780 Loss 1.501597034803126e-05\n",
      "Epoch 781 Loss 1.4982754692027811e-05\n",
      "Epoch 782 Loss 1.4950172044336796e-05\n",
      "Epoch 783 Loss 1.4918579836376011e-05\n",
      "Epoch 784 Loss 1.4884262782288715e-05\n",
      "Epoch 785 Loss 1.4848163118585944e-05\n",
      "Epoch 786 Loss 1.4818904674029909e-05\n",
      "Epoch 787 Loss 1.4785595340072177e-05\n",
      "Epoch 788 Loss 1.4747874956810847e-05\n",
      "Epoch 789 Loss 1.4713462405779865e-05\n",
      "Epoch 790 Loss 1.4684239431517199e-05\n",
      "Epoch 791 Loss 1.4653910511697177e-05\n",
      "Epoch 792 Loss 1.4623882634623442e-05\n",
      "Epoch 793 Loss 1.4592626939702313e-05\n",
      "Epoch 794 Loss 1.4562542673957068e-05\n",
      "Epoch 795 Loss 1.4531706256093457e-05\n",
      "Epoch 796 Loss 1.4498986274702474e-05\n",
      "Epoch 797 Loss 1.446870737709105e-05\n",
      "Epoch 798 Loss 1.4434632248594426e-05\n",
      "Epoch 799 Loss 1.44006589835044e-05\n",
      "Epoch 800 Loss 1.437086484656902e-05\n",
      "Epoch 801 Loss 1.4339008885144722e-05\n",
      "Epoch 802 Loss 1.4305742297437973e-05\n",
      "Epoch 803 Loss 1.4275748981162906e-05\n",
      "Epoch 804 Loss 1.4247634680941701e-05\n",
      "Epoch 805 Loss 1.4217205716704484e-05\n",
      "Epoch 806 Loss 1.4188039131113328e-05\n",
      "Epoch 807 Loss 1.415858969266992e-05\n",
      "Epoch 808 Loss 1.4127392205409706e-05\n",
      "Epoch 809 Loss 1.4098977771936916e-05\n",
      "Epoch 810 Loss 1.4068224118091166e-05\n",
      "Epoch 811 Loss 1.4037692380952649e-05\n",
      "Epoch 812 Loss 1.400638484483352e-05\n",
      "Epoch 813 Loss 1.3978396964375861e-05\n",
      "Epoch 814 Loss 1.3945817954663653e-05\n",
      "Epoch 815 Loss 1.3914239389123395e-05\n",
      "Epoch 816 Loss 1.3885785847378429e-05\n",
      "Epoch 817 Loss 1.3860330909665208e-05\n",
      "Epoch 818 Loss 1.3831025171384681e-05\n",
      "Epoch 819 Loss 1.3800496162730269e-05\n",
      "Epoch 820 Loss 1.3771201338386163e-05\n",
      "Epoch 821 Loss 1.3742601367994212e-05\n",
      "Epoch 822 Loss 1.371408870909363e-05\n",
      "Epoch 823 Loss 1.3684654732060153e-05\n",
      "Epoch 824 Loss 1.3654440408572555e-05\n",
      "Epoch 825 Loss 1.3624413440993521e-05\n",
      "Epoch 826 Loss 1.3600130841950886e-05\n",
      "Epoch 827 Loss 1.3574624972534366e-05\n",
      "Epoch 828 Loss 1.3540693544200622e-05\n",
      "Epoch 829 Loss 1.3510196367860772e-05\n",
      "Epoch 830 Loss 1.3484476767189335e-05\n",
      "Epoch 831 Loss 1.3455508451443166e-05\n",
      "Epoch 832 Loss 1.3427825251710601e-05\n",
      "Epoch 833 Loss 1.339952268608613e-05\n",
      "Epoch 834 Loss 1.3371906788961496e-05\n",
      "Epoch 835 Loss 1.3343269529286772e-05\n",
      "Epoch 836 Loss 1.3317241609911434e-05\n",
      "Epoch 837 Loss 1.3287519323057495e-05\n",
      "Epoch 838 Loss 1.3260249033919536e-05\n",
      "Epoch 839 Loss 1.3232221135695e-05\n",
      "Epoch 840 Loss 1.3207616575527936e-05\n",
      "Epoch 841 Loss 1.3180228052078746e-05\n",
      "Epoch 842 Loss 1.3151070561434608e-05\n",
      "Epoch 843 Loss 1.3124499673722312e-05\n",
      "Epoch 844 Loss 1.3097251212457195e-05\n",
      "Epoch 845 Loss 1.3068533007754013e-05\n",
      "Epoch 846 Loss 1.3044633305980824e-05\n",
      "Epoch 847 Loss 1.3017090168432333e-05\n",
      "Epoch 848 Loss 1.2988037269678898e-05\n",
      "Epoch 849 Loss 1.2963391782250255e-05\n",
      "Epoch 850 Loss 1.2938357031089254e-05\n",
      "Epoch 851 Loss 1.2912229976791423e-05\n",
      "Epoch 852 Loss 1.2884713214589283e-05\n",
      "Epoch 853 Loss 1.2861746654380113e-05\n",
      "Epoch 854 Loss 1.2834201697842218e-05\n",
      "Epoch 855 Loss 1.2807171515305527e-05\n",
      "Epoch 856 Loss 1.2781874829670414e-05\n",
      "Epoch 857 Loss 1.2755757779814303e-05\n",
      "Epoch 858 Loss 1.272909139515832e-05\n",
      "Epoch 859 Loss 1.2701751074928325e-05\n",
      "Epoch 860 Loss 1.267507832380943e-05\n",
      "Epoch 861 Loss 1.265159335162025e-05\n",
      "Epoch 862 Loss 1.2625556337297894e-05\n",
      "Epoch 863 Loss 1.2598899957083631e-05\n",
      "Epoch 864 Loss 1.2573969797813334e-05\n",
      "Epoch 865 Loss 1.2546879588626325e-05\n",
      "Epoch 866 Loss 1.252225320058642e-05\n",
      "Epoch 867 Loss 1.2498028809204698e-05\n",
      "Epoch 868 Loss 1.2472362868720666e-05\n",
      "Epoch 869 Loss 1.2449427231331356e-05\n",
      "Epoch 870 Loss 1.2425378372427076e-05\n",
      "Epoch 871 Loss 1.2401598723954521e-05\n",
      "Epoch 872 Loss 1.2376383892842568e-05\n",
      "Epoch 873 Loss 1.234909905178938e-05\n",
      "Epoch 874 Loss 1.2322676411713473e-05\n",
      "Epoch 875 Loss 1.2300864000280853e-05\n",
      "Epoch 876 Loss 1.2278819667699281e-05\n",
      "Epoch 877 Loss 1.2250191502971575e-05\n",
      "Epoch 878 Loss 1.2226319086039439e-05\n",
      "Epoch 879 Loss 1.2203307960589882e-05\n",
      "Epoch 880 Loss 1.2179412806290202e-05\n",
      "Epoch 881 Loss 1.2154281648690812e-05\n",
      "Epoch 882 Loss 1.2130817594879773e-05\n",
      "Epoch 883 Loss 1.2109379895264283e-05\n",
      "Epoch 884 Loss 1.208524736284744e-05\n",
      "Epoch 885 Loss 1.2059153050358873e-05\n",
      "Epoch 886 Loss 1.2035248801112175e-05\n",
      "Epoch 887 Loss 1.2011261787847616e-05\n",
      "Epoch 888 Loss 1.1990956409135833e-05\n",
      "Epoch 889 Loss 1.1964985787926707e-05\n",
      "Epoch 890 Loss 1.1940561307710595e-05\n",
      "Epoch 891 Loss 1.1919430107809603e-05\n",
      "Epoch 892 Loss 1.189496379083721e-05\n",
      "Epoch 893 Loss 1.1871741662616841e-05\n",
      "Epoch 894 Loss 1.1847094356198795e-05\n",
      "Epoch 895 Loss 1.1822734450106509e-05\n",
      "Epoch 896 Loss 1.1797667866630945e-05\n",
      "Epoch 897 Loss 1.1777405234170146e-05\n",
      "Epoch 898 Loss 1.1753835678973701e-05\n",
      "Epoch 899 Loss 1.1729505786206573e-05\n",
      "Epoch 900 Loss 1.1707441444741562e-05\n",
      "Epoch 901 Loss 1.168482231150847e-05\n",
      "Epoch 902 Loss 1.1661388271022588e-05\n",
      "Epoch 903 Loss 1.163791966973804e-05\n",
      "Epoch 904 Loss 1.1617433301580604e-05\n",
      "Epoch 905 Loss 1.1592148439376615e-05\n",
      "Epoch 906 Loss 1.1572218681976665e-05\n",
      "Epoch 907 Loss 1.1549792361620348e-05\n",
      "Epoch 908 Loss 1.1525692571012769e-05\n",
      "Epoch 909 Loss 1.1504023859743029e-05\n",
      "Epoch 910 Loss 1.148201226897072e-05\n",
      "Epoch 911 Loss 1.1459793313406408e-05\n",
      "Epoch 912 Loss 1.1438631190685555e-05\n",
      "Epoch 913 Loss 1.1418163012422156e-05\n",
      "Epoch 914 Loss 1.1396306035749149e-05\n",
      "Epoch 915 Loss 1.137394519901136e-05\n",
      "Epoch 916 Loss 1.1353268746461254e-05\n",
      "Epoch 917 Loss 1.1330301276757382e-05\n",
      "Epoch 918 Loss 1.1308750799798872e-05\n",
      "Epoch 919 Loss 1.1287339475529734e-05\n",
      "Epoch 920 Loss 1.1264595741522498e-05\n",
      "Epoch 921 Loss 1.1244150300626643e-05\n",
      "Epoch 922 Loss 1.1225329217268154e-05\n",
      "Epoch 923 Loss 1.1204118891328108e-05\n",
      "Epoch 924 Loss 1.1179937246197369e-05\n",
      "Epoch 925 Loss 1.1157823792018462e-05\n",
      "Epoch 926 Loss 1.1138416994072031e-05\n",
      "Epoch 927 Loss 1.1116278074041475e-05\n",
      "Epoch 928 Loss 1.1093316061305813e-05\n",
      "Epoch 929 Loss 1.107286607293645e-05\n",
      "Epoch 930 Loss 1.1051289220631588e-05\n",
      "Epoch 931 Loss 1.103438717109384e-05\n",
      "Epoch 932 Loss 1.101087673305301e-05\n",
      "Epoch 933 Loss 1.098830489354441e-05\n",
      "Epoch 934 Loss 1.0968044080073014e-05\n",
      "Epoch 935 Loss 1.0946134352707304e-05\n",
      "Epoch 936 Loss 1.0926176400971599e-05\n",
      "Epoch 937 Loss 1.0907231626333669e-05\n",
      "Epoch 938 Loss 1.0886523341469001e-05\n",
      "Epoch 939 Loss 1.0866200682357885e-05\n",
      "Epoch 940 Loss 1.0848503734450787e-05\n",
      "Epoch 941 Loss 1.082842118194094e-05\n",
      "Epoch 942 Loss 1.0806216778291855e-05\n",
      "Epoch 943 Loss 1.0785138329083566e-05\n",
      "Epoch 944 Loss 1.0765253136924002e-05\n",
      "Epoch 945 Loss 1.0744599421741441e-05\n",
      "Epoch 946 Loss 1.0722492334025446e-05\n",
      "Epoch 947 Loss 1.0703099178499542e-05\n",
      "Epoch 948 Loss 1.0682868378353305e-05\n",
      "Epoch 949 Loss 1.0664478395483457e-05\n",
      "Epoch 950 Loss 1.0644240319379605e-05\n",
      "Epoch 951 Loss 1.0623401976772584e-05\n",
      "Epoch 952 Loss 1.0606308023852762e-05\n",
      "Epoch 953 Loss 1.0584390111034736e-05\n",
      "Epoch 954 Loss 1.0565767297521234e-05\n",
      "Epoch 955 Loss 1.0546254088694695e-05\n",
      "Epoch 956 Loss 1.0526938240218442e-05\n",
      "Epoch 957 Loss 1.050662285706494e-05\n",
      "Epoch 958 Loss 1.0486355677130632e-05\n",
      "Epoch 959 Loss 1.0467279935255647e-05\n",
      "Epoch 960 Loss 1.0448652574268635e-05\n",
      "Epoch 961 Loss 1.0428189852973446e-05\n",
      "Epoch 962 Loss 1.0410359209345188e-05\n",
      "Epoch 963 Loss 1.0390331226517446e-05\n",
      "Epoch 964 Loss 1.0371176358603407e-05\n",
      "Epoch 965 Loss 1.035259265336208e-05\n",
      "Epoch 966 Loss 1.0333559657738078e-05\n",
      "Epoch 967 Loss 1.0315193321730476e-05\n",
      "Epoch 968 Loss 1.0296432265022304e-05\n",
      "Epoch 969 Loss 1.0277190995111596e-05\n",
      "Epoch 970 Loss 1.0259032023895998e-05\n",
      "Epoch 971 Loss 1.0239715265925042e-05\n",
      "Epoch 972 Loss 1.0219327123195399e-05\n",
      "Epoch 973 Loss 1.020021409203764e-05\n",
      "Epoch 974 Loss 1.01814175650361e-05\n",
      "Epoch 975 Loss 1.0164209015783854e-05\n",
      "Epoch 976 Loss 1.0145648047910072e-05\n",
      "Epoch 977 Loss 1.0128012036147993e-05\n",
      "Epoch 978 Loss 1.0110224138770718e-05\n",
      "Epoch 979 Loss 1.0090935575135518e-05\n",
      "Epoch 980 Loss 1.0071727047034074e-05\n",
      "Epoch 981 Loss 1.0053641744889319e-05\n",
      "Epoch 982 Loss 1.0038132131739985e-05\n",
      "Epoch 983 Loss 1.0017256499850191e-05\n",
      "Epoch 984 Loss 9.999403118854389e-06\n",
      "Epoch 985 Loss 9.981943549064454e-06\n",
      "Epoch 986 Loss 9.96466951619368e-06\n",
      "Epoch 987 Loss 9.946706995833665e-06\n",
      "Epoch 988 Loss 9.927358405548148e-06\n",
      "Epoch 989 Loss 9.910580047289841e-06\n",
      "Epoch 990 Loss 9.893141395878047e-06\n",
      "Epoch 991 Loss 9.873978342511691e-06\n",
      "Epoch 992 Loss 9.858032171905506e-06\n",
      "Epoch 993 Loss 9.84044345386792e-06\n",
      "Epoch 994 Loss 9.82123219728237e-06\n",
      "Epoch 995 Loss 9.803289685805794e-06\n",
      "Epoch 996 Loss 9.78835487330798e-06\n",
      "Epoch 997 Loss 9.770746146386955e-06\n",
      "Epoch 998 Loss 9.75283546722494e-06\n",
      "Epoch 999 Loss 9.736264473758638e-06\n"
     ]
    }
   ],
   "source": [
    "# to train it we need to do a forward pass, calculate the loss, do a backward pass and update the weights\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad() # this is to zero the gradients of the model\n",
    "    output = model(X) # forward pass\n",
    "    #print(output, Y)\n",
    "    loss = criterion(output, Y)  # reshape output to (batch_size, num_classes) and convert Y to long type\n",
    "    loss.backward() # backward pass\n",
    "    optimizer.step() # update the weights \n",
    "    print(f'Epoch {epoch} Loss {loss.item()}') # print the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0027],\n",
      "        [0.9968],\n",
      "        [0.9968],\n",
      "        [0.0033]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# in the above example we actually overfit the model to \"memorize\" the XOR function\n",
    "output = model(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  0.,  1.])\n",
      "tensor([0., 0., 1.]) tensor([0., 0., 1.]) tensor([0., 0., 1.])\n",
      "tensor([0.2689, 0.5000, 0.7311]) tensor([0.2689, 0.5000, 0.7311]) tensor([0.2689, 0.5000, 0.7311])\n",
      "tensor([-0.7616,  0.0000,  0.7616]) tensor([-0.7616,  0.0000,  0.7616]) tensor([-0.7616,  0.0000,  0.7616])\n",
      "tensor(0.0579)\n"
     ]
    }
   ],
   "source": [
    "# let's also check functional API\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tensor_a = torch.tensor([-1.0, 0.0, 1.0])\n",
    "print(tensor_a)\n",
    "\n",
    "print(F.relu(tensor_a), tensor_a.relu(), torch.relu(tensor_a))\n",
    "print(F.sigmoid(tensor_a), tensor_a.sigmoid(), torch.sigmoid(tensor_a))\n",
    "print(F.tanh(tensor_a), tensor_a.tanh(), torch.tanh(tensor_a))\n",
    "\n",
    "# we can also use functional API to calculate the loss\n",
    "output = torch.rand(4,1)\n",
    "target = torch.rand(4,1)\n",
    "print(F.mse_loss(output, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-10.0000],\n",
      "        [ -9.9000],\n",
      "        [ -9.8000],\n",
      "        [ -9.7000],\n",
      "        [ -9.6000],\n",
      "        [ -9.5000],\n",
      "        [ -9.4000],\n",
      "        [ -9.3000],\n",
      "        [ -9.2000],\n",
      "        [ -9.1000]]) tensor([[100.0000],\n",
      "        [ 98.0100],\n",
      "        [ 96.0400],\n",
      "        [ 94.0900],\n",
      "        [ 92.1600],\n",
      "        [ 90.2500],\n",
      "        [ 88.3600],\n",
      "        [ 86.4900],\n",
      "        [ 84.6400],\n",
      "        [ 82.8100]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's create a dataset for a new function\n",
    "# let's imagine the function is y = x^2 \n",
    "\n",
    "X = torch.arange(-10, 10, 0.1).view(-1, 1)\n",
    "Y = X**2\n",
    "\n",
    "print(X[0:10], Y[0:10])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# great now let's split the data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# let's create a model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class SquareModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SquareModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 5)\n",
    "        self.linear2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "model = SquareModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 1]) torch.Size([160, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2139.75732421875\n",
      "Epoch 1 Loss 2131.283935546875\n",
      "Epoch 2 Loss 2122.64453125\n",
      "Epoch 3 Loss 2113.838623046875\n",
      "Epoch 4 Loss 2104.864013671875\n",
      "Epoch 5 Loss 2095.718994140625\n",
      "Epoch 6 Loss 2086.39990234375\n",
      "Epoch 7 Loss 2076.904052734375\n",
      "Epoch 8 Loss 2067.227783203125\n",
      "Epoch 9 Loss 2057.36669921875\n",
      "Epoch 10 Loss 2047.3167724609375\n",
      "Epoch 11 Loss 2037.0736083984375\n",
      "Epoch 12 Loss 2026.6324462890625\n",
      "Epoch 13 Loss 2015.9876708984375\n",
      "Epoch 14 Loss 2005.1351318359375\n",
      "Epoch 15 Loss 1994.0703125\n",
      "Epoch 16 Loss 1982.7867431640625\n",
      "Epoch 17 Loss 1971.280029296875\n",
      "Epoch 18 Loss 1959.5452880859375\n",
      "Epoch 19 Loss 1947.577392578125\n",
      "Epoch 20 Loss 1935.3707275390625\n",
      "Epoch 21 Loss 1922.9208984375\n",
      "Epoch 22 Loss 1910.2230224609375\n",
      "Epoch 23 Loss 1897.2724609375\n",
      "Epoch 24 Loss 1884.0650634765625\n",
      "Epoch 25 Loss 1870.5963134765625\n",
      "Epoch 26 Loss 1856.862060546875\n",
      "Epoch 27 Loss 1842.859375\n",
      "Epoch 28 Loss 1828.5843505859375\n",
      "Epoch 29 Loss 1814.033935546875\n",
      "Epoch 30 Loss 1799.206298828125\n",
      "Epoch 31 Loss 1784.098388671875\n",
      "Epoch 32 Loss 1768.709228515625\n",
      "Epoch 33 Loss 1753.0374755859375\n",
      "Epoch 34 Loss 1737.0823974609375\n",
      "Epoch 35 Loss 1720.8447265625\n",
      "Epoch 36 Loss 1704.32421875\n",
      "Epoch 37 Loss 1687.5228271484375\n",
      "Epoch 38 Loss 1670.442138671875\n",
      "Epoch 39 Loss 1653.085205078125\n",
      "Epoch 40 Loss 1635.455078125\n",
      "Epoch 41 Loss 1617.5562744140625\n",
      "Epoch 42 Loss 1599.3936767578125\n",
      "Epoch 43 Loss 1580.9727783203125\n",
      "Epoch 44 Loss 1562.3004150390625\n",
      "Epoch 45 Loss 1543.384033203125\n",
      "Epoch 46 Loss 1524.231201171875\n",
      "Epoch 47 Loss 1504.8509521484375\n",
      "Epoch 48 Loss 1485.2532958984375\n",
      "Epoch 49 Loss 1465.448486328125\n",
      "Epoch 50 Loss 1445.4471435546875\n",
      "Epoch 51 Loss 1425.261962890625\n",
      "Epoch 52 Loss 1404.9049072265625\n",
      "Epoch 53 Loss 1384.389404296875\n",
      "Epoch 54 Loss 1363.729248046875\n",
      "Epoch 55 Loss 1342.939453125\n",
      "Epoch 56 Loss 1322.034423828125\n",
      "Epoch 57 Loss 1301.030517578125\n",
      "Epoch 58 Loss 1279.943603515625\n",
      "Epoch 59 Loss 1258.790771484375\n",
      "Epoch 60 Loss 1237.5888671875\n",
      "Epoch 61 Loss 1216.355712890625\n",
      "Epoch 62 Loss 1195.109375\n",
      "Epoch 63 Loss 1173.8681640625\n",
      "Epoch 64 Loss 1152.6505126953125\n",
      "Epoch 65 Loss 1131.475341796875\n",
      "Epoch 66 Loss 1110.361572265625\n",
      "Epoch 67 Loss 1089.32861328125\n",
      "Epoch 68 Loss 1068.3958740234375\n",
      "Epoch 69 Loss 1047.5826416015625\n",
      "Epoch 70 Loss 1026.9075927734375\n",
      "Epoch 71 Loss 1006.39013671875\n",
      "Epoch 72 Loss 986.0491333007812\n",
      "Epoch 73 Loss 965.9031372070312\n",
      "Epoch 74 Loss 945.9710693359375\n",
      "Epoch 75 Loss 926.2703247070312\n",
      "Epoch 76 Loss 906.8185424804688\n",
      "Epoch 77 Loss 887.6328125\n",
      "Epoch 78 Loss 868.7291259765625\n",
      "Epoch 79 Loss 850.123046875\n",
      "Epoch 80 Loss 831.8294677734375\n",
      "Epoch 81 Loss 813.8630981445312\n",
      "Epoch 82 Loss 796.2364501953125\n",
      "Epoch 83 Loss 778.9615478515625\n",
      "Epoch 84 Loss 762.0493774414062\n",
      "Epoch 85 Loss 745.51025390625\n",
      "Epoch 86 Loss 729.3526000976562\n",
      "Epoch 87 Loss 713.5836791992188\n",
      "Epoch 88 Loss 698.2098388671875\n",
      "Epoch 89 Loss 683.2359008789062\n",
      "Epoch 90 Loss 668.6654052734375\n",
      "Epoch 91 Loss 654.5003662109375\n",
      "Epoch 92 Loss 640.7418823242188\n",
      "Epoch 93 Loss 627.3890991210938\n",
      "Epoch 94 Loss 614.4402465820312\n",
      "Epoch 95 Loss 601.8919677734375\n",
      "Epoch 96 Loss 589.7394409179688\n",
      "Epoch 97 Loss 577.9771728515625\n",
      "Epoch 98 Loss 566.5977783203125\n",
      "Epoch 99 Loss 555.5931396484375\n",
      "Epoch 100 Loss 544.9539184570312\n",
      "Epoch 101 Loss 534.6697387695312\n",
      "Epoch 102 Loss 524.7293701171875\n",
      "Epoch 103 Loss 515.12109375\n",
      "Epoch 104 Loss 505.83203125\n",
      "Epoch 105 Loss 496.84893798828125\n",
      "Epoch 106 Loss 488.1582946777344\n",
      "Epoch 107 Loss 479.7461853027344\n",
      "Epoch 108 Loss 471.5984802246094\n",
      "Epoch 109 Loss 463.7010803222656\n",
      "Epoch 110 Loss 456.03985595703125\n",
      "Epoch 111 Loss 448.600830078125\n",
      "Epoch 112 Loss 441.37054443359375\n",
      "Epoch 113 Loss 434.335693359375\n",
      "Epoch 114 Loss 427.48370361328125\n",
      "Epoch 115 Loss 420.8023376464844\n",
      "Epoch 116 Loss 414.2801818847656\n",
      "Epoch 117 Loss 407.90655517578125\n",
      "Epoch 118 Loss 401.6712341308594\n",
      "Epoch 119 Loss 395.565185546875\n",
      "Epoch 120 Loss 389.5799865722656\n",
      "Epoch 121 Loss 383.70806884765625\n",
      "Epoch 122 Loss 377.942626953125\n",
      "Epoch 123 Loss 372.2781677246094\n",
      "Epoch 124 Loss 366.70916748046875\n",
      "Epoch 125 Loss 361.2310791015625\n",
      "Epoch 126 Loss 355.8402404785156\n",
      "Epoch 127 Loss 350.5335998535156\n",
      "Epoch 128 Loss 345.30865478515625\n",
      "Epoch 129 Loss 340.1634521484375\n",
      "Epoch 130 Loss 335.0966796875\n",
      "Epoch 131 Loss 330.1073913574219\n",
      "Epoch 132 Loss 325.19500732421875\n",
      "Epoch 133 Loss 320.3592529296875\n",
      "Epoch 134 Loss 315.60028076171875\n",
      "Epoch 135 Loss 310.9183044433594\n",
      "Epoch 136 Loss 306.31378173828125\n",
      "Epoch 137 Loss 301.78729248046875\n",
      "Epoch 138 Loss 297.33953857421875\n",
      "Epoch 139 Loss 292.9712829589844\n",
      "Epoch 140 Loss 288.68328857421875\n",
      "Epoch 141 Loss 284.47625732421875\n",
      "Epoch 142 Loss 280.3509521484375\n",
      "Epoch 143 Loss 276.30816650390625\n",
      "Epoch 144 Loss 272.34906005859375\n",
      "Epoch 145 Loss 268.47412109375\n",
      "Epoch 146 Loss 264.6831970214844\n",
      "Epoch 147 Loss 260.97662353515625\n",
      "Epoch 148 Loss 257.3545227050781\n",
      "Epoch 149 Loss 253.8170623779297\n",
      "Epoch 150 Loss 250.36453247070312\n",
      "Epoch 151 Loss 246.99667358398438\n",
      "Epoch 152 Loss 243.71304321289062\n",
      "Epoch 153 Loss 240.51318359375\n",
      "Epoch 154 Loss 237.39675903320312\n",
      "Epoch 155 Loss 234.363037109375\n",
      "Epoch 156 Loss 231.4114990234375\n",
      "Epoch 157 Loss 228.5413055419922\n",
      "Epoch 158 Loss 225.75155639648438\n",
      "Epoch 159 Loss 223.0414581298828\n",
      "Epoch 160 Loss 220.4099578857422\n",
      "Epoch 161 Loss 217.85595703125\n",
      "Epoch 162 Loss 215.37844848632812\n",
      "Epoch 163 Loss 212.9761199951172\n",
      "Epoch 164 Loss 210.6479949951172\n",
      "Epoch 165 Loss 208.39346313476562\n",
      "Epoch 166 Loss 206.2103729248047\n",
      "Epoch 167 Loss 204.09750366210938\n",
      "Epoch 168 Loss 202.0534210205078\n",
      "Epoch 169 Loss 200.07669067382812\n",
      "Epoch 170 Loss 198.16610717773438\n",
      "Epoch 171 Loss 196.3204803466797\n",
      "Epoch 172 Loss 194.53785705566406\n",
      "Epoch 173 Loss 192.8167266845703\n",
      "Epoch 174 Loss 191.15560913085938\n",
      "Epoch 175 Loss 189.55291748046875\n",
      "Epoch 176 Loss 188.0072021484375\n",
      "Epoch 177 Loss 186.516845703125\n",
      "Epoch 178 Loss 185.08071899414062\n",
      "Epoch 179 Loss 183.69723510742188\n",
      "Epoch 180 Loss 182.3645477294922\n",
      "Epoch 181 Loss 181.08114624023438\n",
      "Epoch 182 Loss 179.84548950195312\n",
      "Epoch 183 Loss 178.65603637695312\n",
      "Epoch 184 Loss 177.5113067626953\n",
      "Epoch 185 Loss 176.40985107421875\n",
      "Epoch 186 Loss 175.35049438476562\n",
      "Epoch 187 Loss 174.33139038085938\n",
      "Epoch 188 Loss 173.35110473632812\n",
      "Epoch 189 Loss 172.40817260742188\n",
      "Epoch 190 Loss 171.50120544433594\n",
      "Epoch 191 Loss 170.62876892089844\n",
      "Epoch 192 Loss 169.78955078125\n",
      "Epoch 193 Loss 168.98214721679688\n",
      "Epoch 194 Loss 168.2052764892578\n",
      "Epoch 195 Loss 167.45758056640625\n",
      "Epoch 196 Loss 166.73788452148438\n",
      "Epoch 197 Loss 166.04489135742188\n",
      "Epoch 198 Loss 165.37786865234375\n",
      "Epoch 199 Loss 164.73544311523438\n",
      "Epoch 200 Loss 164.11630249023438\n",
      "Epoch 201 Loss 163.51931762695312\n",
      "Epoch 202 Loss 162.94338989257812\n",
      "Epoch 203 Loss 162.38754272460938\n",
      "Epoch 204 Loss 161.8511505126953\n",
      "Epoch 205 Loss 161.33282470703125\n",
      "Epoch 206 Loss 160.83163452148438\n",
      "Epoch 207 Loss 160.34677124023438\n",
      "Epoch 208 Loss 159.87777709960938\n",
      "Epoch 209 Loss 159.42333984375\n",
      "Epoch 210 Loss 158.9825897216797\n",
      "Epoch 211 Loss 158.5548095703125\n",
      "Epoch 212 Loss 158.1392059326172\n",
      "Epoch 213 Loss 157.73507690429688\n",
      "Epoch 214 Loss 157.34173583984375\n",
      "Epoch 215 Loss 156.9585723876953\n",
      "Epoch 216 Loss 156.58486938476562\n",
      "Epoch 217 Loss 156.22012329101562\n",
      "Epoch 218 Loss 155.86373901367188\n",
      "Epoch 219 Loss 155.51519775390625\n",
      "Epoch 220 Loss 155.17398071289062\n",
      "Epoch 221 Loss 154.8395538330078\n",
      "Epoch 222 Loss 154.51158142089844\n",
      "Epoch 223 Loss 154.18955993652344\n",
      "Epoch 224 Loss 153.873046875\n",
      "Epoch 225 Loss 153.56175231933594\n",
      "Epoch 226 Loss 153.2552490234375\n",
      "Epoch 227 Loss 152.9540557861328\n",
      "Epoch 228 Loss 152.65713500976562\n",
      "Epoch 229 Loss 152.36416625976562\n",
      "Epoch 230 Loss 152.0748291015625\n",
      "Epoch 231 Loss 151.78887939453125\n",
      "Epoch 232 Loss 151.5060272216797\n",
      "Epoch 233 Loss 151.22607421875\n",
      "Epoch 234 Loss 150.94879150390625\n",
      "Epoch 235 Loss 150.67398071289062\n",
      "Epoch 236 Loss 150.4014434814453\n",
      "Epoch 237 Loss 150.1309814453125\n",
      "Epoch 238 Loss 149.86248779296875\n",
      "Epoch 239 Loss 149.5957489013672\n",
      "Epoch 240 Loss 149.33090209960938\n",
      "Epoch 241 Loss 149.06787109375\n",
      "Epoch 242 Loss 148.8062744140625\n",
      "Epoch 243 Loss 148.5460205078125\n",
      "Epoch 244 Loss 148.28697204589844\n",
      "Epoch 245 Loss 148.02902221679688\n",
      "Epoch 246 Loss 147.77212524414062\n",
      "Epoch 247 Loss 147.51614379882812\n",
      "Epoch 248 Loss 147.2614288330078\n",
      "Epoch 249 Loss 147.00791931152344\n",
      "Epoch 250 Loss 146.7552490234375\n",
      "Epoch 251 Loss 146.50326538085938\n",
      "Epoch 252 Loss 146.25196838378906\n",
      "Epoch 253 Loss 146.00131225585938\n",
      "Epoch 254 Loss 145.75152587890625\n",
      "Epoch 255 Loss 145.50247192382812\n",
      "Epoch 256 Loss 145.25390625\n",
      "Epoch 257 Loss 145.005859375\n",
      "Epoch 258 Loss 144.75820922851562\n",
      "Epoch 259 Loss 144.51101684570312\n",
      "Epoch 260 Loss 144.26416015625\n",
      "Epoch 261 Loss 144.0176544189453\n",
      "Epoch 262 Loss 143.77146911621094\n",
      "Epoch 263 Loss 143.52561950683594\n",
      "Epoch 264 Loss 143.28001403808594\n",
      "Epoch 265 Loss 143.03466796875\n",
      "Epoch 266 Loss 142.7896270751953\n",
      "Epoch 267 Loss 142.54507446289062\n",
      "Epoch 268 Loss 142.3009033203125\n",
      "Epoch 269 Loss 142.05697631835938\n",
      "Epoch 270 Loss 141.81344604492188\n",
      "Epoch 271 Loss 141.57066345214844\n",
      "Epoch 272 Loss 141.32815551757812\n",
      "Epoch 273 Loss 141.0858612060547\n",
      "Epoch 274 Loss 140.84384155273438\n",
      "Epoch 275 Loss 140.60202026367188\n",
      "Epoch 276 Loss 140.36038208007812\n",
      "Epoch 277 Loss 140.11941528320312\n",
      "Epoch 278 Loss 139.87879943847656\n",
      "Epoch 279 Loss 139.63839721679688\n",
      "Epoch 280 Loss 139.398193359375\n",
      "Epoch 281 Loss 139.15821838378906\n",
      "Epoch 282 Loss 138.9186248779297\n",
      "Epoch 283 Loss 138.67947387695312\n",
      "Epoch 284 Loss 138.44049072265625\n",
      "Epoch 285 Loss 138.2017059326172\n",
      "Epoch 286 Loss 137.96310424804688\n",
      "Epoch 287 Loss 137.72470092773438\n",
      "Epoch 288 Loss 137.4864959716797\n",
      "Epoch 289 Loss 137.24842834472656\n",
      "Epoch 290 Loss 137.0105743408203\n",
      "Epoch 291 Loss 136.77285766601562\n",
      "Epoch 292 Loss 136.53530883789062\n",
      "Epoch 293 Loss 136.29794311523438\n",
      "Epoch 294 Loss 136.06130981445312\n",
      "Epoch 295 Loss 135.82481384277344\n",
      "Epoch 296 Loss 135.58853149414062\n",
      "Epoch 297 Loss 135.35256958007812\n",
      "Epoch 298 Loss 135.11691284179688\n",
      "Epoch 299 Loss 134.8815460205078\n",
      "Epoch 300 Loss 134.6462860107422\n",
      "Epoch 301 Loss 134.4112548828125\n",
      "Epoch 302 Loss 134.17636108398438\n",
      "Epoch 303 Loss 133.94163513183594\n",
      "Epoch 304 Loss 133.7071075439453\n",
      "Epoch 305 Loss 133.4727020263672\n",
      "Epoch 306 Loss 133.2384796142578\n",
      "Epoch 307 Loss 133.00439453125\n",
      "Epoch 308 Loss 132.77047729492188\n",
      "Epoch 309 Loss 132.53671264648438\n",
      "Epoch 310 Loss 132.30311584472656\n",
      "Epoch 311 Loss 132.06991577148438\n",
      "Epoch 312 Loss 131.83688354492188\n",
      "Epoch 313 Loss 131.60401916503906\n",
      "Epoch 314 Loss 131.37191772460938\n",
      "Epoch 315 Loss 131.1400604248047\n",
      "Epoch 316 Loss 130.9083709716797\n",
      "Epoch 317 Loss 130.6769561767578\n",
      "Epoch 318 Loss 130.446044921875\n",
      "Epoch 319 Loss 130.21536254882812\n",
      "Epoch 320 Loss 129.98492431640625\n",
      "Epoch 321 Loss 129.754638671875\n",
      "Epoch 322 Loss 129.52456665039062\n",
      "Epoch 323 Loss 129.29464721679688\n",
      "Epoch 324 Loss 129.06494140625\n",
      "Epoch 325 Loss 128.83541870117188\n",
      "Epoch 326 Loss 128.6060333251953\n",
      "Epoch 327 Loss 128.37681579589844\n",
      "Epoch 328 Loss 128.1478729248047\n",
      "Epoch 329 Loss 127.9191665649414\n",
      "Epoch 330 Loss 127.6905746459961\n",
      "Epoch 331 Loss 127.46220397949219\n",
      "Epoch 332 Loss 127.2342529296875\n",
      "Epoch 333 Loss 127.00688171386719\n",
      "Epoch 334 Loss 126.7796630859375\n",
      "Epoch 335 Loss 126.55265808105469\n",
      "Epoch 336 Loss 126.32586669921875\n",
      "Epoch 337 Loss 126.09922790527344\n",
      "Epoch 338 Loss 125.87278747558594\n",
      "Epoch 339 Loss 125.64652252197266\n",
      "Epoch 340 Loss 125.42045593261719\n",
      "Epoch 341 Loss 125.19450378417969\n",
      "Epoch 342 Loss 124.96878814697266\n",
      "Epoch 343 Loss 124.74320983886719\n",
      "Epoch 344 Loss 124.5177993774414\n",
      "Epoch 345 Loss 124.2925796508789\n",
      "Epoch 346 Loss 124.06748962402344\n",
      "Epoch 347 Loss 123.84259033203125\n",
      "Epoch 348 Loss 123.61784362792969\n",
      "Epoch 349 Loss 123.39322662353516\n",
      "Epoch 350 Loss 123.1688461303711\n",
      "Epoch 351 Loss 122.94490051269531\n",
      "Epoch 352 Loss 122.72145080566406\n",
      "Epoch 353 Loss 122.49822998046875\n",
      "Epoch 354 Loss 122.27519226074219\n",
      "Epoch 355 Loss 122.0523681640625\n",
      "Epoch 356 Loss 121.82975006103516\n",
      "Epoch 357 Loss 121.6073226928711\n",
      "Epoch 358 Loss 121.38509368896484\n",
      "Epoch 359 Loss 121.16304016113281\n",
      "Epoch 360 Loss 120.941162109375\n",
      "Epoch 361 Loss 120.71954345703125\n",
      "Epoch 362 Loss 120.498046875\n",
      "Epoch 363 Loss 120.2767562866211\n",
      "Epoch 364 Loss 120.05574035644531\n",
      "Epoch 365 Loss 119.8349838256836\n",
      "Epoch 366 Loss 119.6144027709961\n",
      "Epoch 367 Loss 119.3940200805664\n",
      "Epoch 368 Loss 119.1739273071289\n",
      "Epoch 369 Loss 118.9540786743164\n",
      "Epoch 370 Loss 118.7345199584961\n",
      "Epoch 371 Loss 118.51570892333984\n",
      "Epoch 372 Loss 118.29716491699219\n",
      "Epoch 373 Loss 118.07881164550781\n",
      "Epoch 374 Loss 117.8606948852539\n",
      "Epoch 375 Loss 117.64280700683594\n",
      "Epoch 376 Loss 117.42513275146484\n",
      "Epoch 377 Loss 117.20765686035156\n",
      "Epoch 378 Loss 116.99037170410156\n",
      "Epoch 379 Loss 116.7732925415039\n",
      "Epoch 380 Loss 116.556396484375\n",
      "Epoch 381 Loss 116.33976745605469\n",
      "Epoch 382 Loss 116.1232681274414\n",
      "Epoch 383 Loss 115.9070053100586\n",
      "Epoch 384 Loss 115.6908950805664\n",
      "Epoch 385 Loss 115.4749755859375\n",
      "Epoch 386 Loss 115.2592544555664\n",
      "Epoch 387 Loss 115.0437240600586\n",
      "Epoch 388 Loss 114.8283920288086\n",
      "Epoch 389 Loss 114.6132583618164\n",
      "Epoch 390 Loss 114.39842224121094\n",
      "Epoch 391 Loss 114.18431091308594\n",
      "Epoch 392 Loss 113.97044372558594\n",
      "Epoch 393 Loss 113.75679016113281\n",
      "Epoch 394 Loss 113.54341125488281\n",
      "Epoch 395 Loss 113.33030700683594\n",
      "Epoch 396 Loss 113.11737060546875\n",
      "Epoch 397 Loss 112.9046630859375\n",
      "Epoch 398 Loss 112.69222259521484\n",
      "Epoch 399 Loss 112.47990417480469\n",
      "Epoch 400 Loss 112.2678451538086\n",
      "Epoch 401 Loss 112.0560073852539\n",
      "Epoch 402 Loss 111.8443603515625\n",
      "Epoch 403 Loss 111.63294982910156\n",
      "Epoch 404 Loss 111.4217300415039\n",
      "Epoch 405 Loss 111.210693359375\n",
      "Epoch 406 Loss 110.9998779296875\n",
      "Epoch 407 Loss 110.78924560546875\n",
      "Epoch 408 Loss 110.57889556884766\n",
      "Epoch 409 Loss 110.3687515258789\n",
      "Epoch 410 Loss 110.1587905883789\n",
      "Epoch 411 Loss 109.94941711425781\n",
      "Epoch 412 Loss 109.74053955078125\n",
      "Epoch 413 Loss 109.5318832397461\n",
      "Epoch 414 Loss 109.32350158691406\n",
      "Epoch 415 Loss 109.11528015136719\n",
      "Epoch 416 Loss 108.9073715209961\n",
      "Epoch 417 Loss 108.69966125488281\n",
      "Epoch 418 Loss 108.4922103881836\n",
      "Epoch 419 Loss 108.28497314453125\n",
      "Epoch 420 Loss 108.07792663574219\n",
      "Epoch 421 Loss 107.87113952636719\n",
      "Epoch 422 Loss 107.66459655761719\n",
      "Epoch 423 Loss 107.4582748413086\n",
      "Epoch 424 Loss 107.25215148925781\n",
      "Epoch 425 Loss 107.04627990722656\n",
      "Epoch 426 Loss 106.8405532836914\n",
      "Epoch 427 Loss 106.63520812988281\n",
      "Epoch 428 Loss 106.43003845214844\n",
      "Epoch 429 Loss 106.22511291503906\n",
      "Epoch 430 Loss 106.02040100097656\n",
      "Epoch 431 Loss 105.81584167480469\n",
      "Epoch 432 Loss 105.611572265625\n",
      "Epoch 433 Loss 105.4074935913086\n",
      "Epoch 434 Loss 105.20361328125\n",
      "Epoch 435 Loss 104.99998474121094\n",
      "Epoch 436 Loss 104.79655456542969\n",
      "Epoch 437 Loss 104.59330749511719\n",
      "Epoch 438 Loss 104.39030456542969\n",
      "Epoch 439 Loss 104.1875\n",
      "Epoch 440 Loss 103.98494720458984\n",
      "Epoch 441 Loss 103.7825698852539\n",
      "Epoch 442 Loss 103.58045959472656\n",
      "Epoch 443 Loss 103.3785171508789\n",
      "Epoch 444 Loss 103.17679595947266\n",
      "Epoch 445 Loss 102.975341796875\n",
      "Epoch 446 Loss 102.77406311035156\n",
      "Epoch 447 Loss 102.57303619384766\n",
      "Epoch 448 Loss 102.37220764160156\n",
      "Epoch 449 Loss 102.1716079711914\n",
      "Epoch 450 Loss 101.97123718261719\n",
      "Epoch 451 Loss 101.77110290527344\n",
      "Epoch 452 Loss 101.57123565673828\n",
      "Epoch 453 Loss 101.37153625488281\n",
      "Epoch 454 Loss 101.17210388183594\n",
      "Epoch 455 Loss 100.9729232788086\n",
      "Epoch 456 Loss 100.77394104003906\n",
      "Epoch 457 Loss 100.57524108886719\n",
      "Epoch 458 Loss 100.3768081665039\n",
      "Epoch 459 Loss 100.17857360839844\n",
      "Epoch 460 Loss 99.98057556152344\n",
      "Epoch 461 Loss 99.78279876708984\n",
      "Epoch 462 Loss 99.58525848388672\n",
      "Epoch 463 Loss 99.3879165649414\n",
      "Epoch 464 Loss 99.19084167480469\n",
      "Epoch 465 Loss 98.99394226074219\n",
      "Epoch 466 Loss 98.79730987548828\n",
      "Epoch 467 Loss 98.60087585449219\n",
      "Epoch 468 Loss 98.40472412109375\n",
      "Epoch 469 Loss 98.20875549316406\n",
      "Epoch 470 Loss 98.01302337646484\n",
      "Epoch 471 Loss 97.81753540039062\n",
      "Epoch 472 Loss 97.62226867675781\n",
      "Epoch 473 Loss 97.42725372314453\n",
      "Epoch 474 Loss 97.2330093383789\n",
      "Epoch 475 Loss 97.03914642333984\n",
      "Epoch 476 Loss 96.84554290771484\n",
      "Epoch 477 Loss 96.65225982666016\n",
      "Epoch 478 Loss 96.459228515625\n",
      "Epoch 479 Loss 96.26647186279297\n",
      "Epoch 480 Loss 96.07398986816406\n",
      "Epoch 481 Loss 95.88177490234375\n",
      "Epoch 482 Loss 95.68984985351562\n",
      "Epoch 483 Loss 95.4981689453125\n",
      "Epoch 484 Loss 95.30677795410156\n",
      "Epoch 485 Loss 95.11561584472656\n",
      "Epoch 486 Loss 94.92475128173828\n",
      "Epoch 487 Loss 94.7341079711914\n",
      "Epoch 488 Loss 94.54374694824219\n",
      "Epoch 489 Loss 94.35364532470703\n",
      "Epoch 490 Loss 94.1637954711914\n",
      "Epoch 491 Loss 93.9742202758789\n",
      "Epoch 492 Loss 93.78489685058594\n",
      "Epoch 493 Loss 93.5958023071289\n",
      "Epoch 494 Loss 93.40699005126953\n",
      "Epoch 495 Loss 93.21842956542969\n",
      "Epoch 496 Loss 93.03014373779297\n",
      "Epoch 497 Loss 92.84211730957031\n",
      "Epoch 498 Loss 92.6543197631836\n",
      "Epoch 499 Loss 92.46680450439453\n",
      "Epoch 500 Loss 92.27951049804688\n",
      "Epoch 501 Loss 92.0925064086914\n",
      "Epoch 502 Loss 91.90577697753906\n",
      "Epoch 503 Loss 91.71928405761719\n",
      "Epoch 504 Loss 91.5331039428711\n",
      "Epoch 505 Loss 91.34712219238281\n",
      "Epoch 506 Loss 91.16143035888672\n",
      "Epoch 507 Loss 90.97602844238281\n",
      "Epoch 508 Loss 90.79087829589844\n",
      "Epoch 509 Loss 90.60601806640625\n",
      "Epoch 510 Loss 90.42140197753906\n",
      "Epoch 511 Loss 90.23705291748047\n",
      "Epoch 512 Loss 90.05296325683594\n",
      "Epoch 513 Loss 89.86918640136719\n",
      "Epoch 514 Loss 89.6856460571289\n",
      "Epoch 515 Loss 89.5024185180664\n",
      "Epoch 516 Loss 89.31980895996094\n",
      "Epoch 517 Loss 89.13780212402344\n",
      "Epoch 518 Loss 88.95610046386719\n",
      "Epoch 519 Loss 88.77476501464844\n",
      "Epoch 520 Loss 88.59370422363281\n",
      "Epoch 521 Loss 88.41294860839844\n",
      "Epoch 522 Loss 88.23252868652344\n",
      "Epoch 523 Loss 88.05239868164062\n",
      "Epoch 524 Loss 87.87258911132812\n",
      "Epoch 525 Loss 87.69306945800781\n",
      "Epoch 526 Loss 87.51383209228516\n",
      "Epoch 527 Loss 87.33491516113281\n",
      "Epoch 528 Loss 87.15631103515625\n",
      "Epoch 529 Loss 86.97798919677734\n",
      "Epoch 530 Loss 86.79994201660156\n",
      "Epoch 531 Loss 86.62220764160156\n",
      "Epoch 532 Loss 86.44476318359375\n",
      "Epoch 533 Loss 86.26760864257812\n",
      "Epoch 534 Loss 86.09074401855469\n",
      "Epoch 535 Loss 85.91417694091797\n",
      "Epoch 536 Loss 85.73786926269531\n",
      "Epoch 537 Loss 85.56182098388672\n",
      "Epoch 538 Loss 85.38652038574219\n",
      "Epoch 539 Loss 85.21171569824219\n",
      "Epoch 540 Loss 85.03726196289062\n",
      "Epoch 541 Loss 84.8631362915039\n",
      "Epoch 542 Loss 84.68931579589844\n",
      "Epoch 543 Loss 84.51583099365234\n",
      "Epoch 544 Loss 84.34262084960938\n",
      "Epoch 545 Loss 84.16978454589844\n",
      "Epoch 546 Loss 83.99720764160156\n",
      "Epoch 547 Loss 83.824951171875\n",
      "Epoch 548 Loss 83.65303039550781\n",
      "Epoch 549 Loss 83.48137664794922\n",
      "Epoch 550 Loss 83.31002044677734\n",
      "Epoch 551 Loss 83.1390380859375\n",
      "Epoch 552 Loss 82.96830749511719\n",
      "Epoch 553 Loss 82.79786682128906\n",
      "Epoch 554 Loss 82.62776184082031\n",
      "Epoch 555 Loss 82.45795440673828\n",
      "Epoch 556 Loss 82.2884292602539\n",
      "Epoch 557 Loss 82.11922454833984\n",
      "Epoch 558 Loss 81.95033264160156\n",
      "Epoch 559 Loss 81.78170013427734\n",
      "Epoch 560 Loss 81.61336517333984\n",
      "Epoch 561 Loss 81.44577026367188\n",
      "Epoch 562 Loss 81.27862548828125\n",
      "Epoch 563 Loss 81.11182403564453\n",
      "Epoch 564 Loss 80.94532775878906\n",
      "Epoch 565 Loss 80.7791748046875\n",
      "Epoch 566 Loss 80.61332702636719\n",
      "Epoch 567 Loss 80.4477310180664\n",
      "Epoch 568 Loss 80.28250885009766\n",
      "Epoch 569 Loss 80.11756896972656\n",
      "Epoch 570 Loss 79.95292663574219\n",
      "Epoch 571 Loss 79.7885971069336\n",
      "Epoch 572 Loss 79.62462615966797\n",
      "Epoch 573 Loss 79.46092224121094\n",
      "Epoch 574 Loss 79.29753875732422\n",
      "Epoch 575 Loss 79.13444519042969\n",
      "Epoch 576 Loss 78.9717025756836\n",
      "Epoch 577 Loss 78.80924987792969\n",
      "Epoch 578 Loss 78.64710998535156\n",
      "Epoch 579 Loss 78.48529052734375\n",
      "Epoch 580 Loss 78.3237533569336\n",
      "Epoch 581 Loss 78.16252136230469\n",
      "Epoch 582 Loss 78.00161743164062\n",
      "Epoch 583 Loss 77.84101867675781\n",
      "Epoch 584 Loss 77.6806869506836\n",
      "Epoch 585 Loss 77.52099609375\n",
      "Epoch 586 Loss 77.36186218261719\n",
      "Epoch 587 Loss 77.2030258178711\n",
      "Epoch 588 Loss 77.04457092285156\n",
      "Epoch 589 Loss 76.8863525390625\n",
      "Epoch 590 Loss 76.7284927368164\n",
      "Epoch 591 Loss 76.5709457397461\n",
      "Epoch 592 Loss 76.4137191772461\n",
      "Epoch 593 Loss 76.25679779052734\n",
      "Epoch 594 Loss 76.10020446777344\n",
      "Epoch 595 Loss 75.94390869140625\n",
      "Epoch 596 Loss 75.78794860839844\n",
      "Epoch 597 Loss 75.63229370117188\n",
      "Epoch 598 Loss 75.4769515991211\n",
      "Epoch 599 Loss 75.32189178466797\n",
      "Epoch 600 Loss 75.16719818115234\n",
      "Epoch 601 Loss 75.01277160644531\n",
      "Epoch 602 Loss 74.85863494873047\n",
      "Epoch 603 Loss 74.70484924316406\n",
      "Epoch 604 Loss 74.5513687133789\n",
      "Epoch 605 Loss 74.39817810058594\n",
      "Epoch 606 Loss 74.24530029296875\n",
      "Epoch 607 Loss 74.09272766113281\n",
      "Epoch 608 Loss 73.94049835205078\n",
      "Epoch 609 Loss 73.78851318359375\n",
      "Epoch 610 Loss 73.636962890625\n",
      "Epoch 611 Loss 73.48616027832031\n",
      "Epoch 612 Loss 73.33573150634766\n",
      "Epoch 613 Loss 73.18563842773438\n",
      "Epoch 614 Loss 73.03590393066406\n",
      "Epoch 615 Loss 72.88645935058594\n",
      "Epoch 616 Loss 72.73738861083984\n",
      "Epoch 617 Loss 72.58860778808594\n",
      "Epoch 618 Loss 72.44022369384766\n",
      "Epoch 619 Loss 72.2921142578125\n",
      "Epoch 620 Loss 72.14437103271484\n",
      "Epoch 621 Loss 71.99691009521484\n",
      "Epoch 622 Loss 71.84977722167969\n",
      "Epoch 623 Loss 71.70298767089844\n",
      "Epoch 624 Loss 71.55652618408203\n",
      "Epoch 625 Loss 71.41036224365234\n",
      "Epoch 626 Loss 71.26454162597656\n",
      "Epoch 627 Loss 71.11902618408203\n",
      "Epoch 628 Loss 70.97383117675781\n",
      "Epoch 629 Loss 70.82894897460938\n",
      "Epoch 630 Loss 70.68440246582031\n",
      "Epoch 631 Loss 70.54017639160156\n",
      "Epoch 632 Loss 70.39619445800781\n",
      "Epoch 633 Loss 70.25247192382812\n",
      "Epoch 634 Loss 70.10907745361328\n",
      "Epoch 635 Loss 69.96595764160156\n",
      "Epoch 636 Loss 69.82316589355469\n",
      "Epoch 637 Loss 69.68055725097656\n",
      "Epoch 638 Loss 69.53868103027344\n",
      "Epoch 639 Loss 69.39710998535156\n",
      "Epoch 640 Loss 69.25587463378906\n",
      "Epoch 641 Loss 69.11491394042969\n",
      "Epoch 642 Loss 68.97430419921875\n",
      "Epoch 643 Loss 68.8340072631836\n",
      "Epoch 644 Loss 68.69402313232422\n",
      "Epoch 645 Loss 68.5543441772461\n",
      "Epoch 646 Loss 68.41499328613281\n",
      "Epoch 647 Loss 68.27597045898438\n",
      "Epoch 648 Loss 68.13722229003906\n",
      "Epoch 649 Loss 67.9988021850586\n",
      "Epoch 650 Loss 67.86067962646484\n",
      "Epoch 651 Loss 67.72291564941406\n",
      "Epoch 652 Loss 67.5854263305664\n",
      "Epoch 653 Loss 67.44817352294922\n",
      "Epoch 654 Loss 67.31120300292969\n",
      "Epoch 655 Loss 67.17452239990234\n",
      "Epoch 656 Loss 67.03810119628906\n",
      "Epoch 657 Loss 66.90202331542969\n",
      "Epoch 658 Loss 66.76622009277344\n",
      "Epoch 659 Loss 66.63069152832031\n",
      "Epoch 660 Loss 66.4955062866211\n",
      "Epoch 661 Loss 66.36060333251953\n",
      "Epoch 662 Loss 66.22601318359375\n",
      "Epoch 663 Loss 66.09170532226562\n",
      "Epoch 664 Loss 65.95768737792969\n",
      "Epoch 665 Loss 65.82402038574219\n",
      "Epoch 666 Loss 65.69091796875\n",
      "Epoch 667 Loss 65.55829620361328\n",
      "Epoch 668 Loss 65.42596435546875\n",
      "Epoch 669 Loss 65.29402160644531\n",
      "Epoch 670 Loss 65.16236877441406\n",
      "Epoch 671 Loss 65.03106689453125\n",
      "Epoch 672 Loss 64.90006256103516\n",
      "Epoch 673 Loss 64.76939392089844\n",
      "Epoch 674 Loss 64.6390609741211\n",
      "Epoch 675 Loss 64.5090103149414\n",
      "Epoch 676 Loss 64.3792953491211\n",
      "Epoch 677 Loss 64.24990844726562\n",
      "Epoch 678 Loss 64.12080383300781\n",
      "Epoch 679 Loss 63.992034912109375\n",
      "Epoch 680 Loss 63.86357879638672\n",
      "Epoch 681 Loss 63.73543167114258\n",
      "Epoch 682 Loss 63.60758590698242\n",
      "Epoch 683 Loss 63.480064392089844\n",
      "Epoch 684 Loss 63.35284423828125\n",
      "Epoch 685 Loss 63.225929260253906\n",
      "Epoch 686 Loss 63.099205017089844\n",
      "Epoch 687 Loss 62.9727783203125\n",
      "Epoch 688 Loss 62.84663009643555\n",
      "Epoch 689 Loss 62.720787048339844\n",
      "Epoch 690 Loss 62.59521484375\n",
      "Epoch 691 Loss 62.4698371887207\n",
      "Epoch 692 Loss 62.3447265625\n",
      "Epoch 693 Loss 62.21986770629883\n",
      "Epoch 694 Loss 62.09529495239258\n",
      "Epoch 695 Loss 61.97099685668945\n",
      "Epoch 696 Loss 61.84736251831055\n",
      "Epoch 697 Loss 61.724037170410156\n",
      "Epoch 698 Loss 61.6010627746582\n",
      "Epoch 699 Loss 61.478370666503906\n",
      "Epoch 700 Loss 61.35601043701172\n",
      "Epoch 701 Loss 61.23392868041992\n",
      "Epoch 702 Loss 61.112159729003906\n",
      "Epoch 703 Loss 60.9907112121582\n",
      "Epoch 704 Loss 60.86952590942383\n",
      "Epoch 705 Loss 60.7486686706543\n",
      "Epoch 706 Loss 60.627967834472656\n",
      "Epoch 707 Loss 60.50757598876953\n",
      "Epoch 708 Loss 60.38740921020508\n",
      "Epoch 709 Loss 60.26757049560547\n",
      "Epoch 710 Loss 60.14801025390625\n",
      "Epoch 711 Loss 60.028717041015625\n",
      "Epoch 712 Loss 59.90971755981445\n",
      "Epoch 713 Loss 59.79103469848633\n",
      "Epoch 714 Loss 59.672576904296875\n",
      "Epoch 715 Loss 59.55446243286133\n",
      "Epoch 716 Loss 59.43659591674805\n",
      "Epoch 717 Loss 59.31901168823242\n",
      "Epoch 718 Loss 59.201744079589844\n",
      "Epoch 719 Loss 59.084747314453125\n",
      "Epoch 720 Loss 58.96802520751953\n",
      "Epoch 721 Loss 58.85162353515625\n",
      "Epoch 722 Loss 58.73545455932617\n",
      "Epoch 723 Loss 58.61960983276367\n",
      "Epoch 724 Loss 58.503936767578125\n",
      "Epoch 725 Loss 58.38848876953125\n",
      "Epoch 726 Loss 58.27332305908203\n",
      "Epoch 727 Loss 58.15879440307617\n",
      "Epoch 728 Loss 58.044578552246094\n",
      "Epoch 729 Loss 57.930633544921875\n",
      "Epoch 730 Loss 57.8170166015625\n",
      "Epoch 731 Loss 57.703514099121094\n",
      "Epoch 732 Loss 57.59029006958008\n",
      "Epoch 733 Loss 57.47733688354492\n",
      "Epoch 734 Loss 57.36466598510742\n",
      "Epoch 735 Loss 57.25225830078125\n",
      "Epoch 736 Loss 57.13999557495117\n",
      "Epoch 737 Loss 57.027976989746094\n",
      "Epoch 738 Loss 56.916168212890625\n",
      "Epoch 739 Loss 56.804656982421875\n",
      "Epoch 740 Loss 56.693389892578125\n",
      "Epoch 741 Loss 56.5823974609375\n",
      "Epoch 742 Loss 56.471641540527344\n",
      "Epoch 743 Loss 56.36115264892578\n",
      "Epoch 744 Loss 56.250938415527344\n",
      "Epoch 745 Loss 56.140968322753906\n",
      "Epoch 746 Loss 56.0312614440918\n",
      "Epoch 747 Loss 55.92180633544922\n",
      "Epoch 748 Loss 55.81262969970703\n",
      "Epoch 749 Loss 55.703712463378906\n",
      "Epoch 750 Loss 55.5950927734375\n",
      "Epoch 751 Loss 55.4866943359375\n",
      "Epoch 752 Loss 55.3785514831543\n",
      "Epoch 753 Loss 55.27070236206055\n",
      "Epoch 754 Loss 55.1630859375\n",
      "Epoch 755 Loss 55.05576705932617\n",
      "Epoch 756 Loss 54.94868850708008\n",
      "Epoch 757 Loss 54.841896057128906\n",
      "Epoch 758 Loss 54.735374450683594\n",
      "Epoch 759 Loss 54.62943649291992\n",
      "Epoch 760 Loss 54.523773193359375\n",
      "Epoch 761 Loss 54.4184455871582\n",
      "Epoch 762 Loss 54.3133430480957\n",
      "Epoch 763 Loss 54.20854949951172\n",
      "Epoch 764 Loss 54.10398483276367\n",
      "Epoch 765 Loss 53.9997444152832\n",
      "Epoch 766 Loss 53.8957633972168\n",
      "Epoch 767 Loss 53.79205322265625\n",
      "Epoch 768 Loss 53.6885986328125\n",
      "Epoch 769 Loss 53.58543014526367\n",
      "Epoch 770 Loss 53.4825553894043\n",
      "Epoch 771 Loss 53.37990188598633\n",
      "Epoch 772 Loss 53.27752685546875\n",
      "Epoch 773 Loss 53.1754150390625\n",
      "Epoch 774 Loss 53.0733642578125\n",
      "Epoch 775 Loss 52.97138595581055\n",
      "Epoch 776 Loss 52.8696174621582\n",
      "Epoch 777 Loss 52.768104553222656\n",
      "Epoch 778 Loss 52.666831970214844\n",
      "Epoch 779 Loss 52.565757751464844\n",
      "Epoch 780 Loss 52.4649543762207\n",
      "Epoch 781 Loss 52.36438751220703\n",
      "Epoch 782 Loss 52.26404571533203\n",
      "Epoch 783 Loss 52.1639518737793\n",
      "Epoch 784 Loss 52.06409454345703\n",
      "Epoch 785 Loss 51.9644775390625\n",
      "Epoch 786 Loss 51.8651123046875\n",
      "Epoch 787 Loss 51.76597213745117\n",
      "Epoch 788 Loss 51.66707229614258\n",
      "Epoch 789 Loss 51.568443298339844\n",
      "Epoch 790 Loss 51.470054626464844\n",
      "Epoch 791 Loss 51.372093200683594\n",
      "Epoch 792 Loss 51.27449417114258\n",
      "Epoch 793 Loss 51.177162170410156\n",
      "Epoch 794 Loss 51.08005142211914\n",
      "Epoch 795 Loss 50.98309326171875\n",
      "Epoch 796 Loss 50.88634490966797\n",
      "Epoch 797 Loss 50.78985595703125\n",
      "Epoch 798 Loss 50.69359588623047\n",
      "Epoch 799 Loss 50.597633361816406\n",
      "Epoch 800 Loss 50.50185012817383\n",
      "Epoch 801 Loss 50.4063606262207\n",
      "Epoch 802 Loss 50.31109619140625\n",
      "Epoch 803 Loss 50.21605682373047\n",
      "Epoch 804 Loss 50.12126922607422\n",
      "Epoch 805 Loss 50.02674102783203\n",
      "Epoch 806 Loss 49.932464599609375\n",
      "Epoch 807 Loss 49.838401794433594\n",
      "Epoch 808 Loss 49.74458312988281\n",
      "Epoch 809 Loss 49.65102767944336\n",
      "Epoch 810 Loss 49.55767059326172\n",
      "Epoch 811 Loss 49.46459197998047\n",
      "Epoch 812 Loss 49.37175369262695\n",
      "Epoch 813 Loss 49.279136657714844\n",
      "Epoch 814 Loss 49.18675994873047\n",
      "Epoch 815 Loss 49.094627380371094\n",
      "Epoch 816 Loss 49.002708435058594\n",
      "Epoch 817 Loss 48.91107940673828\n",
      "Epoch 818 Loss 48.81964874267578\n",
      "Epoch 819 Loss 48.72845458984375\n",
      "Epoch 820 Loss 48.63735580444336\n",
      "Epoch 821 Loss 48.546443939208984\n",
      "Epoch 822 Loss 48.45574188232422\n",
      "Epoch 823 Loss 48.365234375\n",
      "Epoch 824 Loss 48.27522659301758\n",
      "Epoch 825 Loss 48.1854248046875\n",
      "Epoch 826 Loss 48.0959358215332\n",
      "Epoch 827 Loss 48.006614685058594\n",
      "Epoch 828 Loss 47.917572021484375\n",
      "Epoch 829 Loss 47.82874298095703\n",
      "Epoch 830 Loss 47.740169525146484\n",
      "Epoch 831 Loss 47.651817321777344\n",
      "Epoch 832 Loss 47.563621520996094\n",
      "Epoch 833 Loss 47.47553253173828\n",
      "Epoch 834 Loss 47.38766860961914\n",
      "Epoch 835 Loss 47.30000686645508\n",
      "Epoch 836 Loss 47.21260452270508\n",
      "Epoch 837 Loss 47.12535858154297\n",
      "Epoch 838 Loss 47.03837966918945\n",
      "Epoch 839 Loss 46.95159912109375\n",
      "Epoch 840 Loss 46.86502456665039\n",
      "Epoch 841 Loss 46.778690338134766\n",
      "Epoch 842 Loss 46.692543029785156\n",
      "Epoch 843 Loss 46.606651306152344\n",
      "Epoch 844 Loss 46.520965576171875\n",
      "Epoch 845 Loss 46.43547821044922\n",
      "Epoch 846 Loss 46.3502082824707\n",
      "Epoch 847 Loss 46.26518249511719\n",
      "Epoch 848 Loss 46.18034744262695\n",
      "Epoch 849 Loss 46.09574890136719\n",
      "Epoch 850 Loss 46.01127624511719\n",
      "Epoch 851 Loss 45.92694854736328\n",
      "Epoch 852 Loss 45.842803955078125\n",
      "Epoch 853 Loss 45.758880615234375\n",
      "Epoch 854 Loss 45.675174713134766\n",
      "Epoch 855 Loss 45.5916633605957\n",
      "Epoch 856 Loss 45.50836944580078\n",
      "Epoch 857 Loss 45.42530059814453\n",
      "Epoch 858 Loss 45.34242630004883\n",
      "Epoch 859 Loss 45.259735107421875\n",
      "Epoch 860 Loss 45.17728042602539\n",
      "Epoch 861 Loss 45.09504318237305\n",
      "Epoch 862 Loss 45.01300811767578\n",
      "Epoch 863 Loss 44.93117904663086\n",
      "Epoch 864 Loss 44.849552154541016\n",
      "Epoch 865 Loss 44.768157958984375\n",
      "Epoch 866 Loss 44.686981201171875\n",
      "Epoch 867 Loss 44.605995178222656\n",
      "Epoch 868 Loss 44.52521514892578\n",
      "Epoch 869 Loss 44.44464874267578\n",
      "Epoch 870 Loss 44.36431121826172\n",
      "Epoch 871 Loss 44.284027099609375\n",
      "Epoch 872 Loss 44.20390701293945\n",
      "Epoch 873 Loss 44.123939514160156\n",
      "Epoch 874 Loss 44.044189453125\n",
      "Epoch 875 Loss 43.964637756347656\n",
      "Epoch 876 Loss 43.885276794433594\n",
      "Epoch 877 Loss 43.806114196777344\n",
      "Epoch 878 Loss 43.72713088989258\n",
      "Epoch 879 Loss 43.64836883544922\n",
      "Epoch 880 Loss 43.569786071777344\n",
      "Epoch 881 Loss 43.49140930175781\n",
      "Epoch 882 Loss 43.41322708129883\n",
      "Epoch 883 Loss 43.33525085449219\n",
      "Epoch 884 Loss 43.25749588012695\n",
      "Epoch 885 Loss 43.17992401123047\n",
      "Epoch 886 Loss 43.1025505065918\n",
      "Epoch 887 Loss 43.02545928955078\n",
      "Epoch 888 Loss 42.94873809814453\n",
      "Epoch 889 Loss 42.87221145629883\n",
      "Epoch 890 Loss 42.79591751098633\n",
      "Epoch 891 Loss 42.7198486328125\n",
      "Epoch 892 Loss 42.64397430419922\n",
      "Epoch 893 Loss 42.568092346191406\n",
      "Epoch 894 Loss 42.492286682128906\n",
      "Epoch 895 Loss 42.41667938232422\n",
      "Epoch 896 Loss 42.34123992919922\n",
      "Epoch 897 Loss 42.2659797668457\n",
      "Epoch 898 Loss 42.19091033935547\n",
      "Epoch 899 Loss 42.11602020263672\n",
      "Epoch 900 Loss 42.04130172729492\n",
      "Epoch 901 Loss 41.966773986816406\n",
      "Epoch 902 Loss 41.892417907714844\n",
      "Epoch 903 Loss 41.818206787109375\n",
      "Epoch 904 Loss 41.744049072265625\n",
      "Epoch 905 Loss 41.67002487182617\n",
      "Epoch 906 Loss 41.59621047973633\n",
      "Epoch 907 Loss 41.522552490234375\n",
      "Epoch 908 Loss 41.44908142089844\n",
      "Epoch 909 Loss 41.375755310058594\n",
      "Epoch 910 Loss 41.302635192871094\n",
      "Epoch 911 Loss 41.22965621948242\n",
      "Epoch 912 Loss 41.15686798095703\n",
      "Epoch 913 Loss 41.084251403808594\n",
      "Epoch 914 Loss 41.01184844970703\n",
      "Epoch 915 Loss 40.939579010009766\n",
      "Epoch 916 Loss 40.867488861083984\n",
      "Epoch 917 Loss 40.79560852050781\n",
      "Epoch 918 Loss 40.72410202026367\n",
      "Epoch 919 Loss 40.65281295776367\n",
      "Epoch 920 Loss 40.58171844482422\n",
      "Epoch 921 Loss 40.51079559326172\n",
      "Epoch 922 Loss 40.439937591552734\n",
      "Epoch 923 Loss 40.369266510009766\n",
      "Epoch 924 Loss 40.29877853393555\n",
      "Epoch 925 Loss 40.22843551635742\n",
      "Epoch 926 Loss 40.158287048339844\n",
      "Epoch 927 Loss 40.08833312988281\n",
      "Epoch 928 Loss 40.01852035522461\n",
      "Epoch 929 Loss 39.948883056640625\n",
      "Epoch 930 Loss 39.87945556640625\n",
      "Epoch 931 Loss 39.8101806640625\n",
      "Epoch 932 Loss 39.74106216430664\n",
      "Epoch 933 Loss 39.67213439941406\n",
      "Epoch 934 Loss 39.603389739990234\n",
      "Epoch 935 Loss 39.53479766845703\n",
      "Epoch 936 Loss 39.466392517089844\n",
      "Epoch 937 Loss 39.39815139770508\n",
      "Epoch 938 Loss 39.33008575439453\n",
      "Epoch 939 Loss 39.26219940185547\n",
      "Epoch 940 Loss 39.194480895996094\n",
      "Epoch 941 Loss 39.1269416809082\n",
      "Epoch 942 Loss 39.0595588684082\n",
      "Epoch 943 Loss 38.992347717285156\n",
      "Epoch 944 Loss 38.92530822753906\n",
      "Epoch 945 Loss 38.85847091674805\n",
      "Epoch 946 Loss 38.79176330566406\n",
      "Epoch 947 Loss 38.72523880004883\n",
      "Epoch 948 Loss 38.65863800048828\n",
      "Epoch 949 Loss 38.59204864501953\n",
      "Epoch 950 Loss 38.525611877441406\n",
      "Epoch 951 Loss 38.459312438964844\n",
      "Epoch 952 Loss 38.393150329589844\n",
      "Epoch 953 Loss 38.327117919921875\n",
      "Epoch 954 Loss 38.26124954223633\n",
      "Epoch 955 Loss 38.19548797607422\n",
      "Epoch 956 Loss 38.129920959472656\n",
      "Epoch 957 Loss 38.0644645690918\n",
      "Epoch 958 Loss 37.99915313720703\n",
      "Epoch 959 Loss 37.933990478515625\n",
      "Epoch 960 Loss 37.868988037109375\n",
      "Epoch 961 Loss 37.80413055419922\n",
      "Epoch 962 Loss 37.73942565917969\n",
      "Epoch 963 Loss 37.67485809326172\n",
      "Epoch 964 Loss 37.610450744628906\n",
      "Epoch 965 Loss 37.54621124267578\n",
      "Epoch 966 Loss 37.48210525512695\n",
      "Epoch 967 Loss 37.41815185546875\n",
      "Epoch 968 Loss 37.354347229003906\n",
      "Epoch 969 Loss 37.29070281982422\n",
      "Epoch 970 Loss 37.22722625732422\n",
      "Epoch 971 Loss 37.16387176513672\n",
      "Epoch 972 Loss 37.10065841674805\n",
      "Epoch 973 Loss 37.03763961791992\n",
      "Epoch 974 Loss 36.97472381591797\n",
      "Epoch 975 Loss 36.91200637817383\n",
      "Epoch 976 Loss 36.84946060180664\n",
      "Epoch 977 Loss 36.78722381591797\n",
      "Epoch 978 Loss 36.725196838378906\n",
      "Epoch 979 Loss 36.66328048706055\n",
      "Epoch 980 Loss 36.60157012939453\n",
      "Epoch 981 Loss 36.54000473022461\n",
      "Epoch 982 Loss 36.478580474853516\n",
      "Epoch 983 Loss 36.41731643676758\n",
      "Epoch 984 Loss 36.35622787475586\n",
      "Epoch 985 Loss 36.29527282714844\n",
      "Epoch 986 Loss 36.23447799682617\n",
      "Epoch 987 Loss 36.173858642578125\n",
      "Epoch 988 Loss 36.11336135864258\n",
      "Epoch 989 Loss 36.05303955078125\n",
      "Epoch 990 Loss 35.99286651611328\n",
      "Epoch 991 Loss 35.932823181152344\n",
      "Epoch 992 Loss 35.8729248046875\n",
      "Epoch 993 Loss 35.81319046020508\n",
      "Epoch 994 Loss 35.753597259521484\n",
      "Epoch 995 Loss 35.69414138793945\n",
      "Epoch 996 Loss 35.63486099243164\n",
      "Epoch 997 Loss 35.57568359375\n",
      "Epoch 998 Loss 35.51667785644531\n",
      "Epoch 999 Loss 35.45781326293945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch} Loss {loss.item()}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model(X_test)\n",
    "plt.scatter(X_test, Y_test)\n",
    "plt.scatter(X_test, pred.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use datasets and dataloaders\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset): # this inherits from the Dataset class\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1.1000],\n",
      "        [ 2.3000],\n",
      "        [-3.9000],\n",
      "        [ 7.2000],\n",
      "        [ 3.6000],\n",
      "        [-0.9000],\n",
      "        [-3.6000],\n",
      "        [ 0.6000],\n",
      "        [ 4.1000],\n",
      "        [ 3.0000],\n",
      "        [ 8.8000],\n",
      "        [ 9.0000],\n",
      "        [-1.3000],\n",
      "        [-5.7000],\n",
      "        [-2.3000],\n",
      "        [ 9.8000]]), tensor([[ 1.2100],\n",
      "        [ 5.2900],\n",
      "        [15.2100],\n",
      "        [51.8400],\n",
      "        [12.9600],\n",
      "        [ 0.8100],\n",
      "        [12.9600],\n",
      "        [ 0.3600],\n",
      "        [16.8100],\n",
      "        [ 9.0000],\n",
      "        [77.4400],\n",
      "        [81.0000],\n",
      "        [ 1.6900],\n",
      "        [32.4900],\n",
      "        [ 5.2900],\n",
      "        [96.0400]])]\n",
      "[tensor([[ 5.5511e-16],\n",
      "        [ 2.7000e+00],\n",
      "        [ 5.2000e+00],\n",
      "        [-7.1000e+00],\n",
      "        [-5.0000e-01],\n",
      "        [-8.7000e+00],\n",
      "        [ 7.0000e-01],\n",
      "        [-2.1000e+00],\n",
      "        [ 7.1000e+00],\n",
      "        [ 4.6000e+00],\n",
      "        [-7.6000e+00],\n",
      "        [-4.3000e+00],\n",
      "        [ 7.9000e+00],\n",
      "        [-4.0000e-01],\n",
      "        [-6.0000e-01],\n",
      "        [-1.1000e+00]]), tensor([[3.0815e-31],\n",
      "        [7.2900e+00],\n",
      "        [2.7040e+01],\n",
      "        [5.0410e+01],\n",
      "        [2.5000e-01],\n",
      "        [7.5690e+01],\n",
      "        [4.9000e-01],\n",
      "        [4.4100e+00],\n",
      "        [5.0410e+01],\n",
      "        [2.1160e+01],\n",
      "        [5.7760e+01],\n",
      "        [1.8490e+01],\n",
      "        [6.2410e+01],\n",
      "        [1.6000e-01],\n",
      "        [3.6000e-01],\n",
      "        [1.2100e+00]])]\n",
      "[tensor([[-2.0000],\n",
      "        [ 9.9000],\n",
      "        [ 5.7000],\n",
      "        [-5.1000],\n",
      "        [ 2.8000],\n",
      "        [ 4.5000],\n",
      "        [ 6.4000],\n",
      "        [-7.2000]]), tensor([[ 4.0000],\n",
      "        [98.0100],\n",
      "        [32.4900],\n",
      "        [26.0100],\n",
      "        [ 7.8400],\n",
      "        [20.2500],\n",
      "        [40.9600],\n",
      "        [51.8400]])]\n"
     ]
    }
   ],
   "source": [
    "# let's create the dataset\n",
    "dataset = CustomDataset(X_test, Y_test)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)   \n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 690.1871337890625\n",
      "Epoch 1 Loss 2434.836181640625\n",
      "Epoch 2 Loss 884.4827880859375\n",
      "Epoch 3 Loss 1340.357177734375\n",
      "Epoch 4 Loss 623.10693359375\n",
      "Epoch 5 Loss 1018.9810791015625\n",
      "Epoch 6 Loss 1084.4005126953125\n",
      "Epoch 7 Loss 750.4230346679688\n",
      "Epoch 8 Loss 448.97247314453125\n",
      "Epoch 9 Loss 1057.5072021484375\n",
      "Epoch 10 Loss 2347.50634765625\n",
      "Epoch 11 Loss 409.1880187988281\n",
      "Epoch 12 Loss 695.044189453125\n",
      "Epoch 13 Loss 2353.4208984375\n",
      "Epoch 14 Loss 811.4481201171875\n",
      "Epoch 15 Loss 690.2714233398438\n",
      "Epoch 16 Loss 753.7164916992188\n",
      "Epoch 17 Loss 265.989501953125\n",
      "Epoch 18 Loss 1503.215576171875\n",
      "Epoch 19 Loss 1568.9332275390625\n",
      "Epoch 20 Loss 433.1850891113281\n",
      "Epoch 21 Loss 1787.18505859375\n",
      "Epoch 22 Loss 668.7205200195312\n",
      "Epoch 23 Loss 1259.6058349609375\n",
      "Epoch 24 Loss 1594.420654296875\n",
      "Epoch 25 Loss 332.53216552734375\n",
      "Epoch 26 Loss 781.3721313476562\n",
      "Epoch 27 Loss 1399.4720458984375\n",
      "Epoch 28 Loss 1037.033935546875\n",
      "Epoch 29 Loss 1184.0841064453125\n",
      "Epoch 30 Loss 961.22509765625\n",
      "Epoch 31 Loss 26.586475372314453\n",
      "Epoch 32 Loss 846.2242431640625\n",
      "Epoch 33 Loss 376.64202880859375\n",
      "Epoch 34 Loss 338.0465393066406\n",
      "Epoch 35 Loss 666.1175537109375\n",
      "Epoch 36 Loss 764.7308959960938\n",
      "Epoch 37 Loss 511.857177734375\n",
      "Epoch 38 Loss 1104.5782470703125\n",
      "Epoch 39 Loss 149.9727783203125\n",
      "Epoch 40 Loss 692.9854125976562\n",
      "Epoch 41 Loss 269.3482971191406\n",
      "Epoch 42 Loss 394.51788330078125\n",
      "Epoch 43 Loss 158.1982879638672\n",
      "Epoch 44 Loss 469.7537841796875\n",
      "Epoch 45 Loss 211.89691162109375\n",
      "Epoch 46 Loss 61.472007751464844\n",
      "Epoch 47 Loss 183.828369140625\n",
      "Epoch 48 Loss 136.1639862060547\n",
      "Epoch 49 Loss 216.19174194335938\n",
      "Epoch 50 Loss 111.809814453125\n",
      "Epoch 51 Loss 106.51573944091797\n",
      "Epoch 52 Loss 154.66995239257812\n",
      "Epoch 53 Loss 217.26217651367188\n",
      "Epoch 54 Loss 176.94622802734375\n",
      "Epoch 55 Loss 151.0890350341797\n",
      "Epoch 56 Loss 238.54901123046875\n",
      "Epoch 57 Loss 119.11157989501953\n",
      "Epoch 58 Loss 208.67047119140625\n",
      "Epoch 59 Loss 125.7984619140625\n",
      "Epoch 60 Loss 157.96340942382812\n",
      "Epoch 61 Loss 138.87315368652344\n",
      "Epoch 62 Loss 113.1534652709961\n",
      "Epoch 63 Loss 70.99708557128906\n",
      "Epoch 64 Loss 200.30587768554688\n",
      "Epoch 65 Loss 146.02017211914062\n",
      "Epoch 66 Loss 331.0325012207031\n",
      "Epoch 67 Loss 179.23574829101562\n",
      "Epoch 68 Loss 200.36151123046875\n",
      "Epoch 69 Loss 93.29906463623047\n",
      "Epoch 70 Loss 131.61990356445312\n",
      "Epoch 71 Loss 180.6934051513672\n",
      "Epoch 72 Loss 115.88628387451172\n",
      "Epoch 73 Loss 150.4718780517578\n",
      "Epoch 74 Loss 116.02452087402344\n",
      "Epoch 75 Loss 178.53656005859375\n",
      "Epoch 76 Loss 155.12118530273438\n",
      "Epoch 77 Loss 175.6534423828125\n",
      "Epoch 78 Loss 129.1663818359375\n",
      "Epoch 79 Loss 82.975830078125\n",
      "Epoch 80 Loss 175.33094787597656\n",
      "Epoch 81 Loss 147.5837860107422\n",
      "Epoch 82 Loss 171.3804473876953\n",
      "Epoch 83 Loss 199.9175567626953\n",
      "Epoch 84 Loss 195.17092895507812\n",
      "Epoch 85 Loss 57.2097053527832\n",
      "Epoch 86 Loss 119.35166931152344\n",
      "Epoch 87 Loss 73.6001968383789\n",
      "Epoch 88 Loss 110.9019775390625\n",
      "Epoch 89 Loss 94.63526916503906\n",
      "Epoch 90 Loss 79.69290161132812\n",
      "Epoch 91 Loss 160.263427734375\n",
      "Epoch 92 Loss 84.20957946777344\n",
      "Epoch 93 Loss 105.0014419555664\n",
      "Epoch 94 Loss 159.26100158691406\n",
      "Epoch 95 Loss 142.69338989257812\n",
      "Epoch 96 Loss 107.47095489501953\n",
      "Epoch 97 Loss 100.40563201904297\n",
      "Epoch 98 Loss 132.44552612304688\n",
      "Epoch 99 Loss 176.8843536376953\n",
      "Epoch 100 Loss 117.51646423339844\n",
      "Epoch 101 Loss 88.94841766357422\n",
      "Epoch 102 Loss 134.47772216796875\n",
      "Epoch 103 Loss 178.82958984375\n",
      "Epoch 104 Loss 150.4344024658203\n",
      "Epoch 105 Loss 86.49524688720703\n",
      "Epoch 106 Loss 66.78572082519531\n",
      "Epoch 107 Loss 125.45184326171875\n",
      "Epoch 108 Loss 114.30706787109375\n",
      "Epoch 109 Loss 100.41539001464844\n",
      "Epoch 110 Loss 55.5457763671875\n",
      "Epoch 111 Loss 129.15220642089844\n",
      "Epoch 112 Loss 97.07044982910156\n",
      "Epoch 113 Loss 58.142608642578125\n",
      "Epoch 114 Loss 177.4323272705078\n",
      "Epoch 115 Loss 126.41487121582031\n",
      "Epoch 116 Loss 47.82600021362305\n",
      "Epoch 117 Loss 123.92221069335938\n",
      "Epoch 118 Loss 134.28468322753906\n",
      "Epoch 119 Loss 88.19464111328125\n",
      "Epoch 120 Loss 129.9691925048828\n",
      "Epoch 121 Loss 75.2666015625\n",
      "Epoch 122 Loss 90.11103820800781\n",
      "Epoch 123 Loss 71.98374938964844\n",
      "Epoch 124 Loss 112.57331848144531\n",
      "Epoch 125 Loss 107.2506103515625\n",
      "Epoch 126 Loss 72.00279998779297\n",
      "Epoch 127 Loss 69.1225357055664\n",
      "Epoch 128 Loss 170.8362274169922\n",
      "Epoch 129 Loss 166.99520874023438\n",
      "Epoch 130 Loss 119.30619812011719\n",
      "Epoch 131 Loss 112.90782165527344\n",
      "Epoch 132 Loss 104.10548400878906\n",
      "Epoch 133 Loss 104.80433654785156\n",
      "Epoch 134 Loss 72.03456115722656\n",
      "Epoch 135 Loss 139.97793579101562\n",
      "Epoch 136 Loss 129.97549438476562\n",
      "Epoch 137 Loss 111.24989318847656\n",
      "Epoch 138 Loss 130.26974487304688\n",
      "Epoch 139 Loss 57.006141662597656\n",
      "Epoch 140 Loss 174.59068298339844\n",
      "Epoch 141 Loss 106.20994567871094\n",
      "Epoch 142 Loss 102.66753387451172\n",
      "Epoch 143 Loss 72.71913146972656\n",
      "Epoch 144 Loss 112.2734375\n",
      "Epoch 145 Loss 108.4541244506836\n",
      "Epoch 146 Loss 75.06171417236328\n",
      "Epoch 147 Loss 60.74628448486328\n",
      "Epoch 148 Loss 79.36210632324219\n",
      "Epoch 149 Loss 52.497493743896484\n",
      "Epoch 150 Loss 92.10639953613281\n",
      "Epoch 151 Loss 44.24624252319336\n",
      "Epoch 152 Loss 90.14340209960938\n",
      "Epoch 153 Loss 115.01882934570312\n",
      "Epoch 154 Loss 96.52266693115234\n",
      "Epoch 155 Loss 62.22400665283203\n",
      "Epoch 156 Loss 31.666263580322266\n",
      "Epoch 157 Loss 82.79568481445312\n",
      "Epoch 158 Loss 89.57452392578125\n",
      "Epoch 159 Loss 53.647151947021484\n",
      "Epoch 160 Loss 81.10951232910156\n",
      "Epoch 161 Loss 119.46086120605469\n",
      "Epoch 162 Loss 84.26638793945312\n",
      "Epoch 163 Loss 116.12326049804688\n",
      "Epoch 164 Loss 52.61344909667969\n",
      "Epoch 165 Loss 98.76008605957031\n",
      "Epoch 166 Loss 72.31375885009766\n",
      "Epoch 167 Loss 51.875205993652344\n",
      "Epoch 168 Loss 125.86312866210938\n",
      "Epoch 169 Loss 86.33633422851562\n",
      "Epoch 170 Loss 64.86356353759766\n",
      "Epoch 171 Loss 59.232208251953125\n",
      "Epoch 172 Loss 33.815086364746094\n",
      "Epoch 173 Loss 141.0453338623047\n",
      "Epoch 174 Loss 20.767181396484375\n",
      "Epoch 175 Loss 101.2908935546875\n",
      "Epoch 176 Loss 79.5885009765625\n",
      "Epoch 177 Loss 73.48905944824219\n",
      "Epoch 178 Loss 110.29475402832031\n",
      "Epoch 179 Loss 75.25428771972656\n",
      "Epoch 180 Loss 140.70761108398438\n",
      "Epoch 181 Loss 50.29072570800781\n",
      "Epoch 182 Loss 64.03704071044922\n",
      "Epoch 183 Loss 56.33856201171875\n",
      "Epoch 184 Loss 39.83982849121094\n",
      "Epoch 185 Loss 81.30684661865234\n",
      "Epoch 186 Loss 60.41558074951172\n",
      "Epoch 187 Loss 80.37249755859375\n",
      "Epoch 188 Loss 100.22180938720703\n",
      "Epoch 189 Loss 57.18802261352539\n",
      "Epoch 190 Loss 76.6895751953125\n",
      "Epoch 191 Loss 38.740875244140625\n",
      "Epoch 192 Loss 26.77261734008789\n",
      "Epoch 193 Loss 65.10734558105469\n",
      "Epoch 194 Loss 62.327049255371094\n",
      "Epoch 195 Loss 42.21590805053711\n",
      "Epoch 196 Loss 35.506954193115234\n",
      "Epoch 197 Loss 54.073143005371094\n",
      "Epoch 198 Loss 73.55963134765625\n",
      "Epoch 199 Loss 116.82106018066406\n",
      "Epoch 200 Loss 31.712339401245117\n",
      "Epoch 201 Loss 100.80060577392578\n",
      "Epoch 202 Loss 85.113037109375\n",
      "Epoch 203 Loss 86.45065307617188\n",
      "Epoch 204 Loss 48.851806640625\n",
      "Epoch 205 Loss 80.37274169921875\n",
      "Epoch 206 Loss 70.3749771118164\n",
      "Epoch 207 Loss 74.70455932617188\n",
      "Epoch 208 Loss 47.880428314208984\n",
      "Epoch 209 Loss 34.39739990234375\n",
      "Epoch 210 Loss 84.01731872558594\n",
      "Epoch 211 Loss 70.63075256347656\n",
      "Epoch 212 Loss 68.08599090576172\n",
      "Epoch 213 Loss 35.81928253173828\n",
      "Epoch 214 Loss 25.044811248779297\n",
      "Epoch 215 Loss 54.514869689941406\n",
      "Epoch 216 Loss 61.86122512817383\n",
      "Epoch 217 Loss 38.413108825683594\n",
      "Epoch 218 Loss 69.88844299316406\n",
      "Epoch 219 Loss 45.982879638671875\n",
      "Epoch 220 Loss 30.14668846130371\n",
      "Epoch 221 Loss 42.43104934692383\n",
      "Epoch 222 Loss 39.74543762207031\n",
      "Epoch 223 Loss 37.1145133972168\n",
      "Epoch 224 Loss 64.80424499511719\n",
      "Epoch 225 Loss 10.144384384155273\n",
      "Epoch 226 Loss 19.58495330810547\n",
      "Epoch 227 Loss 23.850414276123047\n",
      "Epoch 228 Loss 60.24223327636719\n",
      "Epoch 229 Loss 59.14665222167969\n",
      "Epoch 230 Loss 35.67938995361328\n",
      "Epoch 231 Loss 30.9030704498291\n",
      "Epoch 232 Loss 85.46968078613281\n",
      "Epoch 233 Loss 36.83619689941406\n",
      "Epoch 234 Loss 28.544668197631836\n",
      "Epoch 235 Loss 57.68675231933594\n",
      "Epoch 236 Loss 28.269935607910156\n",
      "Epoch 237 Loss 71.78937530517578\n",
      "Epoch 238 Loss 31.89186668395996\n",
      "Epoch 239 Loss 25.542280197143555\n",
      "Epoch 240 Loss 59.10343933105469\n",
      "Epoch 241 Loss 28.98941421508789\n",
      "Epoch 242 Loss 65.98925018310547\n",
      "Epoch 243 Loss 62.808197021484375\n",
      "Epoch 244 Loss 35.69744110107422\n",
      "Epoch 245 Loss 63.71880340576172\n",
      "Epoch 246 Loss 67.09886169433594\n",
      "Epoch 247 Loss 25.015968322753906\n",
      "Epoch 248 Loss 37.412139892578125\n",
      "Epoch 249 Loss 15.62416934967041\n",
      "Epoch 250 Loss 28.848623275756836\n",
      "Epoch 251 Loss 50.92961120605469\n",
      "Epoch 252 Loss 42.700191497802734\n",
      "Epoch 253 Loss 58.00773620605469\n",
      "Epoch 254 Loss 59.93524932861328\n",
      "Epoch 255 Loss 57.76803207397461\n",
      "Epoch 256 Loss 95.811279296875\n",
      "Epoch 257 Loss 42.83406448364258\n",
      "Epoch 258 Loss 18.199766159057617\n",
      "Epoch 259 Loss 47.05718231201172\n",
      "Epoch 260 Loss 39.35272216796875\n",
      "Epoch 261 Loss 19.0272159576416\n",
      "Epoch 262 Loss 69.77289581298828\n",
      "Epoch 263 Loss 44.44970703125\n",
      "Epoch 264 Loss 69.30536651611328\n",
      "Epoch 265 Loss 57.11807632446289\n",
      "Epoch 266 Loss 29.192373275756836\n",
      "Epoch 267 Loss 35.3056640625\n",
      "Epoch 268 Loss 16.89163589477539\n",
      "Epoch 269 Loss 36.68250274658203\n",
      "Epoch 270 Loss 38.79529571533203\n",
      "Epoch 271 Loss 42.65224075317383\n",
      "Epoch 272 Loss 18.614788055419922\n",
      "Epoch 273 Loss 28.205291748046875\n",
      "Epoch 274 Loss 19.592695236206055\n",
      "Epoch 275 Loss 16.913169860839844\n",
      "Epoch 276 Loss 43.85574722290039\n",
      "Epoch 277 Loss 23.428077697753906\n",
      "Epoch 278 Loss 38.88850402832031\n",
      "Epoch 279 Loss 64.11307525634766\n",
      "Epoch 280 Loss 36.88344955444336\n",
      "Epoch 281 Loss 33.85885238647461\n",
      "Epoch 282 Loss 40.10169219970703\n",
      "Epoch 283 Loss 48.57609939575195\n",
      "Epoch 284 Loss 14.96546745300293\n",
      "Epoch 285 Loss 19.59512710571289\n",
      "Epoch 286 Loss 7.731607437133789\n",
      "Epoch 287 Loss 49.781524658203125\n",
      "Epoch 288 Loss 19.67006492614746\n",
      "Epoch 289 Loss 32.571388244628906\n",
      "Epoch 290 Loss 63.59102249145508\n",
      "Epoch 291 Loss 24.645610809326172\n",
      "Epoch 292 Loss 56.993438720703125\n",
      "Epoch 293 Loss 14.613422393798828\n",
      "Epoch 294 Loss 37.00489044189453\n",
      "Epoch 295 Loss 24.00701904296875\n",
      "Epoch 296 Loss 29.111202239990234\n",
      "Epoch 297 Loss 25.62235450744629\n",
      "Epoch 298 Loss 11.794418334960938\n",
      "Epoch 299 Loss 38.708377838134766\n",
      "Epoch 300 Loss 48.23651885986328\n",
      "Epoch 301 Loss 23.114147186279297\n",
      "Epoch 302 Loss 44.558494567871094\n",
      "Epoch 303 Loss 18.882810592651367\n",
      "Epoch 304 Loss 19.12078094482422\n",
      "Epoch 305 Loss 53.23832702636719\n",
      "Epoch 306 Loss 28.280603408813477\n",
      "Epoch 307 Loss 11.15330696105957\n",
      "Epoch 308 Loss 40.10658645629883\n",
      "Epoch 309 Loss 12.175243377685547\n",
      "Epoch 310 Loss 37.02985763549805\n",
      "Epoch 311 Loss 30.58611297607422\n",
      "Epoch 312 Loss 43.17280578613281\n",
      "Epoch 313 Loss 17.443262100219727\n",
      "Epoch 314 Loss 6.626000881195068\n",
      "Epoch 315 Loss 15.888145446777344\n",
      "Epoch 316 Loss 44.35859680175781\n",
      "Epoch 317 Loss 18.234256744384766\n",
      "Epoch 318 Loss 41.51982879638672\n",
      "Epoch 319 Loss 16.72362518310547\n",
      "Epoch 320 Loss 16.71364974975586\n",
      "Epoch 321 Loss 25.76968765258789\n",
      "Epoch 322 Loss 35.290767669677734\n",
      "Epoch 323 Loss 42.960208892822266\n",
      "Epoch 324 Loss 18.256179809570312\n",
      "Epoch 325 Loss 19.639070510864258\n",
      "Epoch 326 Loss 18.282047271728516\n",
      "Epoch 327 Loss 55.040618896484375\n",
      "Epoch 328 Loss 19.318361282348633\n",
      "Epoch 329 Loss 11.09660530090332\n",
      "Epoch 330 Loss 27.410696029663086\n",
      "Epoch 331 Loss 8.573917388916016\n",
      "Epoch 332 Loss 38.87119674682617\n",
      "Epoch 333 Loss 29.86260414123535\n",
      "Epoch 334 Loss 14.240540504455566\n",
      "Epoch 335 Loss 28.785076141357422\n",
      "Epoch 336 Loss 16.85264015197754\n",
      "Epoch 337 Loss 31.728683471679688\n",
      "Epoch 338 Loss 14.998981475830078\n",
      "Epoch 339 Loss 20.77977752685547\n",
      "Epoch 340 Loss 15.735769271850586\n",
      "Epoch 341 Loss 28.564804077148438\n",
      "Epoch 342 Loss 30.52849578857422\n",
      "Epoch 343 Loss 17.173410415649414\n",
      "Epoch 344 Loss 13.348581314086914\n",
      "Epoch 345 Loss 32.88216781616211\n",
      "Epoch 346 Loss 27.40644073486328\n",
      "Epoch 347 Loss 39.03839874267578\n",
      "Epoch 348 Loss 30.601980209350586\n",
      "Epoch 349 Loss 27.29566764831543\n",
      "Epoch 350 Loss 26.955907821655273\n",
      "Epoch 351 Loss 19.108285903930664\n",
      "Epoch 352 Loss 18.439716339111328\n",
      "Epoch 353 Loss 46.98249053955078\n",
      "Epoch 354 Loss 32.067840576171875\n",
      "Epoch 355 Loss 15.647785186767578\n",
      "Epoch 356 Loss 10.311695098876953\n",
      "Epoch 357 Loss 19.792221069335938\n",
      "Epoch 358 Loss 18.57261848449707\n",
      "Epoch 359 Loss 18.874330520629883\n",
      "Epoch 360 Loss 27.724178314208984\n",
      "Epoch 361 Loss 44.089599609375\n",
      "Epoch 362 Loss 17.045190811157227\n",
      "Epoch 363 Loss 19.835725784301758\n",
      "Epoch 364 Loss 18.170394897460938\n",
      "Epoch 365 Loss 8.489931106567383\n",
      "Epoch 366 Loss 29.117692947387695\n",
      "Epoch 367 Loss 21.37639617919922\n",
      "Epoch 368 Loss 23.255516052246094\n",
      "Epoch 369 Loss 32.5573616027832\n",
      "Epoch 370 Loss 35.81427764892578\n",
      "Epoch 371 Loss 16.97309112548828\n",
      "Epoch 372 Loss 14.840471267700195\n",
      "Epoch 373 Loss 15.516386032104492\n",
      "Epoch 374 Loss 10.617109298706055\n",
      "Epoch 375 Loss 19.634666442871094\n",
      "Epoch 376 Loss 17.49843978881836\n",
      "Epoch 377 Loss 6.79871129989624\n",
      "Epoch 378 Loss 24.380111694335938\n",
      "Epoch 379 Loss 7.137864112854004\n",
      "Epoch 380 Loss 19.13178253173828\n",
      "Epoch 381 Loss 26.182022094726562\n",
      "Epoch 382 Loss 30.152029037475586\n",
      "Epoch 383 Loss 13.376932144165039\n",
      "Epoch 384 Loss 45.04619598388672\n",
      "Epoch 385 Loss 16.442096710205078\n",
      "Epoch 386 Loss 17.43737030029297\n",
      "Epoch 387 Loss 28.27233123779297\n",
      "Epoch 388 Loss 19.115983963012695\n",
      "Epoch 389 Loss 10.196647644042969\n",
      "Epoch 390 Loss 26.481929779052734\n",
      "Epoch 391 Loss 19.71432876586914\n",
      "Epoch 392 Loss 6.898097038269043\n",
      "Epoch 393 Loss 21.41522216796875\n",
      "Epoch 394 Loss 19.354022979736328\n",
      "Epoch 395 Loss 17.99523162841797\n",
      "Epoch 396 Loss 34.42966842651367\n",
      "Epoch 397 Loss 7.994784355163574\n",
      "Epoch 398 Loss 13.003583908081055\n",
      "Epoch 399 Loss 37.340232849121094\n",
      "Epoch 400 Loss 7.699172019958496\n",
      "Epoch 401 Loss 5.077680587768555\n",
      "Epoch 402 Loss 9.145000457763672\n",
      "Epoch 403 Loss 13.874170303344727\n",
      "Epoch 404 Loss 22.17974281311035\n",
      "Epoch 405 Loss 18.287370681762695\n",
      "Epoch 406 Loss 35.3284797668457\n",
      "Epoch 407 Loss 24.646692276000977\n",
      "Epoch 408 Loss 13.599035263061523\n",
      "Epoch 409 Loss 11.645517349243164\n",
      "Epoch 410 Loss 19.610511779785156\n",
      "Epoch 411 Loss 12.816869735717773\n",
      "Epoch 412 Loss 11.230178833007812\n",
      "Epoch 413 Loss 20.73192024230957\n",
      "Epoch 414 Loss 14.441234588623047\n",
      "Epoch 415 Loss 11.788018226623535\n",
      "Epoch 416 Loss 21.977859497070312\n",
      "Epoch 417 Loss 6.022196292877197\n",
      "Epoch 418 Loss 11.73243522644043\n",
      "Epoch 419 Loss 13.98823070526123\n",
      "Epoch 420 Loss 10.584896087646484\n",
      "Epoch 421 Loss 11.364165306091309\n",
      "Epoch 422 Loss 18.211997985839844\n",
      "Epoch 423 Loss 8.169439315795898\n",
      "Epoch 424 Loss 24.224910736083984\n",
      "Epoch 425 Loss 20.809730529785156\n",
      "Epoch 426 Loss 10.369994163513184\n",
      "Epoch 427 Loss 8.786474227905273\n",
      "Epoch 428 Loss 6.769036769866943\n",
      "Epoch 429 Loss 11.19744873046875\n",
      "Epoch 430 Loss 21.729915618896484\n",
      "Epoch 431 Loss 19.358673095703125\n",
      "Epoch 432 Loss 11.276548385620117\n",
      "Epoch 433 Loss 15.642918586730957\n",
      "Epoch 434 Loss 9.981884002685547\n",
      "Epoch 435 Loss 27.047897338867188\n",
      "Epoch 436 Loss 6.343451023101807\n",
      "Epoch 437 Loss 19.615528106689453\n",
      "Epoch 438 Loss 21.517589569091797\n",
      "Epoch 439 Loss 5.3459320068359375\n",
      "Epoch 440 Loss 20.77435874938965\n",
      "Epoch 441 Loss 17.9893741607666\n",
      "Epoch 442 Loss 14.240348815917969\n",
      "Epoch 443 Loss 11.730772018432617\n",
      "Epoch 444 Loss 22.83521842956543\n",
      "Epoch 445 Loss 24.71495819091797\n",
      "Epoch 446 Loss 7.847289562225342\n",
      "Epoch 447 Loss 14.088909149169922\n",
      "Epoch 448 Loss 13.137060165405273\n",
      "Epoch 449 Loss 10.35244369506836\n",
      "Epoch 450 Loss 8.399258613586426\n",
      "Epoch 451 Loss 24.04193115234375\n",
      "Epoch 452 Loss 4.262239456176758\n",
      "Epoch 453 Loss 16.306629180908203\n",
      "Epoch 454 Loss 30.792747497558594\n",
      "Epoch 455 Loss 9.928298950195312\n",
      "Epoch 456 Loss 23.006866455078125\n",
      "Epoch 457 Loss 17.873748779296875\n",
      "Epoch 458 Loss 6.31837272644043\n",
      "Epoch 459 Loss 10.323843002319336\n",
      "Epoch 460 Loss 8.150533676147461\n",
      "Epoch 461 Loss 22.856365203857422\n",
      "Epoch 462 Loss 20.337820053100586\n",
      "Epoch 463 Loss 15.314896583557129\n",
      "Epoch 464 Loss 19.457260131835938\n",
      "Epoch 465 Loss 15.234525680541992\n",
      "Epoch 466 Loss 17.729211807250977\n",
      "Epoch 467 Loss 24.84705924987793\n",
      "Epoch 468 Loss 8.61365032196045\n",
      "Epoch 469 Loss 13.88271713256836\n",
      "Epoch 470 Loss 23.58506202697754\n",
      "Epoch 471 Loss 13.719964981079102\n",
      "Epoch 472 Loss 16.169910430908203\n",
      "Epoch 473 Loss 16.76219367980957\n",
      "Epoch 474 Loss 4.027094841003418\n",
      "Epoch 475 Loss 8.054737091064453\n",
      "Epoch 476 Loss 9.451959609985352\n",
      "Epoch 477 Loss 11.442695617675781\n",
      "Epoch 478 Loss 17.105239868164062\n",
      "Epoch 479 Loss 8.330831527709961\n",
      "Epoch 480 Loss 20.197921752929688\n",
      "Epoch 481 Loss 20.58186149597168\n",
      "Epoch 482 Loss 13.909069061279297\n",
      "Epoch 483 Loss 7.545783042907715\n",
      "Epoch 484 Loss 3.48211669921875\n",
      "Epoch 485 Loss 17.54715347290039\n",
      "Epoch 486 Loss 13.808815956115723\n",
      "Epoch 487 Loss 12.36960220336914\n",
      "Epoch 488 Loss 5.815698623657227\n",
      "Epoch 489 Loss 10.384078979492188\n",
      "Epoch 490 Loss 23.524885177612305\n",
      "Epoch 491 Loss 14.959097862243652\n",
      "Epoch 492 Loss 14.912054061889648\n",
      "Epoch 493 Loss 13.556648254394531\n",
      "Epoch 494 Loss 8.016165733337402\n",
      "Epoch 495 Loss 3.8445358276367188\n",
      "Epoch 496 Loss 10.355354309082031\n",
      "Epoch 497 Loss 12.947617530822754\n",
      "Epoch 498 Loss 16.113786697387695\n",
      "Epoch 499 Loss 6.856799602508545\n",
      "Epoch 500 Loss 10.955263137817383\n",
      "Epoch 501 Loss 21.290504455566406\n",
      "Epoch 502 Loss 21.42757797241211\n",
      "Epoch 503 Loss 13.254252433776855\n",
      "Epoch 504 Loss 13.519462585449219\n",
      "Epoch 505 Loss 4.637205123901367\n",
      "Epoch 506 Loss 22.7685489654541\n",
      "Epoch 507 Loss 17.346508026123047\n",
      "Epoch 508 Loss 20.287986755371094\n",
      "Epoch 509 Loss 15.687887191772461\n",
      "Epoch 510 Loss 14.520484924316406\n",
      "Epoch 511 Loss 6.642967700958252\n",
      "Epoch 512 Loss 15.701149940490723\n",
      "Epoch 513 Loss 8.737802505493164\n",
      "Epoch 514 Loss 10.741325378417969\n",
      "Epoch 515 Loss 14.703904151916504\n",
      "Epoch 516 Loss 15.830811500549316\n",
      "Epoch 517 Loss 4.710414886474609\n",
      "Epoch 518 Loss 7.002605438232422\n",
      "Epoch 519 Loss 16.26788330078125\n",
      "Epoch 520 Loss 10.193144798278809\n",
      "Epoch 521 Loss 9.259037017822266\n",
      "Epoch 522 Loss 6.408891201019287\n",
      "Epoch 523 Loss 13.646045684814453\n",
      "Epoch 524 Loss 10.775501251220703\n",
      "Epoch 525 Loss 3.3240456581115723\n",
      "Epoch 526 Loss 9.754678726196289\n",
      "Epoch 527 Loss 12.399677276611328\n",
      "Epoch 528 Loss 18.231937408447266\n",
      "Epoch 529 Loss 9.107823371887207\n",
      "Epoch 530 Loss 8.646857261657715\n",
      "Epoch 531 Loss 14.631717681884766\n",
      "Epoch 532 Loss 14.312070846557617\n",
      "Epoch 533 Loss 5.514223575592041\n",
      "Epoch 534 Loss 12.37488079071045\n",
      "Epoch 535 Loss 14.540972709655762\n",
      "Epoch 536 Loss 8.623893737792969\n",
      "Epoch 537 Loss 8.742173194885254\n",
      "Epoch 538 Loss 7.13907527923584\n",
      "Epoch 539 Loss 7.952739715576172\n",
      "Epoch 540 Loss 19.21561622619629\n",
      "Epoch 541 Loss 14.188680648803711\n",
      "Epoch 542 Loss 12.806488037109375\n",
      "Epoch 543 Loss 7.320827960968018\n",
      "Epoch 544 Loss 14.286171913146973\n",
      "Epoch 545 Loss 3.4982926845550537\n",
      "Epoch 546 Loss 5.880634307861328\n",
      "Epoch 547 Loss 12.062326431274414\n",
      "Epoch 548 Loss 10.881299018859863\n",
      "Epoch 549 Loss 10.063785552978516\n",
      "Epoch 550 Loss 12.689170837402344\n",
      "Epoch 551 Loss 8.700735092163086\n",
      "Epoch 552 Loss 13.429034233093262\n",
      "Epoch 553 Loss 9.69288444519043\n",
      "Epoch 554 Loss 12.119080543518066\n",
      "Epoch 555 Loss 7.10015344619751\n",
      "Epoch 556 Loss 9.257468223571777\n",
      "Epoch 557 Loss 15.363117218017578\n",
      "Epoch 558 Loss 11.934005737304688\n",
      "Epoch 559 Loss 2.313694477081299\n",
      "Epoch 560 Loss 10.809083938598633\n",
      "Epoch 561 Loss 12.885766983032227\n",
      "Epoch 562 Loss 10.049102783203125\n",
      "Epoch 563 Loss 2.414499044418335\n",
      "Epoch 564 Loss 4.356855392456055\n",
      "Epoch 565 Loss 12.339737892150879\n",
      "Epoch 566 Loss 23.260887145996094\n",
      "Epoch 567 Loss 6.586231708526611\n",
      "Epoch 568 Loss 4.709397315979004\n",
      "Epoch 569 Loss 15.037635803222656\n",
      "Epoch 570 Loss 6.891678810119629\n",
      "Epoch 571 Loss 8.375578880310059\n",
      "Epoch 572 Loss 9.444522857666016\n",
      "Epoch 573 Loss 7.845156669616699\n",
      "Epoch 574 Loss 4.867364883422852\n",
      "Epoch 575 Loss 18.06383514404297\n",
      "Epoch 576 Loss 5.929544925689697\n",
      "Epoch 577 Loss 6.419261932373047\n",
      "Epoch 578 Loss 1.4258941411972046\n",
      "Epoch 579 Loss 5.036908149719238\n",
      "Epoch 580 Loss 10.17221450805664\n",
      "Epoch 581 Loss 10.684159278869629\n",
      "Epoch 582 Loss 6.917808532714844\n",
      "Epoch 583 Loss 4.182145595550537\n",
      "Epoch 584 Loss 4.885848522186279\n",
      "Epoch 585 Loss 12.070369720458984\n",
      "Epoch 586 Loss 5.170009613037109\n",
      "Epoch 587 Loss 5.4612836837768555\n",
      "Epoch 588 Loss 6.550792217254639\n",
      "Epoch 589 Loss 4.254154205322266\n",
      "Epoch 590 Loss 4.14601469039917\n",
      "Epoch 591 Loss 6.9855875968933105\n",
      "Epoch 592 Loss 3.4839587211608887\n",
      "Epoch 593 Loss 11.493545532226562\n",
      "Epoch 594 Loss 10.761102676391602\n",
      "Epoch 595 Loss 15.230377197265625\n",
      "Epoch 596 Loss 13.620346069335938\n",
      "Epoch 597 Loss 4.127012729644775\n",
      "Epoch 598 Loss 7.589011192321777\n",
      "Epoch 599 Loss 6.762599945068359\n",
      "Epoch 600 Loss 9.394132614135742\n",
      "Epoch 601 Loss 6.111142635345459\n",
      "Epoch 602 Loss 5.22447395324707\n",
      "Epoch 603 Loss 9.094305038452148\n",
      "Epoch 604 Loss 5.384298324584961\n",
      "Epoch 605 Loss 12.352362632751465\n",
      "Epoch 606 Loss 12.193827629089355\n",
      "Epoch 607 Loss 10.984874725341797\n",
      "Epoch 608 Loss 16.743412017822266\n",
      "Epoch 609 Loss 12.46834659576416\n",
      "Epoch 610 Loss 4.860491752624512\n",
      "Epoch 611 Loss 4.3686418533325195\n",
      "Epoch 612 Loss 16.392864227294922\n",
      "Epoch 613 Loss 6.7269110679626465\n",
      "Epoch 614 Loss 6.565325736999512\n",
      "Epoch 615 Loss 10.656623840332031\n",
      "Epoch 616 Loss 11.91563606262207\n",
      "Epoch 617 Loss 12.935856819152832\n",
      "Epoch 618 Loss 7.931316375732422\n",
      "Epoch 619 Loss 7.533249378204346\n",
      "Epoch 620 Loss 13.54919719696045\n",
      "Epoch 621 Loss 4.2494916915893555\n",
      "Epoch 622 Loss 8.51633358001709\n",
      "Epoch 623 Loss 7.170496463775635\n",
      "Epoch 624 Loss 1.5155504941940308\n",
      "Epoch 625 Loss 7.004209518432617\n",
      "Epoch 626 Loss 12.35161018371582\n",
      "Epoch 627 Loss 3.1784586906433105\n",
      "Epoch 628 Loss 4.000041484832764\n",
      "Epoch 629 Loss 12.814786911010742\n",
      "Epoch 630 Loss 2.5034549236297607\n",
      "Epoch 631 Loss 7.270816802978516\n",
      "Epoch 632 Loss 7.379692554473877\n",
      "Epoch 633 Loss 5.798419952392578\n",
      "Epoch 634 Loss 4.062185764312744\n",
      "Epoch 635 Loss 1.5661704540252686\n",
      "Epoch 636 Loss 1.0433154106140137\n",
      "Epoch 637 Loss 2.455625057220459\n",
      "Epoch 638 Loss 9.90426254272461\n",
      "Epoch 639 Loss 7.089507579803467\n",
      "Epoch 640 Loss 8.038861274719238\n",
      "Epoch 641 Loss 4.495359897613525\n",
      "Epoch 642 Loss 5.796426296234131\n",
      "Epoch 643 Loss 7.433497428894043\n",
      "Epoch 644 Loss 5.2641167640686035\n",
      "Epoch 645 Loss 11.692485809326172\n",
      "Epoch 646 Loss 15.452537536621094\n",
      "Epoch 647 Loss 2.801146984100342\n",
      "Epoch 648 Loss 7.49858283996582\n",
      "Epoch 649 Loss 5.292216777801514\n",
      "Epoch 650 Loss 2.948397636413574\n",
      "Epoch 651 Loss 6.441753387451172\n",
      "Epoch 652 Loss 4.802178382873535\n",
      "Epoch 653 Loss 5.426279067993164\n",
      "Epoch 654 Loss 6.821455955505371\n",
      "Epoch 655 Loss 3.5622072219848633\n",
      "Epoch 656 Loss 13.774826049804688\n",
      "Epoch 657 Loss 3.4913058280944824\n",
      "Epoch 658 Loss 6.7693963050842285\n",
      "Epoch 659 Loss 6.766303062438965\n",
      "Epoch 660 Loss 5.445507049560547\n",
      "Epoch 661 Loss 7.82814359664917\n",
      "Epoch 662 Loss 7.211300373077393\n",
      "Epoch 663 Loss 4.578227519989014\n",
      "Epoch 664 Loss 6.185899257659912\n",
      "Epoch 665 Loss 7.787297248840332\n",
      "Epoch 666 Loss 3.0146920680999756\n",
      "Epoch 667 Loss 3.792119026184082\n",
      "Epoch 668 Loss 6.404105186462402\n",
      "Epoch 669 Loss 6.2006425857543945\n",
      "Epoch 670 Loss 8.234654426574707\n",
      "Epoch 671 Loss 2.920525550842285\n",
      "Epoch 672 Loss 3.5425095558166504\n",
      "Epoch 673 Loss 3.1892805099487305\n",
      "Epoch 674 Loss 2.2431528568267822\n",
      "Epoch 675 Loss 14.68115234375\n",
      "Epoch 676 Loss 2.8303732872009277\n",
      "Epoch 677 Loss 7.103333473205566\n",
      "Epoch 678 Loss 9.545039176940918\n",
      "Epoch 679 Loss 6.019309043884277\n",
      "Epoch 680 Loss 3.225986957550049\n",
      "Epoch 681 Loss 1.2423183917999268\n",
      "Epoch 682 Loss 2.749356746673584\n",
      "Epoch 683 Loss 8.367063522338867\n",
      "Epoch 684 Loss 1.4371862411499023\n",
      "Epoch 685 Loss 14.085291862487793\n",
      "Epoch 686 Loss 10.534656524658203\n",
      "Epoch 687 Loss 9.343199729919434\n",
      "Epoch 688 Loss 7.902980327606201\n",
      "Epoch 689 Loss 3.7039589881896973\n",
      "Epoch 690 Loss 2.2272186279296875\n",
      "Epoch 691 Loss 10.611223220825195\n",
      "Epoch 692 Loss 6.893149375915527\n",
      "Epoch 693 Loss 2.7411234378814697\n",
      "Epoch 694 Loss 0.7531009912490845\n",
      "Epoch 695 Loss 4.322101593017578\n",
      "Epoch 696 Loss 8.536176681518555\n",
      "Epoch 697 Loss 6.386594295501709\n",
      "Epoch 698 Loss 7.2209153175354\n",
      "Epoch 699 Loss 8.520267486572266\n",
      "Epoch 700 Loss 8.665706634521484\n",
      "Epoch 701 Loss 5.931850433349609\n",
      "Epoch 702 Loss 2.2616450786590576\n",
      "Epoch 703 Loss 2.770299196243286\n",
      "Epoch 704 Loss 6.3368706703186035\n",
      "Epoch 705 Loss 5.365757942199707\n",
      "Epoch 706 Loss 7.136260509490967\n",
      "Epoch 707 Loss 2.0751404762268066\n",
      "Epoch 708 Loss 4.402472019195557\n",
      "Epoch 709 Loss 9.849010467529297\n",
      "Epoch 710 Loss 2.894615650177002\n",
      "Epoch 711 Loss 3.6491851806640625\n",
      "Epoch 712 Loss 3.8559370040893555\n",
      "Epoch 713 Loss 9.37507438659668\n",
      "Epoch 714 Loss 10.54234504699707\n",
      "Epoch 715 Loss 4.188058853149414\n",
      "Epoch 716 Loss 5.948623180389404\n",
      "Epoch 717 Loss 5.337345600128174\n",
      "Epoch 718 Loss 2.54685378074646\n",
      "Epoch 719 Loss 0.7239457368850708\n",
      "Epoch 720 Loss 7.84918737411499\n",
      "Epoch 721 Loss 7.352085113525391\n",
      "Epoch 722 Loss 7.334946632385254\n",
      "Epoch 723 Loss 3.2766380310058594\n",
      "Epoch 724 Loss 1.9969987869262695\n",
      "Epoch 725 Loss 2.904510498046875\n",
      "Epoch 726 Loss 9.695561408996582\n",
      "Epoch 727 Loss 3.814321994781494\n",
      "Epoch 728 Loss 0.8292584419250488\n",
      "Epoch 729 Loss 6.495344161987305\n",
      "Epoch 730 Loss 3.094681978225708\n",
      "Epoch 731 Loss 6.13817834854126\n",
      "Epoch 732 Loss 5.418828964233398\n",
      "Epoch 733 Loss 1.94456946849823\n",
      "Epoch 734 Loss 3.509431838989258\n",
      "Epoch 735 Loss 6.063113689422607\n",
      "Epoch 736 Loss 4.33731746673584\n",
      "Epoch 737 Loss 3.8456714153289795\n",
      "Epoch 738 Loss 6.2234930992126465\n",
      "Epoch 739 Loss 1.8464224338531494\n",
      "Epoch 740 Loss 4.393022537231445\n",
      "Epoch 741 Loss 5.5022735595703125\n",
      "Epoch 742 Loss 1.6471590995788574\n",
      "Epoch 743 Loss 2.9877326488494873\n",
      "Epoch 744 Loss 2.4467315673828125\n",
      "Epoch 745 Loss 2.2697668075561523\n",
      "Epoch 746 Loss 3.7819981575012207\n",
      "Epoch 747 Loss 3.797290325164795\n",
      "Epoch 748 Loss 6.4179511070251465\n",
      "Epoch 749 Loss 1.4008113145828247\n",
      "Epoch 750 Loss 4.869555473327637\n",
      "Epoch 751 Loss 4.973354816436768\n",
      "Epoch 752 Loss 3.356020450592041\n",
      "Epoch 753 Loss 6.982330322265625\n",
      "Epoch 754 Loss 5.222423553466797\n",
      "Epoch 755 Loss 4.408663749694824\n",
      "Epoch 756 Loss 0.648025393486023\n",
      "Epoch 757 Loss 5.582481861114502\n",
      "Epoch 758 Loss 6.4630537033081055\n",
      "Epoch 759 Loss 4.922395706176758\n",
      "Epoch 760 Loss 2.6641061305999756\n",
      "Epoch 761 Loss 2.1206142902374268\n",
      "Epoch 762 Loss 3.2857284545898438\n",
      "Epoch 763 Loss 6.237782955169678\n",
      "Epoch 764 Loss 6.373503684997559\n",
      "Epoch 765 Loss 6.227154731750488\n",
      "Epoch 766 Loss 7.495738983154297\n",
      "Epoch 767 Loss 2.6565399169921875\n",
      "Epoch 768 Loss 4.202694892883301\n",
      "Epoch 769 Loss 5.082448959350586\n",
      "Epoch 770 Loss 4.412782192230225\n",
      "Epoch 771 Loss 2.777665615081787\n",
      "Epoch 772 Loss 8.085280418395996\n",
      "Epoch 773 Loss 4.825244903564453\n",
      "Epoch 774 Loss 3.0883655548095703\n",
      "Epoch 775 Loss 4.354200839996338\n",
      "Epoch 776 Loss 2.246274948120117\n",
      "Epoch 777 Loss 9.32373046875\n",
      "Epoch 778 Loss 8.615591049194336\n",
      "Epoch 779 Loss 5.145368576049805\n",
      "Epoch 780 Loss 2.0731616020202637\n",
      "Epoch 781 Loss 3.0858330726623535\n",
      "Epoch 782 Loss 6.903615951538086\n",
      "Epoch 783 Loss 3.909705877304077\n",
      "Epoch 784 Loss 7.9399919509887695\n",
      "Epoch 785 Loss 5.165339469909668\n",
      "Epoch 786 Loss 1.1302098035812378\n",
      "Epoch 787 Loss 0.8354717493057251\n",
      "Epoch 788 Loss 3.452333450317383\n",
      "Epoch 789 Loss 3.85884952545166\n",
      "Epoch 790 Loss 2.3163065910339355\n",
      "Epoch 791 Loss 2.4169363975524902\n",
      "Epoch 792 Loss 1.5876562595367432\n",
      "Epoch 793 Loss 3.977813243865967\n",
      "Epoch 794 Loss 6.333920478820801\n",
      "Epoch 795 Loss 4.5162353515625\n",
      "Epoch 796 Loss 1.2602213621139526\n",
      "Epoch 797 Loss 2.842712879180908\n",
      "Epoch 798 Loss 5.587407112121582\n",
      "Epoch 799 Loss 1.4552870988845825\n",
      "Epoch 800 Loss 4.962763786315918\n",
      "Epoch 801 Loss 4.1125288009643555\n",
      "Epoch 802 Loss 3.784987211227417\n",
      "Epoch 803 Loss 2.2304954528808594\n",
      "Epoch 804 Loss 2.9634592533111572\n",
      "Epoch 805 Loss 3.067380428314209\n",
      "Epoch 806 Loss 2.557481288909912\n",
      "Epoch 807 Loss 5.610945701599121\n",
      "Epoch 808 Loss 5.242776870727539\n",
      "Epoch 809 Loss 6.411722183227539\n",
      "Epoch 810 Loss 3.4732587337493896\n",
      "Epoch 811 Loss 4.219545364379883\n",
      "Epoch 812 Loss 2.595013380050659\n",
      "Epoch 813 Loss 4.6531243324279785\n",
      "Epoch 814 Loss 2.7414376735687256\n",
      "Epoch 815 Loss 3.3601086139678955\n",
      "Epoch 816 Loss 2.6039528846740723\n",
      "Epoch 817 Loss 4.952145576477051\n",
      "Epoch 818 Loss 1.8369086980819702\n",
      "Epoch 819 Loss 5.693962574005127\n",
      "Epoch 820 Loss 2.6267247200012207\n",
      "Epoch 821 Loss 6.984399795532227\n",
      "Epoch 822 Loss 7.908729553222656\n",
      "Epoch 823 Loss 1.5895509719848633\n",
      "Epoch 824 Loss 2.2839016914367676\n",
      "Epoch 825 Loss 5.288578987121582\n",
      "Epoch 826 Loss 4.155067443847656\n",
      "Epoch 827 Loss 5.22245454788208\n",
      "Epoch 828 Loss 1.302418828010559\n",
      "Epoch 829 Loss 8.601448059082031\n",
      "Epoch 830 Loss 1.7052019834518433\n",
      "Epoch 831 Loss 4.309997081756592\n",
      "Epoch 832 Loss 4.272237300872803\n",
      "Epoch 833 Loss 2.0761523246765137\n",
      "Epoch 834 Loss 2.6475911140441895\n",
      "Epoch 835 Loss 2.8900928497314453\n",
      "Epoch 836 Loss 1.6028618812561035\n",
      "Epoch 837 Loss 7.002479553222656\n",
      "Epoch 838 Loss 4.135093688964844\n",
      "Epoch 839 Loss 5.436629295349121\n",
      "Epoch 840 Loss 4.216720104217529\n",
      "Epoch 841 Loss 4.558183670043945\n",
      "Epoch 842 Loss 5.221649646759033\n",
      "Epoch 843 Loss 3.253849983215332\n",
      "Epoch 844 Loss 4.691468715667725\n",
      "Epoch 845 Loss 2.505695343017578\n",
      "Epoch 846 Loss 0.5196171998977661\n",
      "Epoch 847 Loss 7.964432716369629\n",
      "Epoch 848 Loss 1.9228794574737549\n",
      "Epoch 849 Loss 3.088188886642456\n",
      "Epoch 850 Loss 7.504543304443359\n",
      "Epoch 851 Loss 1.1671769618988037\n",
      "Epoch 852 Loss 1.6064366102218628\n",
      "Epoch 853 Loss 7.459633827209473\n",
      "Epoch 854 Loss 1.0563042163848877\n",
      "Epoch 855 Loss 4.807693004608154\n",
      "Epoch 856 Loss 3.8299548625946045\n",
      "Epoch 857 Loss 5.633038520812988\n",
      "Epoch 858 Loss 2.5495500564575195\n",
      "Epoch 859 Loss 4.745033264160156\n",
      "Epoch 860 Loss 4.08209228515625\n",
      "Epoch 861 Loss 6.871744632720947\n",
      "Epoch 862 Loss 3.844297409057617\n",
      "Epoch 863 Loss 2.139185905456543\n",
      "Epoch 864 Loss 1.9283548593521118\n",
      "Epoch 865 Loss 0.5687968730926514\n",
      "Epoch 866 Loss 2.0584497451782227\n",
      "Epoch 867 Loss 6.377278804779053\n",
      "Epoch 868 Loss 4.973226547241211\n",
      "Epoch 869 Loss 5.430091857910156\n",
      "Epoch 870 Loss 3.3146815299987793\n",
      "Epoch 871 Loss 3.438983201980591\n",
      "Epoch 872 Loss 2.916266679763794\n",
      "Epoch 873 Loss 1.2651633024215698\n",
      "Epoch 874 Loss 3.993227481842041\n",
      "Epoch 875 Loss 3.0082919597625732\n",
      "Epoch 876 Loss 3.494596004486084\n",
      "Epoch 877 Loss 6.093234062194824\n",
      "Epoch 878 Loss 4.256198883056641\n",
      "Epoch 879 Loss 5.6803483963012695\n",
      "Epoch 880 Loss 4.444064140319824\n",
      "Epoch 881 Loss 5.207431316375732\n",
      "Epoch 882 Loss 1.487755298614502\n",
      "Epoch 883 Loss 5.450916290283203\n",
      "Epoch 884 Loss 2.320798635482788\n",
      "Epoch 885 Loss 4.4878363609313965\n",
      "Epoch 886 Loss 3.834779977798462\n",
      "Epoch 887 Loss 2.4287331104278564\n",
      "Epoch 888 Loss 1.4106261730194092\n",
      "Epoch 889 Loss 1.3302574157714844\n",
      "Epoch 890 Loss 2.0082602500915527\n",
      "Epoch 891 Loss 3.9139397144317627\n",
      "Epoch 892 Loss 0.5249339938163757\n",
      "Epoch 893 Loss 6.655816555023193\n",
      "Epoch 894 Loss 2.8503143787384033\n",
      "Epoch 895 Loss 4.710357666015625\n",
      "Epoch 896 Loss 2.596924304962158\n",
      "Epoch 897 Loss 4.669045448303223\n",
      "Epoch 898 Loss 3.43998384475708\n",
      "Epoch 899 Loss 2.2212002277374268\n",
      "Epoch 900 Loss 1.1474602222442627\n",
      "Epoch 901 Loss 0.6863152980804443\n",
      "Epoch 902 Loss 4.196991920471191\n",
      "Epoch 903 Loss 2.542187213897705\n",
      "Epoch 904 Loss 0.9818145036697388\n",
      "Epoch 905 Loss 0.8092405796051025\n",
      "Epoch 906 Loss 2.3385283946990967\n",
      "Epoch 907 Loss 4.889683246612549\n",
      "Epoch 908 Loss 1.6252435445785522\n",
      "Epoch 909 Loss 1.394214391708374\n",
      "Epoch 910 Loss 2.965205669403076\n",
      "Epoch 911 Loss 1.3613287210464478\n",
      "Epoch 912 Loss 3.1673576831817627\n",
      "Epoch 913 Loss 1.4906995296478271\n",
      "Epoch 914 Loss 1.0431441068649292\n",
      "Epoch 915 Loss 3.7815921306610107\n",
      "Epoch 916 Loss 2.1545469760894775\n",
      "Epoch 917 Loss 3.1653690338134766\n",
      "Epoch 918 Loss 3.9580578804016113\n",
      "Epoch 919 Loss 3.1968188285827637\n",
      "Epoch 920 Loss 3.886308193206787\n",
      "Epoch 921 Loss 2.991445541381836\n",
      "Epoch 922 Loss 1.6797115802764893\n",
      "Epoch 923 Loss 4.884575843811035\n",
      "Epoch 924 Loss 8.690577507019043\n",
      "Epoch 925 Loss 3.5043883323669434\n",
      "Epoch 926 Loss 1.3409768342971802\n",
      "Epoch 927 Loss 4.20376443862915\n",
      "Epoch 928 Loss 2.8161702156066895\n",
      "Epoch 929 Loss 1.5888937711715698\n",
      "Epoch 930 Loss 0.7304330468177795\n",
      "Epoch 931 Loss 3.4795022010803223\n",
      "Epoch 932 Loss 5.429775714874268\n",
      "Epoch 933 Loss 1.6394984722137451\n",
      "Epoch 934 Loss 4.290287494659424\n",
      "Epoch 935 Loss 3.953002452850342\n",
      "Epoch 936 Loss 2.1522774696350098\n",
      "Epoch 937 Loss 2.078082799911499\n",
      "Epoch 938 Loss 5.744633197784424\n",
      "Epoch 939 Loss 6.118638515472412\n",
      "Epoch 940 Loss 3.7577965259552\n",
      "Epoch 941 Loss 3.565671682357788\n",
      "Epoch 942 Loss 4.606584072113037\n",
      "Epoch 943 Loss 4.775611877441406\n",
      "Epoch 944 Loss 2.253871440887451\n",
      "Epoch 945 Loss 3.7762162685394287\n",
      "Epoch 946 Loss 2.2220559120178223\n",
      "Epoch 947 Loss 2.8914120197296143\n",
      "Epoch 948 Loss 1.3294436931610107\n",
      "Epoch 949 Loss 1.828150987625122\n",
      "Epoch 950 Loss 1.064354419708252\n",
      "Epoch 951 Loss 1.8058608770370483\n",
      "Epoch 952 Loss 3.4206624031066895\n",
      "Epoch 953 Loss 1.7534000873565674\n",
      "Epoch 954 Loss 3.6494743824005127\n",
      "Epoch 955 Loss 2.135129928588867\n",
      "Epoch 956 Loss 2.431995391845703\n",
      "Epoch 957 Loss 2.1228299140930176\n",
      "Epoch 958 Loss 1.9349722862243652\n",
      "Epoch 959 Loss 3.9900293350219727\n",
      "Epoch 960 Loss 4.000060081481934\n",
      "Epoch 961 Loss 3.7142715454101562\n",
      "Epoch 962 Loss 5.110998630523682\n",
      "Epoch 963 Loss 1.5023080110549927\n",
      "Epoch 964 Loss 3.358302593231201\n",
      "Epoch 965 Loss 3.7165207862854004\n",
      "Epoch 966 Loss 1.3286032676696777\n",
      "Epoch 967 Loss 3.118999719619751\n",
      "Epoch 968 Loss 3.690495014190674\n",
      "Epoch 969 Loss 2.050826072692871\n",
      "Epoch 970 Loss 1.4895943403244019\n",
      "Epoch 971 Loss 4.408572196960449\n",
      "Epoch 972 Loss 2.4888720512390137\n",
      "Epoch 973 Loss 2.1119046211242676\n",
      "Epoch 974 Loss 1.792282223701477\n",
      "Epoch 975 Loss 1.682674765586853\n",
      "Epoch 976 Loss 4.340045928955078\n",
      "Epoch 977 Loss 2.2871108055114746\n",
      "Epoch 978 Loss 2.053264856338501\n",
      "Epoch 979 Loss 2.218388319015503\n",
      "Epoch 980 Loss 2.110447406768799\n",
      "Epoch 981 Loss 2.580167531967163\n",
      "Epoch 982 Loss 4.204965591430664\n",
      "Epoch 983 Loss 2.2478113174438477\n",
      "Epoch 984 Loss 0.7046889066696167\n",
      "Epoch 985 Loss 2.604410171508789\n",
      "Epoch 986 Loss 0.702424168586731\n",
      "Epoch 987 Loss 2.9394028186798096\n",
      "Epoch 988 Loss 2.5863399505615234\n",
      "Epoch 989 Loss 1.877658486366272\n",
      "Epoch 990 Loss 3.4134321212768555\n",
      "Epoch 991 Loss 4.259321689605713\n",
      "Epoch 992 Loss 3.3435349464416504\n",
      "Epoch 993 Loss 1.6372692584991455\n",
      "Epoch 994 Loss 2.3423359394073486\n",
      "Epoch 995 Loss 0.6501179337501526\n",
      "Epoch 996 Loss 2.0796682834625244\n",
      "Epoch 997 Loss 2.6764450073242188\n",
      "Epoch 998 Loss 4.204349517822266\n",
      "Epoch 999 Loss 2.9194729328155518\n"
     ]
    }
   ],
   "source": [
    "# we can now use the dataloader to train the model\n",
    "model = SquareModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in dataloader:\n",
    "        X_batch, Y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} Loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's a again do inference\n",
    "\n",
    "pred = model(X_test)\n",
    "plt.scatter(X_test, Y_test)\n",
    "plt.scatter(X_test, pred.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
